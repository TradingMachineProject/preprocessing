{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBRARY IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import time  \n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "from sklearn.preprocessing import minmax_scale  \n",
    "from typing import Dict, List, Optional, Tuple  \n",
    "import seaborn as sns \n",
    "import gc\n",
    "import traceback \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import kendalltau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECK NULL VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_result_df = pd.read_csv(\"./DB/professor_BTC_sum_both_10m.csv\")\n",
    "# 평균이 아닌 이전 값으로 Null 채우기\n",
    "combined_result_df = combined_result_df.fillna(method='ffill') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Feature  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_result_df['liq_last_1'] = np.log10(combined_result_df['liq_last_1'] + 0.00001)\n",
    "combined_result_df['liq_last_2'] = np.log10(combined_result_df['liq_last_2'] + 0.00001)\n",
    "combined_result_df['liq_last_5'] = np.log10(combined_result_df['liq_last_5'] + 0.00001)\n",
    "combined_result_df['liq_last_10'] = np.log10(combined_result_df['liq_last_10'] + 0.00001)\n",
    "combined_result_df['liq_last_15'] = np.log10(combined_result_df['liq_last_15'] + 0.00001)\n",
    "combined_result_df['trade_vol'] = np.log10(combined_result_df['trade_vol'] + 0.00001)\n",
    "combined_result_df['num_trades'] = np.log10(combined_result_df['num_trades'] + 0.00001)\n",
    "\n",
    "combined_result_df['trade.tau'] = np.sqrt(1 / combined_result_df['num_trades'])\n",
    "combined_result_df['tvpl1'] = combined_result_df['trade_vol'] / combined_result_df['liq_last_1']\n",
    "combined_result_df['tvpl2'] = combined_result_df['trade_vol'] / combined_result_df['liq_last_2'] \n",
    "combined_result_df['tvpl5'] = combined_result_df['trade_vol'] / combined_result_df['liq_last_5'] \n",
    "combined_result_df['tvpl10'] = combined_result_df['trade_vol'] / combined_result_df['liq_last_10'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUMERIC FEATURES & CALCULATE CORR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_feature_list = list(combined_result_df.columns)\n",
    "main_feature_list.remove('window_start')\n",
    "main_feature_list.remove('window_end')\n",
    "main_feature_list.remove('time_id')\n",
    "main_feature_list.remove('volume_power')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-numeric columns or columns with missing values\n",
    "main_feature_list = data.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Calculate correlation matrix using kendalltau method\n",
    "correlation_matrix = data[main_feature_list].corr(method=lambda x, y: kendalltau(x, y).correlation)\n",
    "\n",
    "main_feature_list.remove('dv1_realized_volatility')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILD NEIGHBORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NEIGHBORS_MAX = 65 \n",
    "\n",
    "class Neighbors:\n",
    "    def __init__(self, \n",
    "                 name: str, \n",
    "                 pivot: pd.DataFrame, \n",
    "                 p: float, \n",
    "                 metric: str = 'minkowski', \n",
    "                 metric_params: object = None, \n",
    "                 exclude_self: bool = True,\n",
    "                 ):\n",
    "        self.name = name\n",
    "        self.exclude_self = exclude_self\n",
    "        self.p = p\n",
    "        self.metric = metric\n",
    "\n",
    "        nn = NearestNeighbors(\n",
    "            n_neighbors=N_NEIGHBORS_MAX, \n",
    "            p=p, \n",
    "            metric=metric, \n",
    "            metric_params=metric_params\n",
    "        )\n",
    "        \n",
    "        nn.fit(pivot)\n",
    "        _, self.neighbors = nn.kneighbors(pivot, return_distance=True)\n",
    "\n",
    "        self.columns = self.index = self.feature_values = self.feature_col = None\n",
    "\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def make_nn_feature(self, n=5, agg=np.mean) -> pd.DataFrame:\n",
    "        assert self.feature_values is not None, \"should call rearrange_feature_values beforehand\"\n",
    "\n",
    "        start = 1 if self.exclude_self else 0\n",
    "\n",
    "        pivot_aggs = pd.DataFrame(\n",
    "            agg(self.feature_values[start:n,:,0], axis=0), \n",
    "            columns=self.columns, \n",
    "            index=self.index\n",
    "        )\n",
    "\n",
    "        dst = pivot_aggs.reset_index() # unstack().\n",
    "        # print(\"dst.shape:\", dst.shape)\n",
    "        new_column_names = ['time_id', f'{self.feature_col}_nn{n}_{self.name}_{agg.__name__}'] # 3개를 예측했는데 2개만 들어왔다??\n",
    "        dst.columns = new_column_names \n",
    "        return dst\n",
    "    \n",
    "\n",
    "class TimeIdNeighbors(Neighbors):\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        # feature_pivot = df.pivot(index='time_id', values=feature_col)\n",
    "        # feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "\n",
    "        feature_df = df[['time_id', feature_col]]\n",
    "        feature_df.set_index('time_id', inplace=True)\n",
    "        feature_df = feature_df.fillna(feature_df.mean())\n",
    "\n",
    "        feature_values = np.zeros((N_NEIGHBORS_MAX, feature_df.shape[0], 1))\n",
    "\n",
    "        for i in range(N_NEIGHBORS_MAX):\n",
    "            feature_values[i, :, 0] += feature_df.values[self.neighbors[:, i], 0]\n",
    "\n",
    "        self.columns = list(feature_df.columns)\n",
    "        self.index = list(feature_df.index)\n",
    "        self.feature_values = feature_values\n",
    "        self.feature_col = feature_col\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"time-id NN (name={self.name}, metric={self.metric}, p={self.p})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROGRESS CHECK FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - s\n",
    "    print(f'[{name}] {elapsed: .3f}초')\n",
    "\n",
    "def print_trace(name: str = ''):\n",
    "    print(f'{name or \"익명\"}에서 에러가 발생했습니다.')\n",
    "    print(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET NN CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# StandardScaler 객체 생성\n",
    "scaler = StandardScaler()\n",
    "\n",
    "USE_ONE_FEATURE_C = True\n",
    "USE_ONE_FEATURE_M_1 = True\n",
    "USE_ONE_FEATURE_M_2 = True\n",
    "\n",
    "USE_TWO_FEATURES = True\n",
    "\n",
    "USE_ALL_FEATURES = True\n",
    "USE_SEVALRAL_FEATURES = True\n",
    "\n",
    "# Top 5 Related Feature\n",
    "top_5_high_feat = list(correlation_matrix['realized_volatility'].sort_values().keys())[:5]\n",
    "top_5_low_feat = list(correlation_matrix['realized_volatility'].sort_values().keys())[-6:-1]\n",
    "\n",
    "\n",
    "# Top 5 Absolute Related Feature\n",
    "\n",
    "sorted_data = correlation_matrix['realized_volatility'].abs().sort_values(ascending=False)\n",
    "\n",
    "top_5_high_abs_feat = list(sorted_data.head(6).keys())[1:]\n",
    "top_5_low_abs_feat = list(sorted_data.tail(5).keys())\n",
    "\n",
    "# time_id_neighbors List \n",
    "time_id_neighbors: List[Neighbors] = []\n",
    "\n",
    "with timer('knn fit'):\n",
    "    df_pv = combined_result_df.copy()\n",
    "    df_pv = df_pv.drop(['window_start', 'window_end','volume_power'], axis=1)\n",
    "    \n",
    "    # Standard All Feature\n",
    "    df_pv[main_feature_list] = scaler.fit_transform(df_pv[main_feature_list])\n",
    "\n",
    "    # USE ONLY ONE FACTOR\n",
    "    ## Canberra Distance\n",
    "    if USE_ONE_FEATURE_C :\n",
    "        for feat in main_feature_list :\n",
    "            df_nn = df_pv[['time_id',feat]]\n",
    "            df_nn.set_index('time_id', inplace=True)\n",
    "            df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "            time_id_neighbors.append(\n",
    "                TimeIdNeighbors(\n",
    "                    feat + '_c', \n",
    "                    df_nn, \n",
    "                    p=2, \n",
    "                    metric='canberra', \n",
    "                    exclude_self=True\n",
    "                )\n",
    "            )\n",
    "    ## Manhattan Distance\n",
    "    \n",
    "    if USE_ONE_FEATURE_M_1:\n",
    "        for feat in main_feature_list :\n",
    "            df_nn = df_pv[['time_id',feat]]\n",
    "            df_nn.set_index('time_id', inplace=True)\n",
    "            df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "            time_id_neighbors.append(\n",
    "                TimeIdNeighbors(feat + '_m_p1', df_nn, p=1)\n",
    "            )\n",
    "\n",
    "    ## Euclidean Distance\n",
    "\n",
    "    if USE_ONE_FEATURE_M_2:\n",
    "        for feat in main_feature_list :\n",
    "            df_nn = df_pv[['time_id',feat]]\n",
    "            df_nn.set_index('time_id', inplace=True)\n",
    "            df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "            time_id_neighbors.append(\n",
    "                TimeIdNeighbors(feat + '_m_p2', df_nn, p=2)\n",
    "            )\n",
    "\n",
    "    # TWO FACTOR\n",
    "\n",
    "    if USE_TWO_FEATURES:\n",
    "        feature_list = ['time_id','realized_volatility','bidask_spread_0']\n",
    "        df_nn = df_pv[feature_list]\n",
    "        df_nn.set_index('time_id', inplace=True)\n",
    "        df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "        ## Canberra\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "                TimeIdNeighbors(\n",
    "                    feat + 'two_c', \n",
    "                    df_nn, \n",
    "                    p=2, \n",
    "                    metric='canberra', \n",
    "                    exclude_self=True\n",
    "                )\n",
    "            )\n",
    "        ## Euclidean Distance\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'two_m', \n",
    "                df_nn, \n",
    "                p=2, \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # USE SEVALRAL FACTOR\n",
    "    if USE_SEVALRAL_FEATURES:\n",
    "        ## High Related Feature \n",
    "        feature_list = ['time_id']\n",
    "        feature_list += top_5_high_feat\n",
    "        df_nn = df_pv[feature_list]\n",
    "        df_nn.set_index('time_id', inplace=True)\n",
    "        df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "        ### Euclidean Distance\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'sev_high_nn_m', \n",
    "                df_nn, \n",
    "                p=2, \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ## Low Related Feature\n",
    "\n",
    "        feature_list = ['time_id']\n",
    "        feature_list += top_5_low_feat\n",
    "        df_nn = df_pv[feature_list]\n",
    "        df_nn.set_index('time_id', inplace=True)\n",
    "        df_nn = df_nn.fillna(df_nn.mean())        \n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'sev_low_nn_m', \n",
    "                df_nn, \n",
    "                p=2, \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ## High Abs Related Feature\n",
    "\n",
    "        feature_list = ['time_id']\n",
    "        feature_list += top_5_high_abs_feat\n",
    "        df_nn = df_pv[feature_list]\n",
    "        df_nn.set_index('time_id', inplace=True)\n",
    "        df_nn = df_nn.fillna(df_nn.mean())\n",
    "        \n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'sev_high_abs_nn_m', \n",
    "                df_nn, \n",
    "                p=2, \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ## Low Abs Related Feature\n",
    "\n",
    "        feature_list = ['time_id']\n",
    "        feature_list += top_5_low_abs_feat\n",
    "        df_nn = df_pv[feature_list]\n",
    "        df_nn.set_index('time_id', inplace=True)\n",
    "        df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'sev_low_abs_nn_m', \n",
    "                df_nn, \n",
    "                p=2, \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    # USE ALL FACTOR\n",
    "\n",
    "    if USE_ALL_FEATURES:\n",
    "        df_nn = df_pv.copy()\n",
    "        df_nn = df_nn.drop(['dv1_realized_volatility'], axis=1)\n",
    "        df_nn.set_index('time_id', inplace=True)\n",
    "        df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'all_nn_m_p1', \n",
    "                df_nn, \n",
    "                p=1, \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'all_nn_m_p2', \n",
    "                df_nn, \n",
    "                p=2, \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate Features With NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nearest_neighbor_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df2 = combined_result_df.copy()\n",
    "    print(df2.shape)\n",
    "\n",
    "    ### time_id를 기준으로 얻어진 neighbor를 대상으로 feature 만들기\n",
    "    feature_cols = {\n",
    "        'realized_volatility': [np.mean, np.min, np.max, np.std],\n",
    "        'lowest_return': [np.max, np.mean, np.min],\n",
    "        'num_trades': [np.mean],\n",
    "        'trade.tau': [np.mean],\n",
    "        'trade_vol': [np.mean],\n",
    "        'dv1_realized_volatility': [np.mean],\n",
    "    }\n",
    "\n",
    "    time_id_neigbor_sizes = [2, 4, 8, 16, 32, 48, 64]\n",
    "\n",
    "    ndf: Optional[pd.DataFrame] = None\n",
    "    \n",
    "    # 새로운 feature를 기존 df에 추가하는 함수\n",
    "    def _add_ndf(ndf: Optional[pd.DataFrame], dst: pd.DataFrame) -> pd.DataFrame:\n",
    "        if ndf is None:\n",
    "            return dst\n",
    "        else:\n",
    "            ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
    "            #columns_to_convert = [dst.columns[-1]]  # 열 변환 대상을 선택하거나 여러 열을 지정할 수 있음\n",
    "            #converted_columns = dst[columns_to_convert].astype(np.float32)\n",
    "            #ndf = pd.concat([ndf, converted_columns], axis=1)\n",
    "\n",
    "            return ndf\n",
    "\n",
    "    # neighbor time_id\n",
    "    for feature_col in feature_cols.keys():\n",
    "        try: \n",
    "            for nn in time_id_neighbors:\n",
    "                nn.rearrange_feature_values(df2, feature_col)\n",
    "\n",
    "\n",
    "            time_id_ns = time_id_neigbor_sizes\n",
    "\n",
    "            for agg in feature_cols[feature_col]:\n",
    "                for n in time_id_ns:\n",
    "                    try:\n",
    "                        for nn in time_id_neighbors:\n",
    "                            dst = nn.make_nn_feature(n, agg)\n",
    "                            ndf = _add_ndf(ndf, dst)\n",
    "                    except Exception:\n",
    "                        WHERE_ERROR = feature_col\n",
    "                        print_trace('time-id nn')\n",
    "                        pass\n",
    "        except Exception:\n",
    "            print_trace('time-id nn')\n",
    "\n",
    "    if ndf is not None:\n",
    "        df2 = pd.merge(df2, ndf, on=['time_id'], how='left')\n",
    "    \n",
    "    print(df2.shape)\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "with timer('make nearest neighbor feature'):\n",
    "    df3 = make_nearest_neighbor_feature(combined_result_df)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN PREDICTION\n",
    "- NN을 기준으로 예측한 dv1_rv 값\n",
    "- NN을 기준으로 비슷한 경우들의 dv1_rv 값들의 평균 (비슷했던 경우들의 평균)\n",
    "\n",
    "아래 과정은 PASS 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_value = \"dv1_realized_volatility\"\n",
    "predict_list = []\n",
    "for item in df3.columns:\n",
    "    if target_value in item:\n",
    "        predict_list.append(item)\n",
    "\n",
    "predict_list = predict_list[1:]\n",
    "\n",
    "\n",
    "predict = {}\n",
    "for item in predict_list :\n",
    "    predict[item] = rmspe(\n",
    "        np.array(df3[\"dv1_realized_volatility\"]),\n",
    "        np.array(df3[item])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_items = sorted(predict.items(), key=lambda x: x[1])\n",
    "\n",
    "for key, value in sorted_items[:10]:\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DF\n",
    "# df3.to_parquet(\"my.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE SELECTION\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
