{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f66688f-0347-493c-88ec-997e71aa7911",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# STEP1: 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de464b-dc5b-4819-bb40-2f9370a81eda",
   "metadata": {},
   "source": [
    "## 1-0: 전체 파일을 코인 별로 나누어 저장\n",
    "\n",
    "<!-- 전체파일을 sort_values 해보자. (시간순으로 정렬) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe57557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyarrow\n",
    "# pip install modin[all]\n",
    "# pip install distributed\n",
    "# pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a0233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 크기가 커서 저장이 어려움. 따라서 dask dataframe으로 저장.\n",
    "pip install \"dask[dataframe]\" --upgrade     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923065c2-92c9-45d1-baad-4bdef12bcaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#USE ONLY ONE OF THESE:\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"ray\"  # Modin will use Ray\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "\n",
    "# Change \"current directory\"\n",
    "# new_dir = 'D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'\n",
    "new_dir = 'C:\\\\Users\\\\user\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'\n",
    "os.chdir(new_dir)\n",
    "\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import csv\n",
    "import dask.dataframe as dd # This is a main package to process a large csv file.\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "# import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb57a4d-03a3-4589-ab0e-b2813a0bff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reader = csv.reader(in_csv)\n",
    "# header = next(reader)  # Skip header row\n",
    "# data = sorted(reader, key=lambda row: row[column_index_to_sort])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d449e17c-ac93-4593-872f-5de0cf2e34de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = 202303271051 # 실질적으로는 12/16부터 2/26까지 데이터 존재.\n",
    "# data_id = 202302230905 \n",
    "\n",
    "in_csv =\"D:\\\\ticker_data_{}.csv\".format(data_id)\n",
    "# in_csv =\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\ticker_data_{}.csv\".format(data_id)\n",
    "\n",
    "dtype={'ask_bid': 'object',\n",
    "       'change': 'object',\n",
    "       'highest_52_week_date': 'object',\n",
    "       'lowest_52_week_date': 'object',\n",
    "       'market_state': 'object',\n",
    "       'market_warning': 'object',\n",
    "       'orderbook_ap_0': 'float32',\n",
    "       'orderbook_ap_1': 'float32',\n",
    "       'orderbook_ap_2': 'float32',\n",
    "       'orderbook_ap_3': 'float32',\n",
    "       'orderbook_bp_0': 'float32',\n",
    "       'orderbook_bp_1': 'float32',\n",
    "       'orderbook_bp_10': 'float32',\n",
    "       'orderbook_bp_11': 'float32',\n",
    "       'orderbook_bp_12': 'float32',\n",
    "       'orderbook_bp_13': 'float32',\n",
    "       'orderbook_bp_14': 'float32',\n",
    "       'orderbook_bp_2': 'float32',\n",
    "       'orderbook_bp_3': 'float32',\n",
    "       'orderbook_bp_4': 'float32',\n",
    "       'orderbook_bp_5': 'float32',\n",
    "       'orderbook_bp_6': 'float32',\n",
    "       'orderbook_bp_7': 'float32',\n",
    "       'orderbook_bp_8': 'float32',\n",
    "       'orderbook_bp_9': 'float32',\n",
    "       'stream_type': 'object',\n",
    "       'trade_time': 'object'}\n",
    "\n",
    "# Read the CSV file using a Dask DataFrame\n",
    "df = dd.read_csv(in_csv, dtype=dtype) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4955a86c-5c6a-421a-94e5-f1f610923808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 수집된 파일에는 네개의 코인이 함께 저장되어 있음.\n",
    "## 따라서, Separate files by COIN\n",
    "\n",
    "coin_list = ['BTC', 'ETH', 'DOGE', 'XRP']\n",
    "# data_num = 3\n",
    "for coin in coin_list:\n",
    "    if coin == 'BTC':\n",
    "        globals()[\"df_BTC\"] = globals()[\"df\"][globals()[\"df\"].code == 'KRW-BTC']\n",
    "        df_BTC.to_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\t_data_{}_{}.csv\".format(coin, data_id))\n",
    "        print('Complete: {}'.format(coin))\n",
    "    elif coin == 'ETH':\n",
    "        globals()[\"df_ETH\"] = globals()[\"df\"][globals()[\"df\"].code == 'KRW-ETH']\n",
    "        df_ETH.to_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\t_data_{}_{}.csv\".format(coin, data_id))\n",
    "        print('Complete: {}'.format(coin))\n",
    "    elif coin == 'DOGE':\n",
    "        globals()[\"df_DOGE\"] = globals()[\"df\"][globals()[\"df\"].code == 'KRW-DOGE']\n",
    "        df_DOGE.to_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\t_data_{}_{}.csv\".format(coin, data_id))\n",
    "        print('Complete: {}'.format(coin))\n",
    "    else:\n",
    "        globals()[\"df_XRP\"] = globals()[\"df\"][globals()[\"df\"].code == 'KRW-XRP']\n",
    "        df_XRP.to_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\t_data_{}_{}.csv\".format(coin, data_id))\n",
    "        print('Complete: {}'.format(coin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7802b4-c1cb-488f-8d04-cd4bca445c94",
   "metadata": {},
   "source": [
    "## 1-1: 코인별로 나누어진 파일을, 시간순으로 sorting 후 다시 저장\n",
    "### duplicates 제거 및 30초/60초 ticker 값을 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfbc2f7-6e5c-43ff-83b7-05b9fc68b1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#USE ONLY ONE OF THESE:\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"ray\"  # Modin will use Ray\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "\n",
    "# Change \"current directory\"\n",
    "new_dir = 'D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'\n",
    "os.chdir(new_dir)\n",
    "\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import csv\n",
    "import dask.dataframe as dd # This is a main package to process a large csv file.\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "# import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4e889-9748-4b1f-9aa7-aff6f73f52e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtype={'ask_bid': 'object',\n",
    "       'change': 'object',\n",
    "       'highest_52_week_date': 'object',\n",
    "       'lowest_52_week_date': 'object',\n",
    "       'market_state': 'object',\n",
    "       'market_warning': 'object',\n",
    "       'orderbook_ap_0': 'float32',\n",
    "       'orderbook_ap_1': 'float32',\n",
    "       'orderbook_ap_2': 'float32',\n",
    "       'orderbook_ap_3': 'float32',\n",
    "       'orderbook_bp_0': 'float32',\n",
    "       'orderbook_bp_1': 'float32',\n",
    "       'orderbook_bp_10': 'float32',\n",
    "       'orderbook_bp_11': 'float32',\n",
    "       'orderbook_bp_12': 'float32',\n",
    "       'orderbook_bp_13': 'float32',\n",
    "       'orderbook_bp_14': 'float32',\n",
    "       'orderbook_bp_2': 'float32',\n",
    "       'orderbook_bp_3': 'float32',\n",
    "       'orderbook_bp_4': 'float32',\n",
    "       'orderbook_bp_5': 'float32',\n",
    "       'orderbook_bp_6': 'float32',\n",
    "       'orderbook_bp_7': 'float32',\n",
    "       'orderbook_bp_8': 'float32',\n",
    "       'orderbook_bp_9': 'float32',\n",
    "       'stream_type': 'object',\n",
    "       'trade_time': 'object'}\n",
    "\n",
    "\n",
    "data_id = 202303271051 # 해당 틱데이터 커버 기간: 22년 12월 16일 오후 9시 ~ 23년 2월 26일 오전 4시\n",
    "coin_list = [ 'BTC',  'ETH',  'DOGE', 'XRP'] \n",
    "\n",
    "for coin in coin_list:\n",
    "\n",
    "    # Read dask \"part\" files\n",
    "    part_files_path =  \"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\t_data_{}_{}.csv\\\\\".format(coin, data_id)\n",
    "    df = dd.read_csv( part_files_path + '*.part', dtype=dtype)    \n",
    "    # df = dd.read_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\t_data_{}_{}.csv\".format(coin, data_id))\n",
    "    \n",
    "    # Sort dataframe\n",
    "    df = df.sort_values('sys_datetime', ascending = True, na_position = 'last')\n",
    "    print('Complete: Sorting')\n",
    "    \n",
    "    # Write the sorted DataFrame to a new dask-type csv file\n",
    "    df.to_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\s_t_data_{}_{}.csv\".format(coin, data_id), index=False)\n",
    "    \n",
    "    # # Write the sorted DataFrame to a new parquet file\n",
    "    # df.to_parquet('s_t_data_{}_{}.parquet'.format(coin,data_id), engine='pyarrow')\n",
    "\n",
    "## Note: 용량 부족 메시지가 떠서, 한 코인씩 돌림. (for 문을 돌리지 못함.)    \n",
    "## Note: parquet으로는 저장되지 않음 (메모리 문제라는 메시지가 뜸.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457199bc-eb8c-48cd-88ca-b9889b35adac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(3) # Raw data가 어떤 모양인지 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba9fe5-1ce5-470a-9b62-d6ad0c115837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.tail(3) # 몇일까지 데이터가 존재하나 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44950a7",
   "metadata": {},
   "source": [
    "### duplicates 제거 및 30초/60초 ticker 값을 제거 (단위 시간마다 신규 거래가 없더라도 ticker 값이 들어온다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66158477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "new_dir = 'D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'\n",
    "os.chdir(new_dir)\n",
    "import numpy as np\n",
    "import csv\n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import gc\n",
    "\n",
    "\n",
    "dtype={'ask_bid': 'object',\n",
    "       'change': 'object',\n",
    "       'highest_52_week_date': 'object',\n",
    "       'lowest_52_week_date': 'object',\n",
    "       'market_state': 'object',\n",
    "       'market_warning': 'object',\n",
    "       'orderbook_ap_0': 'float32',\n",
    "       'orderbook_ap_1': 'float32',\n",
    "       'orderbook_ap_2': 'float32',\n",
    "       'orderbook_ap_3': 'float32',\n",
    "       'orderbook_bp_0': 'float32',\n",
    "       'orderbook_bp_1': 'float32',\n",
    "       'orderbook_bp_10': 'float32',\n",
    "       'orderbook_bp_11': 'float32',\n",
    "       'orderbook_bp_12': 'float32',\n",
    "       'orderbook_bp_13': 'float32',\n",
    "       'orderbook_bp_14': 'float32',\n",
    "       'orderbook_bp_2': 'float32',\n",
    "       'orderbook_bp_3': 'float32',\n",
    "       'orderbook_bp_4': 'float32',\n",
    "       'orderbook_bp_5': 'float32',\n",
    "       'orderbook_bp_6': 'float32',\n",
    "       'orderbook_bp_7': 'float32',\n",
    "       'orderbook_bp_8': 'float32',\n",
    "       'orderbook_bp_9': 'float32',\n",
    "       'stream_type': 'object',\n",
    "       'trade_time': 'object'}\n",
    "\n",
    "data_id = 202303271051 # 해당 틱데이터 커버 기간: 22년 12월 16일 오후 9시 ~ 23년 2월 26일 오전 4시\n",
    "coin_list = ['BTC', 'ETH',  'DOGE', 'XRP'] \n",
    "\n",
    "for coin in coin_list:\n",
    "\n",
    "    ### Read dask \"part\" failes\n",
    "    part_files_path =  \"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\s_t_data_{}_{}.csv\\\\\".format(coin, data_id)\n",
    "    df = dd.read_csv( part_files_path + '*.part', dtype=dtype)    \n",
    "    df = df.compute()\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    ### Drop duplicates\n",
    "    df.drop_duplicates(subset=[ 'type_websocket', 'timestamp','sys_datetime'], keep='first', inplace=True, ignore_index=True)\n",
    "\n",
    "    # df.drop_duplicates(subset=[ 'type_websocket', 'opening_price',  'high_price',   'low_price',  'trade_price',  'prev_closing_price', 'change',  'change_price',  'signed_change_price',  'change_rate',  'signed_change_rate',  'trade_volume',  'acc_trade_volume',  'acc_trade_volume_24h',  'acc_trade_price',  'acc_trade_price_24h',  'trade_date', 'ask_bid',  'acc_ask_volume',  'acc_bid_volume',  'highest_52_week_price', 'highest_52_week_date',  'lowest_52_week_price', 'lowest_52_week_date', 'market_state',  'is_trading_suspended'  ], keep='first', inplace=True)\n",
    "    # df.drop_duplicates(subset=[ 'type_websocket', 'total_ask_size',  'total_bid_size',  'orderbook_ap_0',  'orderbook_as_0',  'orderbook_bp_0',  'orderbook_bs_0',  'orderbook_ap_1',  'orderbook_as_1',  'orderbook_bp_1',  'orderbook_bs_1',  'orderbook_ap_2',  'orderbook_as_2',  'orderbook_bp_2',  'orderbook_bs_2',  'orderbook_ap_3',  'orderbook_as_3',  'orderbook_bp_3',  'orderbook_bs_3',  'orderbook_ap_4',  'orderbook_as_4',  'orderbook_bp_4',  'orderbook_bs_4',  'orderbook_ap_5',  'orderbook_as_5',  'orderbook_bp_5',  'orderbook_bs_5',  'orderbook_ap_6',  'orderbook_as_6',  'orderbook_bp_6',  'orderbook_bs_6',  'orderbook_ap_7',  'orderbook_as_7',  'orderbook_bp_7',  'orderbook_bs_7',  'orderbook_ap_8',  'orderbook_as_8',  'orderbook_bp_8',  'orderbook_bs_8',  'orderbook_ap_9',  'orderbook_as_9',  'orderbook_bp_9',  'orderbook_bs_9',  'orderbook_ap_10',  'orderbook_as_10',  'orderbook_bp_10',  'orderbook_bs_10',  'orderbook_ap_11',  'orderbook_as_11',  'orderbook_bp_11',  'orderbook_bs_11',  'orderbook_ap_12',  'orderbook_as_12',  'orderbook_bp_12',  'orderbook_bs_12',  'orderbook_ap_13',  'orderbook_as_13',  'orderbook_bp_13',  'orderbook_bs_13',  'orderbook_ap_14',  'orderbook_as_14',  'orderbook_bp_14',  'orderbook_bs_14'    ], keep='first', inplace=True)\n",
    "    df.drop_duplicates(subset=[ 'type_websocket',  'opening_price',  'high_price',   'low_price',  'trade_price',  'prev_closing_price', 'change',  'change_price',  'signed_change_price',  'change_rate',  'signed_change_rate',  'trade_volume',  'acc_trade_volume',   'acc_trade_price',    'trade_date', 'ask_bid',  'acc_ask_volume',  'acc_bid_volume',  'highest_52_week_price', 'highest_52_week_date',  'lowest_52_week_price', 'lowest_52_week_date', 'market_state',  'is_trading_suspended'   , 'total_ask_size',  'total_bid_size',  'orderbook_ap_0',  'orderbook_as_0',  'orderbook_bp_0',  'orderbook_bs_0',  'orderbook_ap_1',  'orderbook_as_1',  'orderbook_bp_1',  'orderbook_bs_1',  'orderbook_ap_2',  'orderbook_as_2',  'orderbook_bp_2',  'orderbook_bs_2',  'orderbook_ap_3',  'orderbook_as_3',  'orderbook_bp_3',  'orderbook_bs_3',  'orderbook_ap_4',  'orderbook_as_4',  'orderbook_bp_4',  'orderbook_bs_4',  'orderbook_ap_5',  'orderbook_as_5',  'orderbook_bp_5',  'orderbook_bs_5',  'orderbook_ap_6',  'orderbook_as_6',  'orderbook_bp_6',  'orderbook_bs_6',  'orderbook_ap_7',  'orderbook_as_7',  'orderbook_bp_7',  'orderbook_bs_7',  'orderbook_ap_8',  'orderbook_as_8',  'orderbook_bp_8',  'orderbook_bs_8',  'orderbook_ap_9',  'orderbook_as_9',  'orderbook_bp_9',  'orderbook_bs_9',  'orderbook_ap_10',  'orderbook_as_10',  'orderbook_bp_10',  'orderbook_bs_10',  'orderbook_ap_11',  'orderbook_as_11',  'orderbook_bp_11',  'orderbook_bs_11',  'orderbook_ap_12',  'orderbook_as_12',  'orderbook_bp_12',  'orderbook_bs_12',  'orderbook_ap_13',  'orderbook_as_13',  'orderbook_bp_13',  'orderbook_bs_13',  'orderbook_ap_14',  'orderbook_as_14',  'orderbook_bp_14',  'orderbook_bs_14'    ], keep='first', inplace=True, ignore_index=True)\n",
    "\n",
    "    print('Complete: Drop duplicates')\n",
    "    \n",
    "    dask_df = dd.from_pandas(df, npartitions=1331)\n",
    "    print('Complete: PD into DD')\n",
    "\n",
    "    ### Write the sorted DataFrame to a new dask-type csv file\n",
    "    dask_df.to_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\s_t_d_data_{}_{}.csv\".format(coin, data_id), index=False)\n",
    "    print('Complete: File Saving')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4127ea48",
   "metadata": {},
   "source": [
    "### Ex) 파일을 열어서 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c0d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "new_dir = 'D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'\n",
    "os.chdir(new_dir)\n",
    "import numpy as np\n",
    "import csv\n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "dtype={'ask_bid': 'object',\n",
    "       'change': 'object',\n",
    "       'highest_52_week_date': 'object',\n",
    "       'lowest_52_week_date': 'object',\n",
    "       'market_state': 'object',\n",
    "       'market_warning': 'object',\n",
    "       'orderbook_ap_0': 'float32',\n",
    "       'orderbook_ap_1': 'float32',\n",
    "       'orderbook_ap_2': 'float32',\n",
    "       'orderbook_ap_3': 'float32',\n",
    "       'orderbook_bp_0': 'float32',\n",
    "       'orderbook_bp_1': 'float32',\n",
    "       'orderbook_bp_10': 'float32',\n",
    "       'orderbook_bp_11': 'float32',\n",
    "       'orderbook_bp_12': 'float32',\n",
    "       'orderbook_bp_13': 'float32',\n",
    "       'orderbook_bp_14': 'float32',\n",
    "       'orderbook_bp_2': 'float32',\n",
    "       'orderbook_bp_3': 'float32',\n",
    "       'orderbook_bp_4': 'float32',\n",
    "       'orderbook_bp_5': 'float32',\n",
    "       'orderbook_bp_6': 'float32',\n",
    "       'orderbook_bp_7': 'float32',\n",
    "       'orderbook_bp_8': 'float32',\n",
    "       'orderbook_bp_9': 'float32',\n",
    "       'stream_type': 'object',\n",
    "       'trade_time': 'object'}\n",
    "\n",
    "data_id = 202303271051 # 해당 틱데이터 커버 기간: 22년 12월 16일 오후 9시 ~ 23년 2월 26일 오전 4시\n",
    "coin_list = [ 'BTC']  # \n",
    "\n",
    "for coin in coin_list:\n",
    "\n",
    "    # Read dask \"part\" failes\n",
    "    part_files_path =  \"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\s_t_d_data_{}_{}.csv\\\\\".format(coin, data_id)\n",
    "    df = dd.read_csv( part_files_path + '1000.part', dtype=dtype)    \n",
    "\n",
    "print(df.head(1000).to_string(max_rows=None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d5e5e",
   "metadata": {},
   "source": [
    "## 1-2(b): 각 코인의 틱데이터를 10분씩 묶는 전처리 작업 (rolling window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85063092",
   "metadata": {},
   "source": [
    "### Setting & Running => Output: {}_sum_both_10m.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5d402f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hhkim\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  \n",
      "c:\\Users\\hhkim\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###interval: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hhkim\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "\n",
    "working_directory = 'D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'\n",
    "os.chdir(working_directory)\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "from contextlib import contextmanager  # 컨텍스트 관리자를 사용하기 위한 contextlib 모듈을 가져옵니다.\n",
    "import time  # 시간 관련 기능을 사용하기 위한 time 모듈을 가져옵니다.\n",
    "\n",
    "\n",
    "def log_return(series: np.ndarray):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series ** 2))\n",
    "\n",
    "def flatten_name(prefix, src_names):\n",
    "    ret = []\n",
    "    for c in src_names:\n",
    "        if c[0] in ['time_id', 'stock_id']:\n",
    "            ret.append(c[0])\n",
    "        else:\n",
    "            ret.append('.'.join([prefix] + list(c)))\n",
    "    return ret\n",
    "\n",
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - s\n",
    "    print(f'[{name}] {elapsed: .3f}초')\n",
    "\n",
    "\n",
    "dtype={'ask_bid': 'object',\n",
    "       'change': 'object',\n",
    "       'highest_52_week_date': 'object',\n",
    "       'lowest_52_week_date': 'object',\n",
    "       'market_state': 'object',\n",
    "       'market_warning': 'object',\n",
    "       'orderbook_ap_0': 'float32',\n",
    "       'orderbook_ap_1': 'float32',\n",
    "       'orderbook_ap_2': 'float32',\n",
    "       'orderbook_ap_3': 'float32',\n",
    "       'orderbook_bp_0': 'float32',\n",
    "       'orderbook_bp_1': 'float32',\n",
    "       'orderbook_bp_10': 'float32',\n",
    "       'orderbook_bp_11': 'float32',\n",
    "       'orderbook_bp_12': 'float32',\n",
    "       'orderbook_bp_13': 'float32',\n",
    "       'orderbook_bp_14': 'float32',\n",
    "       'orderbook_bp_2': 'float32',\n",
    "       'orderbook_bp_3': 'float32',\n",
    "       'orderbook_bp_4': 'float32',\n",
    "       'orderbook_bp_5': 'float32',\n",
    "       'orderbook_bp_6': 'float32',\n",
    "       'orderbook_bp_7': 'float32',\n",
    "       'orderbook_bp_8': 'float32',\n",
    "       'orderbook_bp_9': 'float32',\n",
    "       'stream_type': 'object',\n",
    "       'trade_time': 'object'}\n",
    "\n",
    "data_id = 202303271051 # 해당 틱데이터 커버 기간: 22년 12월 16일 오후 9시 ~ 23년 2월 26일 오전 4시\n",
    "# coin_list = ['BTC']  # \n",
    "coin_list = ['BTC', 'ETH', 'DOGE', 'XRP']  # \n",
    "# coin = 'ETH'\n",
    "\n",
    "for coin in coin_list:\n",
    "\n",
    "    # Read dask \"part\" failes\n",
    "    part_files_path =  working_directory + \"output\\\\s_t_d_data_{}_{}.csv\\\\\".format(coin, data_id)\n",
    "    df = dd.read_csv( part_files_path + '*.part', dtype=dtype)    \n",
    "\n",
    "    # 티커 데이터와 오더북 데이터를 구분하여 저장. \n",
    "    df_ticker = df[df['type_websocket']==\"ticker\"]\n",
    "    df_orderbook = df[df['type_websocket']==\"orderbook\"]\n",
    "    df_ticker = df_ticker.drop(\"Unnamed: 0\", axis=1)\n",
    "    df_features_ticker = feature_engineering_ticker(df_ticker)\n",
    "    df_features_ticker.to_csv(working_directory + \"output\\\\{}_sum_ticker_10m.csv\".format(coin), index=False)\n",
    "\n",
    "    df_features_orderbook = feature_engineering_orderbook(df_orderbook)\n",
    "    df_features_orderbook.to_csv(working_directory + \"output\\\\{}_sum_orderbook_10m.csv\".format(coin), index=False)\n",
    "\n",
    "    ## df_features_ticker & df_features_orderbook 두개를 concat 하자.\n",
    "    df_features_orderbook = df_features_orderbook.drop(columns = ['window_end', 'time_id'])\n",
    "    combined_result_df = pd.merge(df_features_ticker, df_features_orderbook, on='window_start', suffixes=('_ticker', '_orderbook'))\n",
    "\n",
    "    ## 추가변수 만들기\n",
    "    combined_result_df['realized_volatility_30s'] = np.nan\n",
    "    combined_result_df['dv4_realized_volatility_30s'] = np.nan\n",
    "    combined_result_df['prices_30s_for_NN_onlyprices'] = np.nan\n",
    "    combined_result_df['prices_30s_for_NN_onlyprices'] = combined_result_df['prices_30s_for_NN_onlyprices'].astype(str)\n",
    "\n",
    "    num_rows = combined_result_df.shape[0]\n",
    "    for i in range(num_rows):\n",
    "        temp_series = combined_result_df['prices_30s_for_NN'].iloc[i]\n",
    "        # Split the string into a list based on the \"\\n\" marker\n",
    "        # tokenized_list = temp_series.split(\"\\n\")\n",
    "        # # Remove any leading or trailing whitespace from each element\n",
    "        # tokenized_list = [item.strip() for item in tokenized_list]\n",
    "        # # Assuming you already have a list named \"numbers\"\n",
    "        # if tokenized_list:\n",
    "        #     tokenized_list.pop(0)  # This will remove the first item in the list\n",
    "        #     tokenized_list.pop(-1)  # This will remove the first item in the list\n",
    "        # # Extract the return numbers using list comprehension\n",
    "        # return_numbers = [float(item.split()[-1]) for item in tokenized_list]\n",
    "        return_numbers = temp_series.tolist()\n",
    "        temp_len = len(return_numbers)\n",
    "        squared_numbers = [x ** 2 for x in return_numbers]\n",
    "        sum_of_squared_numbers = sum(squared_numbers)\n",
    "        mean_of_sum_of_squared_numbers = sum_of_squared_numbers/temp_len\n",
    "        temp_vol = mean_of_sum_of_squared_numbers ** 0.5\n",
    "        combined_result_df.at[i, 'realized_volatility_30s'] = temp_vol\n",
    "        return_numbers_str = ' '.join(str(num) for num in return_numbers)\n",
    "        combined_result_df.at[i, 'prices_30s_for_NN_onlyprices'] = return_numbers_str\n",
    "\n",
    "    combined_result_df['dv4_realized_volatility_30s'] = combined_result_df['realized_volatility_30s'].shift(-10)\n",
    "\n",
    "    ## Additional feature engineering (ex: TVLQ)\n",
    "    combined_result_df['trade.tau'] = np.sqrt(1 / combined_result_df['num_trades'])\n",
    "\n",
    "    combined_result_df['tvpl1'] = combined_result_df['trade_vol'] / combined_result_df['liq_last_1']\n",
    "    combined_result_df['tvpl2'] = combined_result_df['trade_vol'] / combined_result_df['liq_last_2'] \n",
    "    combined_result_df['tvpl5'] = combined_result_df['trade_vol'] / combined_result_df['liq_last_5'] \n",
    "    combined_result_df['tvpl10'] = combined_result_df['trade_vol'] / combined_result_df['liq_last_10'] \n",
    "    combined_result_df['tvpl15'] = combined_result_df['trade_vol'] / combined_result_df['liq_last_15']\n",
    "\n",
    "    combined_result_df['tvpl_ep1'] = combined_result_df['trade_vol'] / combined_result_df['ep_liq_1']\n",
    "    combined_result_df['tvpl_ep2'] = combined_result_df['trade_vol'] / combined_result_df['ep_liq_2']\n",
    "    combined_result_df['tvpl_ep5'] = combined_result_df['trade_vol'] / combined_result_df['ep_liq_5']\n",
    "    combined_result_df['tvpl_ep10'] = combined_result_df['trade_vol'] / combined_result_df['ep_liq_10']\n",
    "    combined_result_df['tvpl_ep15'] = combined_result_df['trade_vol'] / combined_result_df['ep_liq_15']\n",
    "\n",
    "    # 아래 작업시 빠른 작업을 위해 10분씩 정리된 dataframe을 중간 저장.\n",
    "    combined_result_df.to_csv(working_directory + \"output\\\\{}_sum_both_10m.csv\".format(coin), index=False)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b80c566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_rows = combined_result_df.shape[0]\n",
    "# for i in range(num_rows):\n",
    "#     temp_series = combined_result_df['prices_30s_for_NN'].iloc[i]\n",
    "#     # Split the string into a list based on the \"\\n\" marker\n",
    "#     # tokenized_list = temp_series.split(\"\\n\")\n",
    "#     # # Remove any leading or trailing whitespace from each element\n",
    "#     # tokenized_list = [item.strip() for item in tokenized_list]\n",
    "#     # # Assuming you already have a list named \"numbers\"\n",
    "#     # if tokenized_list:\n",
    "#     #     tokenized_list.pop(0)  # This will remove the first item in the list\n",
    "#     #     tokenized_list.pop(-1)  # This will remove the first item in the list\n",
    "#     # # Extract the return numbers using list comprehension\n",
    "#     # return_numbers = [float(item.split()[-1]) for item in tokenized_list]\n",
    "#     return_numbers = temp_series.tolist()\n",
    "#     temp_len = len(return_numbers)\n",
    "#     squared_numbers = [x ** 2 for x in return_numbers]\n",
    "#     sum_of_squared_numbers = sum(squared_numbers)\n",
    "#     mean_of_sum_of_squared_numbers = sum_of_squared_numbers/temp_len\n",
    "#     temp_vol = mean_of_sum_of_squared_numbers ** 0.5\n",
    "#     combined_result_df.at[i, 'realized_volatility_30s'] = temp_vol\n",
    "#     return_numbers_str = ' '.join(str(num) for num in return_numbers)\n",
    "#     combined_result_df.at[i, 'prices_30s_for_NN_onlyprices'] = return_numbers_str\n",
    "\n",
    "# combined_result_df['dv4_realized_volatility_30s'] = combined_result_df['realized_volatility_30s'].shift(-10)\n",
    "\n",
    "# ## Additional feature engineering (ex: TVLQ)\n",
    "# combined_result_df['tvpl'] = combined_result_df['trade_vol'] / combined_result_df['liq_last_2']\n",
    "# combined_result_df['tvpl_epliq5'] = combined_result_df['trade_vol'] / combined_result_df['ep_liq_5']\n",
    "\n",
    "# # 아래 작업시 빠른 작업을 위해 10분씩 정리된 dataframe을 중간 저장.\n",
    "# combined_result_df.to_csv(working_directory + \"output\\\\{}_sum_both_10m.csv\".format(coin), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90799563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_series.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444ba768",
   "metadata": {},
   "source": [
    "### Feature Engineering (TICKER 부문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b7ce827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_ticker(df_ticker):\n",
    "\n",
    "    df_ticker_pd = df_ticker.compute()\n",
    "\n",
    "    df_ticker_pd['datetime'] = pd.to_datetime(df_ticker_pd['datetime'])\n",
    "    df_ticker_pd['log_return'] = df_ticker_pd['trade_price'].pct_change().apply(lambda x: pd.np.log(1 + x))\n",
    "\n",
    "    # # signed volume (to calculate volume power)\n",
    "    # df_ticker_pd['ask_bid'] = df_ticker_pd['ask_bid'].map({'ASK': -1, 'BID': 1})\n",
    "    # df_ticker_pd['signed_volume'] = df_ticker_pd['trade_volume'] * df_ticker_pd['ask_bid'] \n",
    "\n",
    "    # Set 'timestamp' as the index for resampling\n",
    "    df_ticker_pd.set_index('datetime', inplace=True)\n",
    "    # Resample data into 10-minute bins and take the last value in each bin\n",
    "    df_ticker_pd_1m = df_ticker_pd['trade_price'].resample('1T').last()\n",
    "    # Shift the 1-minute prices one step forward to align with the original seconds data\n",
    "    df_ticker_pd_1m = df_ticker_pd_1m.shift(1)\n",
    "    # Now, reindex the original DataFrame with 10-minute frequency and forward fill the 'ending_price' values\n",
    "    df_ticker_pd['ending_price_b1m'] = df_ticker_pd_1m.reindex(df_ticker_pd.index, method='ffill') # 이를 이용해서 10분 동안 얼마나 하락하는가 확인.\n",
    "    # Reset the index to get the DataFrame with the original 'datetime', 'trade_price', and 'ending_price' columns\n",
    "    df_ticker_pd.reset_index(inplace=True)\n",
    "    # datetime을 1분 단위로 정리하는 변수 (dt_1m)을 만듬 (이는 데이터를 '정리'하는 데에 활용될 단위임)\n",
    "    df_ticker_pd['dt_1m'] = df_ticker_pd['datetime'].dt.floor('1min').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Set 'dt_1m' as the index for easier resampling\n",
    "    df_ticker_pd['dt_1m'] = pd.to_datetime(df_ticker_pd['dt_1m'])\n",
    "    df_ticker_pd.set_index('dt_1m', inplace=True, drop = False)\n",
    "    # Define the size of the moving window (10 minutes in this case)\n",
    "    window_size = pd.Timedelta(minutes=10)\n",
    "\n",
    "    # Create an empty list to store features for each rolling window\n",
    "    # realized_volatility = []\n",
    "    realized_volatility_mean0 = []\n",
    "    realized_up_volatility_mean0 = []\n",
    "    realized_down_volatility_mean0 = []\n",
    "    num_trades = []\n",
    "    lowest_return = []\n",
    "    trade_vol = []\n",
    "    volume_power = []\n",
    "    highest_return = []\n",
    "    high_low_gap = []\n",
    "    end_price = []\n",
    "    # prices_30s_for_NN = []\n",
    "    num_of_trades = []\n",
    "\n",
    "    temp_index = df_ticker_pd.index.unique().tolist()\n",
    "\n",
    "    # Iterate through each time window using the rolling method\n",
    "    for window_start in temp_index[:-10]:\n",
    "        window_end = window_start + window_size\n",
    "\n",
    "        # print(\"window_start:\", window_start, \" & window_end:\", window_end)\n",
    "\n",
    "        # Filter the data for each rolling window\n",
    "        rolling_window_data = df_ticker_pd.loc[(df_ticker_pd.index >= window_start) & (df_ticker_pd.index < window_end)]\n",
    "        previous_price = rolling_window_data['ending_price_b1m'].iloc[0] # 10분 bin이 시작하기 직전 마지막 trade price 값을 의미.\n",
    "        rolling_window_data['log_return_from_pp'] = np.log(rolling_window_data['trade_price'] / previous_price)\n",
    "        \n",
    "        # VARIABLE: realized_volatility \n",
    "        # Calculate the returns (percentage change) for each tick in the rolling window\n",
    "        returns = rolling_window_data['log_return']\n",
    "        # Calculate the volatility as the standard deviation of returns\n",
    "        # volatility = returns.std()\n",
    "        # Append the average volatility to the list\n",
    "        # realized_volatility.append(volatility)\n",
    "\n",
    "        # returns_from_pp = rolling_window_data['log_return_from_pp']\n",
    "        squared_returns = np.square(returns)\n",
    "        average_squared_returns = np.mean(squared_returns)\n",
    "        realized_volatility_mean0.append(np.sqrt(average_squared_returns))\n",
    "\n",
    "        squared_returns_up = np.square(returns[returns >= 0])\n",
    "        squared_returns_down = np.square(returns[returns < 0])\n",
    "        average_squared_returns_up = np.mean(squared_returns_up)\n",
    "        average_squared_returns_down = np.mean(squared_returns_down)\n",
    "        realized_up_volatility_mean0.append(np.sqrt(average_squared_returns_up))\n",
    "        realized_down_volatility_mean0.append(np.sqrt(average_squared_returns_down))\n",
    "\n",
    "        # VARIABLE: number of trades \n",
    "        # Calculate the volatility as the standard deviation of returns\n",
    "        n_trades = rolling_window_data.shape[0]\n",
    "        # Append n_trades to the list\n",
    "        num_trades.append(n_trades)\n",
    "\n",
    "        # VARIABLE: lowest return\n",
    "        # Calculate the lowest return\n",
    "        l_return = rolling_window_data['log_return_from_pp'].min()\n",
    "        # Append n_trades to the list\n",
    "        lowest_return.append(l_return)\n",
    "\n",
    "        # VARIABLE: highest return\n",
    "        h_return = rolling_window_data['log_return_from_pp'].max()\n",
    "        highest_return.append(h_return) \n",
    "\n",
    "        # VARIABLE: high_low_gap\n",
    "        hl_gap = h_return - l_return\n",
    "        high_low_gap.append(hl_gap)\n",
    "\n",
    "        # VARIABLE: total trade volume\n",
    "        # Calculate the trading volume during the 10m\n",
    "        tv = rolling_window_data['trade_volume'].sum()\n",
    "        # Append n_trades to the list\n",
    "        trade_vol.append(tv)\n",
    "\n",
    "        # VARIABLE: volume power\n",
    "        # Calculate the trading volume during the 10m\n",
    "        ask_sum = rolling_window_data[rolling_window_data['ask_bid'] == 'ASK']['trade_volume'].sum()\n",
    "        bid_sum = rolling_window_data[rolling_window_data['ask_bid'] == 'BID']['trade_volume'].sum()\n",
    "        vp = bid_sum - ask_sum\n",
    "        # if ask_sum >= bid_sum:\n",
    "        #     vp = -ask_sum/bid_sum\n",
    "        # elif bid_sum > ask_sum:\n",
    "        #     vp = bid_sum/ask_sum\n",
    "        # Append n_trades to the list\n",
    "        volume_power.append(vp)\n",
    "\n",
    "        ep = rolling_window_data['trade_price'].iloc[-1]\n",
    "        end_price.append(ep)\n",
    "\n",
    "        # VARIABLE: prices_30s_for_NN\n",
    "        # rolling_window_data.set_index('datetime', inplace=True, drop = False)\n",
    "        # temp = rolling_window_data['log_return_from_pp'].resample('30S').last()\n",
    "        # prices_30s_for_NN.append(temp)\n",
    "\n",
    "    # Create a new DataFrame to store the results\n",
    "    result_df = pd.DataFrame({\n",
    "        'window_start': temp_index[:-10],\n",
    "        'window_end': temp_index[10:],\n",
    "        # 'realized_volatility': realized_volatility,\n",
    "        'realized_volatility_mean0':realized_volatility_mean0,\n",
    "        'realized_up_volatility_mean0':realized_up_volatility_mean0,\n",
    "        'realized_down_volatility_mean0':realized_down_volatility_mean0,\n",
    "        'num_trades': num_trades,\n",
    "        'lowest_return': lowest_return,\n",
    "        'highest_return': highest_return,\n",
    "        'high_low_gap': high_low_gap,\n",
    "        'trade_vol': trade_vol,\n",
    "        'volume_power': volume_power,\n",
    "        'end_price': end_price,\n",
    "        # 'prices_30s_for_NN': prices_30s_for_NN,\n",
    "    })\n",
    "\n",
    "    # Reset the index of the result DataFrame\n",
    "    result_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # 10분동안의 return을 안 만들었었네//\n",
    "    result_df['return'] = np.log(result_df['end_price'].shift(-10) / result_df['end_price'])\n",
    "\n",
    "    # time_id 만들어주기.\n",
    "    result_df['window_start'] = pd.to_datetime(result_df['window_start'])\n",
    "    result_df['time_id'] = result_df['window_start'].dt.floor('1min').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    result_df['time_id'] = pd.to_datetime(result_df['time_id'])\n",
    "\n",
    "    # Bolinger Band \n",
    "    result_df['BB_width_w20'] = result_df['end_price'].rolling(20).std()*4\n",
    "    result_df['BB_width_w40'] = result_df['end_price'].rolling(40).std()*4\n",
    "    result_df['BB_width_w10'] = result_df['end_price'].rolling(10).std()*4\n",
    "\n",
    "    # 종속변수 만들기 (향후 10분의 realized vol / lowest return)\n",
    "    df_ticker_pd_1m = df_ticker_pd_1m.shift(1)\n",
    "    # result_df['dv1_realized_volatility'] = result_df['realized_volatility'].shift(-10)\n",
    "    result_df['dv5_realized_volatility_mean0'] = result_df['realized_volatility_mean0'].shift(-10)\n",
    "    result_df['dv2_lowest_return'] = result_df['lowest_return'].shift(-10)\n",
    "    result_df['dv3_highest_return'] = result_df['highest_return'].shift(-10)\n",
    "\n",
    "    ##### 450/300/150초 기준 변수 만들기.\n",
    "\n",
    "    # Iterate through each time window using the rolling method\n",
    "\n",
    "    time_intervals = [150, 300, 450]\n",
    "\n",
    "    for interval in time_intervals:\n",
    "\n",
    "        # realized_volatility = []\n",
    "        realized_volatility_mean0 = []\n",
    "        num_trades = []\n",
    "        lowest_return = []\n",
    "        highest_return = []\n",
    "        trade_vol = []\n",
    "        high_low_gap = []\n",
    "        volume_power = []\n",
    "\n",
    "        print(\"###interval:\", interval)\n",
    "        for window_start in temp_index[:-10]:\n",
    "            window_end = window_start + window_size\n",
    "\n",
    "            # Filter the data for each rolling window\n",
    "            rolling_window_data = df_ticker_pd.loc[(df_ticker_pd['datetime'] >= window_start) & (df_ticker_pd['datetime'] < window_start + pd.Timedelta(seconds=interval))]\n",
    "            previous_price = rolling_window_data['ending_price_b1m'].iloc[0] # 10분 bin이 시작하기 직전 마지막 trade price 값을 의미.\n",
    "            rolling_window_data['log_return_from_pp'] = np.log(rolling_window_data['trade_price'] / previous_price)\n",
    "            \n",
    "            # VARIABLE: realized_volatility \n",
    "            returns = rolling_window_data['log_return']\n",
    "            # volatility = returns.std()\n",
    "            # realized_volatility.append(volatility)\n",
    "\n",
    "            squared_returns = np.square(returns)\n",
    "            average_squared_returns = np.mean(squared_returns)\n",
    "            realized_volatility_mean0.append(np.sqrt(average_squared_returns))\n",
    "\n",
    "            # VARIABLE: number of trades \n",
    "            n_trades = rolling_window_data.shape[0]\n",
    "            num_trades.append(n_trades)\n",
    "\n",
    "            # VARIABLE: lowest return\n",
    "            l_return = rolling_window_data['log_return_from_pp'].min()\n",
    "            lowest_return.append(l_return)\n",
    "\n",
    "            # VARIABLE: highest return\n",
    "            h_return = rolling_window_data['log_return_from_pp'].max()\n",
    "            highest_return.append(h_return) \n",
    "\n",
    "            # VARIABLE: high_low_gap\n",
    "            hl_gap = h_return - l_return\n",
    "            high_low_gap.append(hl_gap)\n",
    "\n",
    "            # VARIABLE: total trade volume\n",
    "            # Calculate the trading volume during the 10m\n",
    "            tv = rolling_window_data['trade_volume'].sum()\n",
    "            # Append n_trades to the list\n",
    "            trade_vol.append(tv)\n",
    "\n",
    "            # VARIABLE: volume power\n",
    "            ask_sum = rolling_window_data[rolling_window_data['ask_bid'] == 'ASK']['trade_volume'].sum()\n",
    "            bid_sum = rolling_window_data[rolling_window_data['ask_bid'] == 'BID']['trade_volume'].sum()\n",
    "            vp = bid_sum-ask_sum\n",
    "            # if ask_sum >= bid_sum:\n",
    "            #     vp = -ask_sum/bid_sum\n",
    "            # elif bid_sum > ask_sum:\n",
    "            #     vp = bid_sum/ask_sum\n",
    "            volume_power.append(vp)\n",
    "\n",
    "            ep = rolling_window_data['trade_price'].iloc[-1]\n",
    "            end_price.append(ep)\n",
    "\n",
    "\n",
    "        # Create a new DataFrame to store the results\n",
    "        d = pd.DataFrame({\n",
    "            'window_start': temp_index[:-10],\n",
    "            f'window_end_{interval}': temp_index[10:],\n",
    "            # f'realized_volatility_{interval}': realized_volatility,\n",
    "            f'realized_volatility_mean0_{interval}': realized_volatility_mean0,\n",
    "            f'num_trades_{interval}': num_trades,\n",
    "            f'lowest_return_{interval}': lowest_return,\n",
    "            f'highest_return_{interval}': highest_return,\n",
    "            f'high_low_gap_{interval}': high_low_gap,\n",
    "            f'trade_vol_{interval}': trade_vol,\n",
    "            f'volume_power_{interval}': volume_power\n",
    "        })\n",
    "        # print(\"###result_df:\", result_df.head(3))\n",
    "        # print(\"###result_df_{}:\".format(interval), result_df_150.head(3))\n",
    "        # result_df = pd.merge(result_df, result_df_150, on = 'window_start', how='inner')\n",
    "        # print(\"###merged result_df:\", result_df.head(3))\n",
    "        result_df = pd.merge(result_df, d, on = 'window_start', how='inner')\n",
    "\n",
    "    # agg = pd.merge(agg, d, on='time_id', how='left')  # 계산된 통계량을 기존 데이터프레임에 병합합니다.\n",
    "\n",
    "    # 첫 1분 및 마지막 11분은 데이터가 complete 하지 않을 것이므로 삭제.\n",
    "    start_time = result_df['time_id'].iloc[0]\n",
    "    end_time = result_df['time_id'].iloc[-11]\n",
    "    result_df_flt = result_df[(result_df['time_id'] > start_time) & (result_df['time_id'] < end_time)]\n",
    "\n",
    "    return result_df_flt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a532a71",
   "metadata": {},
   "source": [
    "### Feature Engineering (ORDERBOOK 부문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad2c89cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_orderbook(df_orderbook):\n",
    "\n",
    "    df_orderbook_pd2 = df_orderbook.compute() # dask dataframe => pandas dataframe\n",
    "\n",
    "    # for testing:\n",
    "    # df_orderbook_pd2 = df_orderbook_pd.iloc[:]\n",
    "\n",
    "    df_orderbook_pd2['datetime'] = pd.to_datetime(df_orderbook_pd2['datetime'])\n",
    "    # datetime을 1분 단위로 정리하는 변수 (dt_1m)을 만듬 (이는 데이터를 '정리'하는 데에 활용될 단위임)\n",
    "    df_orderbook_pd2['dt_1m'] = df_orderbook_pd2['datetime'].dt.floor('1min').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df_orderbook_pd2['dt_1m'] = pd.to_datetime(df_orderbook_pd2['dt_1m'])\n",
    "    df_orderbook_pd2.set_index('dt_1m', inplace=True, drop = False)\n",
    "\n",
    "    # Define the size of the moving window (10 minutes in this case)\n",
    "    window_size = pd.Timedelta(minutes=10)\n",
    "\n",
    "    # Create an empty list to store features for each rolling window\n",
    "    liq_last_1 = []\n",
    "    liq_last_2 = []\n",
    "    liq_last_5 = []\n",
    "    liq_last_10 = []\n",
    "    liq_last_15 = []\n",
    "    ep_liq_1 =[]\n",
    "    ep_liq_2 =[]\n",
    "    ep_liq_5 =[]\n",
    "    ep_liq_10 =[]\n",
    "    ep_liq_15 =[]\n",
    "    bidask_spread_0 = []\n",
    "    bidask_spread_1 = []\n",
    "    prices_30s_for_NN = []\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    temp_index = df_orderbook_pd2.index.unique().tolist()\n",
    "    # Iterate through each time window using the rolling method\n",
    "    for window_start in temp_index[:-10]:\n",
    "        window_end = window_start + window_size\n",
    "\n",
    "        # print(\"window_start:\", window_start, \" & window_end:\", window_end)\n",
    "\n",
    "        # Filter the data for each rolling window\n",
    "        rolling_window_data = df_orderbook_pd2.loc[(df_orderbook_pd2.index >= window_start) & (df_orderbook_pd2.index < window_end)]\n",
    "        # previous_price = rolling_window_data['ending_price_b1m'].iloc[0] # 10분 bin이 시작하기 직전 마지막 trade price 값을 의미.\n",
    "        # rolling_window_data['log_return_from_pp'] = np.log(rolling_window_data['trade_price'] / previous_price)\n",
    "\n",
    "        # VARIABLE: liquidity measures\n",
    "        last_order_book = rolling_window_data.iloc[-1]\n",
    "        wap_1 = (last_order_book['orderbook_bp_0'] * last_order_book['orderbook_as_0'] + last_order_book['orderbook_ap_0'] * last_order_book['orderbook_bs_0'])/(last_order_book['orderbook_bs_0'] + last_order_book['orderbook_as_0'])\n",
    "        \n",
    "        liq_1 = 0\n",
    "        for i in range(1):\n",
    "            liq_1 += last_order_book[f'orderbook_bs_{i}']/(wap_1 - last_order_book[f'orderbook_bp_{i}']) + last_order_book[f'orderbook_as_{i}']/(last_order_book[f'orderbook_ap_{i}'] - wap_1)\n",
    "\n",
    "        liq_2 = 0\n",
    "        for i in range(2):\n",
    "            liq_2 += last_order_book[f'orderbook_bs_{i}']/(wap_1 - last_order_book[f'orderbook_bp_{i}']) + last_order_book[f'orderbook_as_{i}']/(last_order_book[f'orderbook_ap_{i}'] - wap_1)\n",
    "\n",
    "        liq_5 = 0\n",
    "        for i in range(5):\n",
    "            liq_5 += last_order_book[f'orderbook_bs_{i}']/(wap_1 - last_order_book[f'orderbook_bp_{i}']) + last_order_book[f'orderbook_as_{i}']/(last_order_book[f'orderbook_ap_{i}'] - wap_1)\n",
    "\n",
    "        liq_10 = 0\n",
    "        for i in range(10):\n",
    "            liq_10 += last_order_book[f'orderbook_bs_{i}']/(wap_1 - last_order_book[f'orderbook_bp_{i}']) + last_order_book[f'orderbook_as_{i}']/(last_order_book[f'orderbook_ap_{i}'] - wap_1)\n",
    "\n",
    "        liq_15 = 0\n",
    "        for i in range(15):\n",
    "            liq_15 += last_order_book[f'orderbook_bs_{i}']/(wap_1 - last_order_book[f'orderbook_bp_{i}']) + last_order_book[f'orderbook_as_{i}']/(last_order_book[f'orderbook_ap_{i}'] - wap_1)\n",
    "\n",
    "        liq_last_1.append(liq_1)\n",
    "        liq_last_2.append(liq_2)\n",
    "        liq_last_5.append(liq_5)\n",
    "        liq_last_10.append(liq_10)\n",
    "        liq_last_15.append(liq_15)\n",
    "\n",
    "        ep_wap_1 = (rolling_window_data['orderbook_bp_0'].mean() * rolling_window_data['orderbook_as_0'].mean() + rolling_window_data['orderbook_ap_0'].mean() * rolling_window_data['orderbook_bs_0'].mean())/(rolling_window_data['orderbook_bs_0'].mean() + rolling_window_data['orderbook_as_0'].mean())\n",
    "\n",
    "        entire_period_liq_1 = 0\n",
    "        for i in range(1):\n",
    "            entire_period_liq_1 += rolling_window_data[f'orderbook_bs_{i}'].mean()/(ep_wap_1 - rolling_window_data[f'orderbook_bp_{i}'].mean()) + rolling_window_data[f'orderbook_as_{i}'].mean()/(rolling_window_data[f'orderbook_ap_{i}'].mean() - ep_wap_1)\n",
    "\n",
    "        entire_period_liq_2 = 0\n",
    "        for i in range(2):\n",
    "            entire_period_liq_2 += rolling_window_data[f'orderbook_bs_{i}'].mean()/(ep_wap_1 - rolling_window_data[f'orderbook_bp_{i}'].mean()) + rolling_window_data[f'orderbook_as_{i}'].mean()/(rolling_window_data[f'orderbook_ap_{i}'].mean() - ep_wap_1)\n",
    "\n",
    "        entire_period_liq_5 = 0\n",
    "        for i in range(5):\n",
    "            entire_period_liq_5 += rolling_window_data[f'orderbook_bs_{i}'].mean()/(ep_wap_1 - rolling_window_data[f'orderbook_bp_{i}'].mean()) + rolling_window_data[f'orderbook_as_{i}'].mean()/(rolling_window_data[f'orderbook_ap_{i}'].mean() - ep_wap_1)\n",
    "\n",
    "        entire_period_liq_10 = 0\n",
    "        for i in range(10):\n",
    "            entire_period_liq_10 += rolling_window_data[f'orderbook_bs_{i}'].mean()/(ep_wap_1 - rolling_window_data[f'orderbook_bp_{i}'].mean()) + rolling_window_data[f'orderbook_as_{i}'].mean()/(rolling_window_data[f'orderbook_ap_{i}'].mean() - ep_wap_1)\n",
    "\n",
    "        entire_period_liq_15 = 0\n",
    "        for i in range(15):\n",
    "            entire_period_liq_15 += rolling_window_data[f'orderbook_bs_{i}'].mean()/(ep_wap_1 - rolling_window_data[f'orderbook_bp_{i}'].mean()) + rolling_window_data[f'orderbook_as_{i}'].mean()/(rolling_window_data[f'orderbook_ap_{i}'].mean() - ep_wap_1)\n",
    "\n",
    "        ep_liq_1.append(entire_period_liq_1)\n",
    "        ep_liq_2.append(entire_period_liq_2)\n",
    "        ep_liq_5.append(entire_period_liq_5)\n",
    "        ep_liq_10.append(entire_period_liq_10)\n",
    "        ep_liq_15.append(entire_period_liq_15)\n",
    "\n",
    "        # VARIABLE: bidask spread\n",
    "        ob_price_diff = []\n",
    "        for i in range(14):\n",
    "            ob_price_diff.append(last_order_book[f'orderbook_bp_{i}'] - last_order_book[f'orderbook_bp_{i+1}'])\n",
    "            ob_price_diff.append(last_order_book[f'orderbook_ap_{i+1}'] - last_order_book[f'orderbook_ap_{i}'])\n",
    "        tick_size = min(ob_price_diff)\n",
    "\n",
    "        # 마지막 1분 정보만 추출해서 bid ask spread의 평균을 구하자 (마지막 타이밍의 orderbook만 쓰는 것보다 낫지 않을까?)\n",
    "        last_1m = rolling_window_data.loc[(rolling_window_data.dt_1m == rolling_window_data['dt_1m'].iloc[-1])]\n",
    "\n",
    "        ba_sp_0 = (last_1m['orderbook_ap_0'] - last_1m['orderbook_bp_0'])/tick_size\n",
    "        bidask_spread_0.append(ba_sp_0.mean())\n",
    "        ba_sp_1 = (last_1m['orderbook_ap_1'] - last_1m['orderbook_bp_1'])/tick_size\n",
    "        bidask_spread_1.append(ba_sp_1.mean())\n",
    "        \n",
    "        # liq_1 = [last_order_book['orderbook_bs_0']/(wap_1 - last_order_book['orderbook_bp_0']) + last_order_book['orderbook_as_0']/(last_order_book['orderbook_ap_0'] - wap_1)]\n",
    "        # liq_1 = liq_1 +  \n",
    "\n",
    "        # 30s prices 를 orderbook에서 가져와보자:\n",
    "        returns_per_30s_temp = []\n",
    "        distance = pd.Timedelta(seconds=30)\n",
    "        first_order_book = rolling_window_data.iloc[0]\n",
    "        first_wap = (first_order_book['orderbook_bp_0'] * first_order_book['orderbook_as_0'] + first_order_book['orderbook_ap_0'] * first_order_book['orderbook_bs_0'])/(first_order_book['orderbook_bs_0'] + first_order_book['orderbook_as_0'])\n",
    "        for i in range(20):\n",
    "            temp_30s_window_data = df_orderbook_pd2.loc[(df_orderbook_pd2.index >= window_start) & (df_orderbook_pd2.index < window_start+distance*(i+1))]\n",
    "            temp_mid_order_book = temp_30s_window_data.iloc[-1]\n",
    "            mid_wap = (temp_mid_order_book['orderbook_bp_0'] * temp_mid_order_book['orderbook_as_0'] + temp_mid_order_book['orderbook_ap_0'] * temp_mid_order_book['orderbook_bs_0'])/(temp_mid_order_book['orderbook_bs_0'] + temp_mid_order_book['orderbook_as_0'])\n",
    "            temp_return = np.log(mid_wap/first_wap)\n",
    "            returns_per_30s_temp.append(temp_return)\n",
    "        prices_30s_for_NN.append(returns_per_30s_temp)\n",
    "\n",
    "    # Create a new DataFrame to store the results\n",
    "    result_df_orderbook = pd.DataFrame({\n",
    "        'window_start': temp_index[:-10],\n",
    "        'window_end': temp_index[10:],\n",
    "        'liq_last_1': liq_last_1,\n",
    "        'liq_last_2': liq_last_2,\n",
    "        'liq_last_5': liq_last_5,\n",
    "        'liq_last_10': liq_last_10,\n",
    "        'liq_last_15': liq_last_15,\n",
    "        'ep_liq_1': ep_liq_1,\n",
    "        'ep_liq_2': ep_liq_2,\n",
    "        'ep_liq_5': ep_liq_5,\n",
    "        'ep_liq_10': ep_liq_10,\n",
    "        'ep_liq_15': ep_liq_15,\n",
    "        'bidask_spread_0': bidask_spread_0,\n",
    "        'bidask_spread_1': bidask_spread_1,\n",
    "        'prices_30s_for_NN': prices_30s_for_NN\n",
    "    })\n",
    "\n",
    "    # Reset the index of the result DataFrame\n",
    "    result_df_orderbook.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # time_id 만들어주기.\n",
    "    result_df_orderbook['window_start'] = pd.to_datetime(result_df_orderbook['window_start'])\n",
    "    result_df_orderbook['time_id'] = result_df_orderbook['window_start'].dt.floor('1min').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    result_df_orderbook['time_id'] = pd.to_datetime(result_df_orderbook['time_id'])\n",
    "\n",
    "\n",
    "\n",
    "    ###### To-do : 450/300/150초 기준 변수 만들기.\n",
    "\n",
    "    # Create an empty list to store features for each rolling window\n",
    "\n",
    "    time_intervals = [150, 300, 450]\n",
    "\n",
    "    for interval in time_intervals:\n",
    "        liq_last_1 = []\n",
    "        liq_last_2 = []\n",
    "        liq_last_5 = []\n",
    "        liq_last_10 = []\n",
    "        liq_last_15 = []\n",
    "        ep_liq_1 =[]\n",
    "        ep_liq_2 =[]\n",
    "        ep_liq_5 =[]\n",
    "        ep_liq_10 =[]\n",
    "        ep_liq_15 =[]        \n",
    "        bidask_spread_0 = []\n",
    "        bidask_spread_1 = []\n",
    "\n",
    "        for window_start in temp_index[:-10]:\n",
    "            window_end = window_start + window_size\n",
    "\n",
    "            # Filter the data for each rolling window\n",
    "            rolling_window_data = df_orderbook_pd2.loc[(df_orderbook_pd2['datetime'] >= window_start) & (df_orderbook_pd2['datetime'] < window_start + pd.Timedelta(seconds=interval))]\n",
    "\n",
    "            # VARIABLE: liquidity measures\n",
    "            last_order_book = rolling_window_data.iloc[-1]\n",
    "            wap_1 = (last_order_book['orderbook_bp_0'] * last_order_book['orderbook_as_0'] + last_order_book['orderbook_ap_0'] * last_order_book['orderbook_bs_0'])/(last_order_book['orderbook_bs_0'] + last_order_book['orderbook_as_0'])\n",
    "            \n",
    "            liq_1 = 0\n",
    "            for i in range(1):\n",
    "                liq_1 += last_order_book[f'orderbook_bs_{i}']/(wap_1 - last_order_book[f'orderbook_bp_{i}']) + last_order_book[f'orderbook_as_{i}']/(last_order_book[f'orderbook_ap_{i}'] - wap_1)\n",
    "\n",
    "            liq_2 = 0\n",
    "            for i in range(2):\n",
    "                liq_2 += last_order_book[f'orderbook_bs_{i}']/(wap_1 - last_order_book[f'orderbook_bp_{i}']) + last_order_book[f'orderbook_as_{i}']/(last_order_book[f'orderbook_ap_{i}'] - wap_1)\n",
    "\n",
    "            liq_5 = 0\n",
    "            for i in range(5):\n",
    "                liq_5 += last_order_book[f'orderbook_bs_{i}']/(wap_1 - last_order_book[f'orderbook_bp_{i}']) + last_order_book[f'orderbook_as_{i}']/(last_order_book[f'orderbook_ap_{i}'] - wap_1)\n",
    "\n",
    "            liq_10 = 0\n",
    "            for i in range(10):\n",
    "                liq_10 += last_order_book[f'orderbook_bs_{i}']/(wap_1 - last_order_book[f'orderbook_bp_{i}']) + last_order_book[f'orderbook_as_{i}']/(last_order_book[f'orderbook_ap_{i}'] - wap_1)\n",
    "\n",
    "            liq_15 = 0\n",
    "            for i in range(15):\n",
    "                liq_15 += last_order_book[f'orderbook_bs_{i}']/(wap_1 - last_order_book[f'orderbook_bp_{i}']) + last_order_book[f'orderbook_as_{i}']/(last_order_book[f'orderbook_ap_{i}'] - wap_1)\n",
    "\n",
    "            liq_last_1.append(liq_1)\n",
    "            liq_last_2.append(liq_2)\n",
    "            liq_last_5.append(liq_5)\n",
    "            liq_last_10.append(liq_10)\n",
    "            liq_last_15.append(liq_15)\n",
    "\n",
    "            ep_wap_1 = (rolling_window_data['orderbook_bp_0'].mean() * rolling_window_data['orderbook_as_0'].mean() + rolling_window_data['orderbook_ap_0'].mean() * rolling_window_data['orderbook_bs_0'].mean())/(rolling_window_data['orderbook_bs_0'].mean() + rolling_window_data['orderbook_as_0'].mean())\n",
    "\n",
    "            entire_period_liq_1 = 0\n",
    "            for i in range(1):\n",
    "                entire_period_liq_1 += rolling_window_data[f'orderbook_bs_{i}'].mean()/(ep_wap_1 - rolling_window_data[f'orderbook_bp_{i}'].mean()) + rolling_window_data[f'orderbook_as_{i}'].mean()/(rolling_window_data[f'orderbook_ap_{i}'].mean() - ep_wap_1)\n",
    "\n",
    "            entire_period_liq_2 = 0\n",
    "            for i in range(2):\n",
    "                entire_period_liq_2 += rolling_window_data[f'orderbook_bs_{i}'].mean()/(ep_wap_1 - rolling_window_data[f'orderbook_bp_{i}'].mean()) + rolling_window_data[f'orderbook_as_{i}'].mean()/(rolling_window_data[f'orderbook_ap_{i}'].mean() - ep_wap_1)\n",
    "\n",
    "            entire_period_liq_5 = 0\n",
    "            for i in range(5):\n",
    "                entire_period_liq_5 += rolling_window_data[f'orderbook_bs_{i}'].mean()/(ep_wap_1 - rolling_window_data[f'orderbook_bp_{i}'].mean()) + rolling_window_data[f'orderbook_as_{i}'].mean()/(rolling_window_data[f'orderbook_ap_{i}'].mean() - ep_wap_1)\n",
    "\n",
    "            entire_period_liq_10 = 0\n",
    "            for i in range(10):\n",
    "                entire_period_liq_10 += rolling_window_data[f'orderbook_bs_{i}'].mean()/(ep_wap_1 - rolling_window_data[f'orderbook_bp_{i}'].mean()) + rolling_window_data[f'orderbook_as_{i}'].mean()/(rolling_window_data[f'orderbook_ap_{i}'].mean() - ep_wap_1)\n",
    "\n",
    "            entire_period_liq_15 = 0\n",
    "            for i in range(15):\n",
    "                entire_period_liq_15 += rolling_window_data[f'orderbook_bs_{i}'].mean()/(ep_wap_1 - rolling_window_data[f'orderbook_bp_{i}'].mean()) + rolling_window_data[f'orderbook_as_{i}'].mean()/(rolling_window_data[f'orderbook_ap_{i}'].mean() - ep_wap_1)\n",
    "\n",
    "            ep_liq_1.append(entire_period_liq_1)\n",
    "            ep_liq_2.append(entire_period_liq_2)\n",
    "            ep_liq_5.append(entire_period_liq_5)\n",
    "            ep_liq_10.append(entire_period_liq_10)\n",
    "            ep_liq_15.append(entire_period_liq_15)\n",
    "\n",
    "            # VARIABLE: bidask spread\n",
    "            ob_price_diff = []\n",
    "            for i in range(14):\n",
    "                ob_price_diff.append(last_order_book[f'orderbook_bp_{i}'] - last_order_book[f'orderbook_bp_{i+1}'])\n",
    "                ob_price_diff.append(last_order_book[f'orderbook_ap_{i+1}'] - last_order_book[f'orderbook_ap_{i}'])\n",
    "            tick_size = min(ob_price_diff)\n",
    "\n",
    "            # 마지막 1분 정보만 추출해서 bid ask spread의 평균을 구하자 (마지막 타이밍의 orderbook만 쓰는 것보다 낫지 않을까?)\n",
    "            last_1m = rolling_window_data.loc[(rolling_window_data.dt_1m == rolling_window_data['dt_1m'].iloc[-1])]\n",
    "\n",
    "            ba_sp_0 = (last_1m['orderbook_ap_0'] - last_1m['orderbook_bp_0'])/tick_size\n",
    "            bidask_spread_0.append(ba_sp_0.mean())\n",
    "            ba_sp_1 = (last_1m['orderbook_ap_1'] - last_1m['orderbook_bp_1'])/tick_size\n",
    "            bidask_spread_1.append(ba_sp_1.mean())\n",
    "\n",
    "        # Create a new DataFrame to store the results\n",
    "        d = pd.DataFrame({\n",
    "        'window_start': temp_index[:-10],\n",
    "        f'window_end_{interval}': temp_index[10:],\n",
    "        f'liq_last_1_{interval}': liq_last_1,\n",
    "        f'liq_last_2_{interval}': liq_last_2,\n",
    "        f'liq_last_5_{interval}': liq_last_5,\n",
    "        f'liq_last_10_{interval}': liq_last_10,\n",
    "        f'liq_last_15_{interval}': liq_last_15,\n",
    "        f'ep_liq_1_{interval}': ep_liq_1,\n",
    "        f'ep_liq_2_{interval}': ep_liq_2,\n",
    "        f'ep_liq_5_{interval}': ep_liq_5,\n",
    "        f'ep_liq_10_{interval}': ep_liq_10,\n",
    "        f'ep_liq_15_{interval}': ep_liq_15,\n",
    "        f'bidask_spread_0_{interval}': bidask_spread_0,\n",
    "        f'bidask_spread_1_{interval}': bidask_spread_1\n",
    "        })\n",
    "\n",
    "        result_df_orderbook = pd.merge(result_df_orderbook, d, on = 'window_start', how='inner')\n",
    "\n",
    "\n",
    "    # 첫 1분 및 마지막 11분은 데이터가 complete 하지 않을 것이므로 삭제.\n",
    "    start_time = result_df_orderbook['time_id'].iloc[0]\n",
    "    end_time = result_df_orderbook['time_id'].iloc[-11]\n",
    "    result_df_orderbook_flt = result_df_orderbook[(result_df_orderbook['time_id'] > start_time) & (result_df_orderbook['time_id'] < end_time)]\n",
    "\n",
    "    return result_df_orderbook_flt\n",
    "\n",
    "    # # Orderbook 10분 정리 dataframe을 중간 저장.\n",
    "    # result_df_orderbook_flt.to_csv(working_directory + \"output\\\\{}_sum_orderbook_10m.csv\".format(coin), index=False)\n",
    "\n",
    "    # ## result_df & result_df_orderbook 두개를 concat 하자.\n",
    "    # result_df_orderbook_flt = result_df_orderbook_flt.drop(columns = ['window_end', 'time_id'])\n",
    "    # combined_result_df = pd.merge(result_df_flt, result_df_orderbook_flt, on='window_start', suffixes=('_ticker', '_orderbook'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3342751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c29576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca09583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For Checking ###\n",
    "\n",
    "import os\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "\n",
    "working_directory = 'D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'\n",
    "os.chdir(working_directory)\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "coin = 'BTC'\n",
    "file_path =  working_directory + \"output\\\\{}_sum_both_10m.csv\".format(coin)\n",
    "temp_df = pd.read_csv(file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba101ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7677ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and drop rows (새벽시간 삭제하기)\n",
    "\n",
    "temp_df['window_start'] = pd.to_datetime(temp_df['window_start'])  # Convert to datetime\n",
    "\n",
    "# Define the time range\n",
    "start_time = pd.to_datetime('00:00:00').time()\n",
    "end_time = pd.to_datetime('06:00:00').time()\n",
    "\n",
    "# Filter and drop rows (새벽시간 삭제하기)\n",
    "temp_filtered_df = temp_df[~temp_df['window_start'].apply(lambda x: start_time <= x.time() <= end_time)]\n",
    "print(\"# of rows of filtered_df:\", temp_filtered_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba38b349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce74829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## realized volatility가 뭉쳐서 나타나는가에 관한 분석.\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate example data\n",
    "x = temp_filtered_df['window_start']\n",
    "y = temp_filtered_df['dv5_realized_volatility_mean0']  # Corresponding y values\n",
    "\n",
    "# Create a line plot\n",
    "plt.plot(x, y,  color='blue', linewidth=0.05) # label='target realized volatility',\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('window start time')\n",
    "plt.ylabel('Target realized volatility')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9825dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## highest return 분포도 확인.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a histogram\n",
    "plt.hist(temp_filtered_df['dv5_realized_volatility_mean0'], bins=200, color='blue', edgecolor='black')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Target realized volatility')\n",
    "plt.ylabel('Frequency')\n",
    "# plt.title('Histogram of Continuous Variable')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed52d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lowest return 분포 확인\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a histogram\n",
    "plt.hist(combined_result_df['dv2_lowest_return'], bins=200, color='blue', edgecolor='black')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Continuous Variable')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ede0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dv1 분포 확인\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a histogram\n",
    "plt.hist(combined_result_df['dv1_realized_volatility'], bins=200, color='blue', edgecolor='black')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Continuous Variable')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033813b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78578000",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 10분 단위로 정리하기. Ex) realized vol\n",
    "\n",
    "# features = {\n",
    "#     'log_return': [realized_volatility],\n",
    "#     'sys_datetime': ['count'],\n",
    "#     'trade_volume': [np.sum, np.mean],\n",
    "#     'acc_ask_volume': [np.mean],\n",
    "#     'acc_bid_volume': [np.mean],\n",
    "# }\n",
    "# agg = df_ticker_pd_flt.groupby('time_id').agg(features).reset_index()  # time_id로 그룹화하여 통계량을 계산합니다.\n",
    "# agg.columns = flatten_name('trade', agg.columns)  # 계산된 통계량의 열 이름을 변경합니다.\n",
    "\n",
    "# for time in [450, 300, 150]:\n",
    "#     d = df_ticker_pd_flt[df_ticker_pd_flt['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)  # 주어진 시간 이상의 데이터를 사용하여 통계량을 계산합니다.\n",
    "#     d.columns = flatten_name(f'trade_{time}', d.columns)  # 계산된 통계량의 열 이름을 변경합니다.\n",
    "#     agg = pd.merge(agg, d, on='time_id', how='left')  # 계산된 통계량을 기존 데이터프레임에 병합합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf549f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996c26c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee91a7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee2f027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f66c1d44",
   "metadata": {},
   "source": [
    "# Nearest-Neighbor Features (여기서부터는 활용하지 않음 => C2 코드 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae8ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "\n",
    "working_directory = 'D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'\n",
    "os.chdir(working_directory)\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "from contextlib import contextmanager  # 컨텍스트 관리자를 사용하기 위한 contextlib 모듈을 가져옵니다.\n",
    "import time  # 시간 관련 기능을 사용하기 위한 time 모듈을 가져옵니다.\n",
    "from sklearn.neighbors import NearestNeighbors  # 최근접 이웃 알고리즘을 사용하기 위한 모듈을 가져옵니다.\n",
    "from sklearn.preprocessing import minmax_scale  # 데이터 스케일링을 위한 모듈을 가져옵니다.\n",
    "from typing import Dict, List, Optional, Tuple  # 타입 힌트를 사용하기 위한 typing 모듈을 가져옵니다.\n",
    "import seaborn as sns  # 시각화 라이브러리인 seaborn을 가져옵니다.\n",
    "import gc\n",
    "import traceback  # 예외 정보를 출력하기 위한 traceback 모듈을 가져옵니다.\n",
    "\n",
    "file_path =  working_directory + \"output\\\\{}_sum_both_10m.csv\"\n",
    "combined_result_df = pd.read_csv(file_path) \n",
    "\n",
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - s\n",
    "    print(f'[{name}] {elapsed: .3f}초')\n",
    "\n",
    "def print_trace(name: str = ''):\n",
    "    print(f'{name or \"익명\"}에서 에러가 발생했습니다.')\n",
    "    print(traceback.format_exc())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d184c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NEIGHBORS_MAX = 80 \n",
    "\n",
    "class Neighbors:\n",
    "    def __init__(self, \n",
    "                 name: str, \n",
    "                 pivot: pd.DataFrame, \n",
    "                 p: float, \n",
    "                 metric: str = 'minkowski', \n",
    "                 metric_params: Optional[Dict] = None, \n",
    "                 exclude_self: bool = False):\n",
    "        self.name = name\n",
    "        self.exclude_self = exclude_self\n",
    "        self.p = p\n",
    "        self.metric = metric\n",
    "        \n",
    "        if metric == 'random':\n",
    "            n_queries = len(pivot)\n",
    "            self.neighbors = np.random.randint(n_queries, size=(n_queries, N_NEIGHBORS_MAX))\n",
    "        else:\n",
    "            nn = NearestNeighbors(\n",
    "                n_neighbors=N_NEIGHBORS_MAX, \n",
    "                p=p, \n",
    "                metric=metric, \n",
    "                metric_params=metric_params\n",
    "            )\n",
    "            nn.fit(pivot)\n",
    "            _, self.neighbors = nn.kneighbors(pivot, return_distance=True)\n",
    "\n",
    "        self.columns = self.index = self.feature_values = self.feature_col = None\n",
    "\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def make_nn_feature(self, n=5, agg=np.mean) -> pd.DataFrame:\n",
    "        assert self.feature_values is not None, \"should call rearrange_feature_values beforehand\"\n",
    "\n",
    "        start = 1 if self.exclude_self else 0\n",
    "\n",
    "        pivot_aggs = pd.DataFrame(\n",
    "            agg(self.feature_values[start:n,:,0], axis=0), \n",
    "            columns=self.columns, \n",
    "            index=self.index\n",
    "        )\n",
    "\n",
    "        dst = pivot_aggs.reset_index() # unstack().\n",
    "        # print(\"dst.shape:\", dst.shape)\n",
    "        new_column_names = ['time_id', f'{self.feature_col}_nn{n}_{self.name}_{agg.__name__}'] # 3개를 예측했는데 2개만 들어왔다??\n",
    "        dst.columns = new_column_names \n",
    "        return dst\n",
    "    \n",
    "\n",
    "class TimeIdNeighbors(Neighbors):\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        # feature_pivot = df.pivot(index='time_id', values=feature_col)\n",
    "        # feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "\n",
    "        feature_df = df[['time_id', feature_col]]\n",
    "        feature_df.set_index('time_id', inplace=True)\n",
    "        feature_df = feature_df.fillna(feature_df.mean())\n",
    "\n",
    "        feature_values = np.zeros((N_NEIGHBORS_MAX, feature_df.shape[0], 1))\n",
    "\n",
    "        for i in range(N_NEIGHBORS_MAX):\n",
    "            feature_values[i, :, 0] += feature_df.values[self.neighbors[:, i], 0]\n",
    "\n",
    "        self.columns = list(feature_df.columns)\n",
    "        self.index = list(feature_df.index)\n",
    "        self.feature_values = feature_values\n",
    "        self.feature_col = feature_col\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"time-id NN (name={self.name}, metric={self.metric}, p={self.p})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacba510",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_result_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d5b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 기타 Feature 추가.\n",
    "\n",
    "# the tau itself is meaningless for GBDT, but useful as input to aggregate in Nearest Neighbor features\n",
    "combined_result_df['trade.tau'] = np.sqrt(1 / combined_result_df['num_trades'])\n",
    "combined_result_df['trade_150.tau'] = np.sqrt(1 / combined_result_df['num_trades_150'])\n",
    "combined_result_df['trade_300.tau'] = np.sqrt(1 / combined_result_df['num_trades_300'])\n",
    "combined_result_df['trade_450.tau'] = np.sqrt(1 / combined_result_df['num_trades_450'])\n",
    "\n",
    "combined_result_df['tvpl_150'] = combined_result_df['trade_vol_150'] / combined_result_df['liq_last_2_150']\n",
    "combined_result_df['tvpl_300'] = combined_result_df['trade_vol_300'] / combined_result_df['liq_last_2_300']\n",
    "combined_result_df['tvpl_450'] = combined_result_df['trade_vol_450'] / combined_result_df['liq_last_2_450']\n",
    "\n",
    "\n",
    "# df['trade_150.tau'] = np.sqrt(1 / df['trade_150.seconds_in_bucket.count'])\n",
    "# df['book.tau'] = np.sqrt(1 / df['book.seconds_in_bucket.count'])\n",
    "# df['real_price'] = 0.01 / df['tick_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe2c302",
   "metadata": {},
   "source": [
    "#### Build Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a068f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PRICE_NN_FEATURES = True\n",
    "USE_VOL_NN_FEATURES = True\n",
    "USE_SIZE_NN_FEATURES = True\n",
    "USE_RANDOM_NN_FEATURES = False\n",
    "USE_TIME_ID_NN = True\n",
    "\n",
    "time_id_neighbors: List[Neighbors] = []\n",
    "\n",
    "with timer('knn fit'):\n",
    "    df_pv = combined_result_df[['time_id']].copy()\n",
    "    # df_pv['price'] = 0.01 / df['tick_size'] \n",
    "    df_pv['lowest_return'] = combined_result_df['lowest_return']\n",
    "    df_pv['trade.tau'] = combined_result_df['trade.tau']\n",
    "    df_pv['trade_vol'] = combined_result_df['trade_vol']\n",
    "    df_pv['realized_volatility'] = combined_result_df['realized_volatility']\n",
    "    \n",
    "    if USE_PRICE_NN_FEATURES:\n",
    "        df_nn = df_pv[['time_id','lowest_return']]\n",
    "        df_nn.set_index('time_id', inplace=True)\n",
    "        df_nn = df_nn.fillna(df_nn.mean())\n",
    "        df_nn = pd.DataFrame(minmax_scale(df_nn))\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'time_price_c', \n",
    "                df_nn, \n",
    "                p=2, \n",
    "                metric='canberra', \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "        # time_id_neighbors.append(\n",
    "        #     TimeIdNeighbors(\n",
    "        #         'time_price_m', \n",
    "        #         df_nn, \n",
    "        #         p=2, \n",
    "        #         metric='mahalanobis',\n",
    "        #         metric_params={'VI':np.cov(df_nn['lowest_return'].values.T)}\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "    if USE_VOL_NN_FEATURES:\n",
    "        df_nn = df_pv[['time_id','realized_volatility']]\n",
    "        df_nn.set_index('time_id', inplace=True)\n",
    "\n",
    "        # pivot = df_pv.pivot(index='time_id', columns='stock_id', values= 'vol')\n",
    "        df_nn = df_nn.fillna(df_nn.mean())\n",
    "        df_nn = pd.DataFrame(minmax_scale(df_nn))\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors('time_vol_l1', df_nn, p=1)\n",
    "        )\n",
    "\n",
    "    if USE_SIZE_NN_FEATURES:\n",
    "\n",
    "        df_nn = df_pv[['time_id','trade_vol']]\n",
    "        df_nn.set_index('time_id', inplace=True)\n",
    "        # pivot = df_pv.pivot(index='time_id', columns='stock_id', values= 'trade.size.sum')\n",
    "        df_nn = df_nn.fillna(df_nn.mean())\n",
    "        df_nn = pd.DataFrame(minmax_scale(df_nn))\n",
    "\n",
    "        # time_id_neighbors.append(\n",
    "        #     TimeIdNeighbors(\n",
    "        #         'time_size_m', \n",
    "        #         df_nn, \n",
    "        #         p=2, \n",
    "        #         metric='mahalanobis', \n",
    "        #         metric_params={'VI':np.cov(df_nn.values.T)}\n",
    "        #     )\n",
    "        # )\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'time_size_c', \n",
    "                df_nn, \n",
    "                p=2, \n",
    "                metric='canberra'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    if USE_RANDOM_NN_FEATURES:\n",
    "        # pivot = df_pv.pivot(index='time_id', columns='stock_id', values= 'vol')\n",
    "        # pivot = pivot.fillna(pivot.mean())\n",
    "        # pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "\n",
    "        df_nn = df_pv[['time_id','realized_volatility']]\n",
    "        df_nn.set_index('time_id', inplace=True)\n",
    "        df_nn = df_nn.fillna(df_nn.mean())\n",
    "        df_nn = pd.DataFrame(minmax_scale(df_nn))\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'time_random', \n",
    "                df_nn, \n",
    "                p=2, \n",
    "                metric='random'\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "if not USE_TIME_ID_NN:\n",
    "    time_id_neighbors = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70238004",
   "metadata": {},
   "source": [
    "#### Check Neighbor Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2445035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rank_correraltion(neighbors, top_n=10):\n",
    "    if not neighbors:\n",
    "        return\n",
    "    neighbor_indices = pd.DataFrame()\n",
    "    for n in neighbors:\n",
    "        neighbor_indices[n.name] = n.neighbors[:,:top_n].flatten()\n",
    "        display(neighbor_indices[n.name])\n",
    "    sns.heatmap(neighbor_indices.corr('kendall'), annot=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b192913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3a3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rank_correraltion(time_id_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6098752b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac079606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19ab8004",
   "metadata": {},
   "source": [
    "#### Aggregate Features With Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4224aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 같은 time_id 안에서 서로 다른 stock_id 간에 rank를 부여한 것으로 보임. 우리는 소수의 비트코인만 분석하므로 관련 없음. \n",
    "# \n",
    "# # features with large changes over time are converted to relative ranks within time-id\n",
    "# if ENABLE_RANK_NORMALIZATION:\n",
    "#     df['trade.order_count.mean'] = df.groupby('time_id')['trade.order_count.mean'].rank()\n",
    "#     df['book.total_volume.sum']  = df.groupby('time_id')['book.total_volume.sum'].rank()\n",
    "#     df['book.total_volume.mean'] = df.groupby('time_id')['book.total_volume.mean'].rank()\n",
    "#     df['book.total_volume.std']  = df.groupby('time_id')['book.total_volume.std'].rank()\n",
    "\n",
    "#     df['trade.tau'] = df.groupby('time_id')['trade.tau'].rank()\n",
    "\n",
    "#     for dt in [150, 300, 450]:\n",
    "#         df[f'trade_{dt}.order_count.mean'] = df.groupby('time_id')[f'trade_{dt}.order_count.mean'].rank()\n",
    "#         df[f'book_{dt}.total_volume.sum']  = df.groupby('time_id')[f'book_{dt}.total_volume.sum'].rank()\n",
    "#         df[f'book_{dt}.total_volume.mean'] = df.groupby('time_id')[f'book_{dt}.total_volume.mean'].rank()\n",
    "#         df[f'book_{dt}.total_volume.std']  = df.groupby('time_id')[f'book_{dt}.total_volume.std'].rank()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ab76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이웃을 얻는 방식 1개를 가져와서 shape를 살펴보자. (각 time_id마다 80개의 neighbor를 얻음)\n",
    "time_id_neighbors[0].neighbors.shape\n",
    "\n",
    "#test_nn.rearrange_feature_values(df2, feature_col)\n",
    "\n",
    "## TEST\n",
    "nn =  time_id_neighbors[0]           \n",
    "nn.rearrange_feature_values(combined_result_df,'realized_volatility')\n",
    "print(\"feature_values.shape:\", nn.feature_values.shape)\n",
    "\n",
    "# agg = np.min\n",
    "# n = 10\n",
    "# pivot_aggs = pd.DataFrame(\n",
    "#     agg(nn.feature_values[0:n,:,0], axis=0), \n",
    "#     columns=nn.columns, \n",
    "#     index=nn.index\n",
    "# )\n",
    "\n",
    "# print(\"pivot_aggs.shape:\", pivot_aggs.shape)\n",
    "# dst = pivot_aggs.reset_index() # unstack().\n",
    "# print(\"dst.shape:\", dst.shape)\n",
    "# print(\"dst.columns:\", dst.columns)\n",
    "\n",
    "# dst.columns = ['time_id', f'{nn.feature_col}_nn{n}_{nn.name}_{agg.__name__}'] # 3개를 예측했는데 2개만 들어왔다??\n",
    "# print(dst.head(3))\n",
    "\n",
    "# return dst\n",
    "\n",
    "# dst = nn.make_nn_feature(5, np.min) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f1c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nearest_neighbor_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df2 = combined_result_df.copy()\n",
    "    print(df2.shape)\n",
    "\n",
    "    ### stock_id를 기준으로 얻어진 neighbor를 대상으로 feature 만들기\n",
    "    # feature_cols_stock = {\n",
    "    #     'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n",
    "    #     'trade.seconds_in_bucket.count': [np.mean],\n",
    "    #     'trade.tau': [np.mean],\n",
    "    #     'trade_150.tau': [np.mean],\n",
    "    #     'book.tau': [np.mean],\n",
    "    #     'trade.size.sum': [np.mean],\n",
    "    #     'book.seconds_in_bucket.count': [np.mean],\n",
    "    # }\n",
    "    \n",
    "    ### time_id를 기준으로 얻어진 neighbor를 대상으로 feature 만들기\n",
    "    feature_cols = {\n",
    "        'realized_volatility': [np.mean, np.min, np.max, np.std],\n",
    "        'lowest_return': [np.max, np.mean, np.min],\n",
    "        'num_trades': [np.mean],\n",
    "        'trade.tau': [np.mean],\n",
    "        'trade_vol': [np.mean],\n",
    "        # 'book.seconds_in_bucket.count': [np.mean],\n",
    "        # 'trade_150.tau_nn20_stock_vol_l1_mean': [np.mean],\n",
    "        # 'trade.size.sum_nn20_stock_vol_l1_mean': [np.mean],\n",
    "    }\n",
    "\n",
    "    time_id_neigbor_sizes = [3, 5, 10, 20, 40]\n",
    "    time_id_neigbor_sizes_vol = [2, 3, 5, 10, 20, 40]\n",
    "    # stock_id_neighbor_sizes = [10, 20, 40]\n",
    "\n",
    "    ndf: Optional[pd.DataFrame] = None\n",
    "    \n",
    "    # 새로운 feature를 기존 df에 추가하는 함수\n",
    "    def _add_ndf(ndf: Optional[pd.DataFrame], dst: pd.DataFrame) -> pd.DataFrame:\n",
    "        if ndf is None:\n",
    "            return dst\n",
    "        else:\n",
    "            ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
    "            return ndf\n",
    "\n",
    "    # # neighbor stock_id\n",
    "    # for feature_col in feature_cols_stock.keys():\n",
    "    # # 'book.log_return1.realized_volatility', 'trade.seconds_in_bucket.count', 'trade.tau', 'trade_150.tau', 'book.tau', 'trade.size.sum', 'book.seconds_in_bucket.count' \n",
    "    #     try:\n",
    "    #         # 해당 특성이 기존 df에 있다면 Pass\n",
    "    #         if feature_col not in df2.columns:\n",
    "    #             print(f\"column {feature_col} is skipped\")\n",
    "    #             continue\n",
    "    #         # stock_id_neighbors가 아무것도 없다면 Pass\n",
    "    #         if not stock_id_neighbors:\n",
    "    #             continue\n",
    "            \n",
    "    #         # stock_id_neighbors의 각 Class (2개)를 반복\n",
    "    #         for nn in stock_id_neighbors:\n",
    "    #             # stock_id_neighbors에 feature_col로 rearrange_feature_values\n",
    "    #             nn.rearrange_feature_values(df2, feature_col)\n",
    "            \n",
    "    #         # agg : feature_cols_stock의 function pointer (np.mean, np.max ...)\n",
    "    #         for agg in feature_cols_stock[feature_col]:\n",
    "    #             for n in stock_id_neighbor_sizes: # n : [10, 20, 40]\n",
    "    #                 try:\n",
    "    #                     for nn in stock_id_neighbors:\n",
    "    #                         dst = nn.make_nn_feature(n, agg) \n",
    "    #                         # e.g. n : 10, agg : np.mean\n",
    "    #                         ndf = _add_ndf(ndf, dst)\n",
    "    #                         # add columns\n",
    "    #                 except Exception:\n",
    "    #                     print_trace('stock-id nn')\n",
    "    #                     pass\n",
    "    #     except Exception:\n",
    "    #         print_trace('stock-id nn')\n",
    "    #         pass\n",
    "\n",
    "    # if ndf is not None:\n",
    "    #     df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n",
    "    # ndf = None\n",
    "\n",
    "    # print(df2.shape)\n",
    "\n",
    "    # neighbor time_id\n",
    "    for feature_col in feature_cols.keys():\n",
    "        try:\n",
    "            if not USE_PRICE_NN_FEATURES and feature_col == 'lowest_return':\n",
    "                continue\n",
    "            if feature_col not in df2.columns:\n",
    "                print(f\"column {feature_col} is skipped\")\n",
    "                continue\n",
    "            \n",
    "            for nn in time_id_neighbors:\n",
    "                nn.rearrange_feature_values(df2, feature_col)\n",
    "\n",
    "            if 'realized_volatility' in feature_col:\n",
    "                time_id_ns = time_id_neigbor_sizes_vol\n",
    "            else:\n",
    "                time_id_ns = time_id_neigbor_sizes\n",
    "\n",
    "            for agg in feature_cols[feature_col]:\n",
    "                for n in time_id_ns:\n",
    "                    try:\n",
    "                        for nn in time_id_neighbors:\n",
    "                            dst = nn.make_nn_feature(n, agg)\n",
    "                            ndf = _add_ndf(ndf, dst)\n",
    "                    except Exception:\n",
    "                        print_trace('time-id nn')\n",
    "                        pass\n",
    "        except Exception:\n",
    "            print_trace('time-id nn')\n",
    "\n",
    "    if ndf is not None:\n",
    "        df2 = pd.merge(df2, ndf, on=['time_id'], how='left')\n",
    "    \n",
    "    print(df2.shape)\n",
    "    \n",
    "    # features further derived from nearest neighbor features\n",
    "    try:\n",
    "        if USE_PRICE_NN_FEATURES:\n",
    "            for sz in time_id_neigbor_sizes:\n",
    "                denominator = f\"lowest_return_nn{sz}_time_price_c\"\n",
    "\n",
    "                df2[f'lowest_return_rankmin_{sz}']  = df2['lowest_return'] / df2[f\"{denominator}_amin\"]\n",
    "                df2[f'lowest_return_rankmax_{sz}']  = df2['lowest_return'] / df2[f\"{denominator}_amax\"]\n",
    "                df2[f'lowest_return_rankmean_{sz}'] = df2['lowest_return'] / df2[f\"{denominator}_mean\"]\n",
    "\n",
    "            for sz in time_id_neigbor_sizes_vol:\n",
    "                denominator = f\"realized_volatility_nn{sz}_time_price_c\"\n",
    "\n",
    "                df2[f'vol_rankmin_{sz}'] = \\\n",
    "                    df2['realized_volatility'] / df2[f\"{denominator}_amin\"]\n",
    "                df2[f'vol_rankmax_{sz}'] = \\\n",
    "                    df2['realized_volatility'] / df2[f\"{denominator}_amax\"]\n",
    "\n",
    "        price_cols = [c for c in df2.columns if 'lowest_return' in c and 'rank' not in c]\n",
    "        for c in price_cols:\n",
    "            del df2[c]\n",
    "\n",
    "        # if USE_PRICE_NN_FEATURES:\n",
    "        #     for sz in time_id_neigbor_sizes_vol:\n",
    "        #         tgt = f'realized_volatility_nn{sz}_time_price_m_mean'\n",
    "        #         df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n",
    "    except Exception:\n",
    "        print_trace('nn features')\n",
    "    \n",
    "    print(df2.shape)\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eca54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "with timer('make nearest neighbor feature'):\n",
    "    df3 = make_nearest_neighbor_feature(combined_result_df)\n",
    "\n",
    "print(df3.shape)\n",
    "#df2.reset_index(drop=True).to_feather('optiver_df2.f')\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc2a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfdd52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정리된 dataframe을 저장.\n",
    "df3.to_csv(working_directory + \"output\\\\{}_features.csv\".format(coin), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7e674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c44d2b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b666a0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b1b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b29064a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91264412",
   "metadata": {},
   "source": [
    "df_ticker_pandas['log_return']  = df_ticker_pandas.groupby(['trade_price'])['trade_price'].apply(log_return).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87c5665-2aab-4488-a915-0074aff33c4d",
   "metadata": {},
   "source": [
    "## 1-2: 각 코인의 틱데이터를 10초/1분씩 묶는 전처리 작업 (이 아래로는 현재 코드에서는 활용하지 않음. 단순 참고용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b048ac1e-656d-4f76-afa5-13de8417f604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 파일을 하나씩 열어서 10초씩 묶는 전처리를 수행하자.\n",
    "# 이후 6*100개(100분)를 look back 함으로써 단기 분석을 수행하고 향후 (10초는 뛰어넘고), 5분 혹은 10분간 수익률이 어떻게 될 것인지 추정한다.\n",
    "# 또다른 프로젝트로는 변동성을 예측. Ex) 최근 10시간을 분석한 후 향후 1시간의 변동성을 예측하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154dbc6-4e24-41c7-82d5-7641c86c042f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00edd56c-1785-4088-a24e-7650cce31494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 파일을 for문 안에서 하나씩 열기\n",
    "\n",
    "coin_list = ['BTC', 'ETH', 'DOGE', 'XRP']\n",
    "# data_num = 2\n",
    "data_id = 202303271051 # 해당 틱데이터 커버 기간: 22년 12월 16일 오후 9시 ~ 23년 2월 26일 오전 4시\n",
    "\n",
    "# num_list = list(range(0, 79000000, 1000000))\n",
    "\n",
    "# cat = dd.read_csv(paths.data + \"cat.csv/001.PART\")\n",
    "# for num in range(0, 16):\n",
    "\n",
    "for coin in coin_list:\n",
    "\n",
    "    # 해당 폴더에 거대 csv 파일이 쪼개져서 저장되어 있는데, 그 폴더 내 파일 리스트를 가져온다. 그 리스트에 따라 하나하나 열면서 처리를 진행한다.\n",
    "    partition_list = os.listdir(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\s_t_data_{}_{}.csv\\\\\".format(coin, data_id))         \n",
    "    for p in range(len(partition_list)):\n",
    "        \n",
    "        df2 = dd.read_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\s_t_data_{}_{}.csv\\\\{}\".format(coin, data_id, partition_list[p]), dtype={'ask_bid': 'object',\n",
    "        'change': 'object',\n",
    "        'highest_52_week_date': 'object',\n",
    "        'lowest_52_week_date': 'object',\n",
    "        'market_state': 'object',\n",
    "        'market_warning': 'object',\n",
    "        'stream_type': 'object',\n",
    "        'trade_time': 'object'})\n",
    "        \n",
    "        print('Just opened: s_t_data_{}_{}.csv\\\\{}'.format(coin, data_id, partition_list[p]))\n",
    "        \n",
    "        # 각 column마다 자료 타입 확인\n",
    "        # df2 = df2.astype(dtype)\n",
    "        # Dataframe으로 전환시키기 (이전에는 Dask dataframe?)\n",
    "        df2 = df2.compute() # pandas dataframe 으로 전환\n",
    "        print(\"Dask Dataframe => Pandas Dataframe\")\n",
    "        \n",
    "        # 새롭게 데이터를 작성할 틀 만들기 (my_data)\n",
    "        my_data = pd.DataFrame(columns=['datetime_10s', 'beginning_orderbook_midprice','beginning_price', 'highest_price',\n",
    "                                   'lowest_price', 'ending_price','ending_orderbook_midprice',\n",
    "                                   'trading_volume', 'volume_power','trade_volume_bid', 'trade_volume_ask',\n",
    "                                   'ending_orderbook_total_ask_size', 'ending_orderbook_total_bid_size', \n",
    "                                    'ending_orderbook_ap_0','ending_orderbook_as_0',\n",
    "                                    'ending_orderbook_ap_1','ending_orderbook_as_1',\n",
    "                                   'ending_orderbook_ap_2','ending_orderbook_as_2',\n",
    "                                   'ending_orderbook_ap_3','ending_orderbook_as_3',\n",
    "                                   'ending_orderbook_ap_4','ending_orderbook_as_4',\n",
    "                                   'ending_orderbook_ap_5','ending_orderbook_as_5',\n",
    "                                   'ending_orderbook_ap_6','ending_orderbook_as_6',\n",
    "                                   'ending_orderbook_ap_7','ending_orderbook_as_7',\n",
    "                                   'ending_orderbook_ap_8','ending_orderbook_as_8',\n",
    "                                   'ending_orderbook_ap_9','ending_orderbook_as_9',\n",
    "                                   'ending_orderbook_ap_10','ending_orderbook_as_10',\n",
    "                                   'ending_orderbook_ap_11','ending_orderbook_as_11',\n",
    "                                   'ending_orderbook_ap_12','ending_orderbook_as_12',\n",
    "                                   'ending_orderbook_ap_13','ending_orderbook_as_13',\n",
    "                                   'ending_orderbook_ap_14','ending_orderbook_as_14',\n",
    "                                    'ending_orderbook_bp_0','ending_orderbook_bs_0',\n",
    "                                    'ending_orderbook_bp_1','ending_orderbook_bs_1',\n",
    "                                   'ending_orderbook_bp_2','ending_orderbook_bs_2',\n",
    "                                   'ending_orderbook_bp_3','ending_orderbook_bs_3',\n",
    "                                   'ending_orderbook_bp_4','ending_orderbook_bs_4',\n",
    "                                   'ending_orderbook_bp_5','ending_orderbook_bs_5',\n",
    "                                   'ending_orderbook_bp_6','ending_orderbook_bs_6',\n",
    "                                   'ending_orderbook_bp_7','ending_orderbook_bs_7',\n",
    "                                   'ending_orderbook_bp_8','ending_orderbook_bs_8',\n",
    "                                   'ending_orderbook_bp_9','ending_orderbook_bs_9',\n",
    "                                   'ending_orderbook_bp_10','ending_orderbook_bs_10',\n",
    "                                   'ending_orderbook_bp_11','ending_orderbook_bs_11',\n",
    "                                   'ending_orderbook_bp_12','ending_orderbook_bs_12',\n",
    "                                   'ending_orderbook_bp_13','ending_orderbook_bs_13',\n",
    "                                   'ending_orderbook_bp_14','ending_orderbook_bs_14'])\n",
    "        \n",
    "        # Finding unit price \n",
    "        first_row = df.head(1)\n",
    "        num_list = [0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "        df_temp = pd.DataFrame(columns=['price_diff'])\n",
    "        for i in num_list:\n",
    "            df_temp = df_temp.append({'price_diff': first_row['ending_orderbook_ap_{}'.format(i+1)] - first_row['ending_orderbook_ap_{}'.format(i)]}, ignore_index=True)\n",
    "        # Find the minimum difference\n",
    "        min_diff = diff_df['price_diff'].min()\n",
    "        print(\"unit price\": min_diff)\n",
    "        \n",
    "        ## 10초/1분 동안 다음의 변수값을 도출 할 예정 (일단 모두 0으로 세팅)\n",
    "        # datetime_m=0 \n",
    "        datetime_10s=0 \n",
    "        beginning_orderbook_midprice = 0 ## ask price 0과 bid price 0의 평균 \n",
    "        beginning_price = 0 ## 10초 시작하고 시가\n",
    "        highest_price = 0 ## 고가\n",
    "        lowest_price = 0 ## 저가\n",
    "        ending_price = 0 ## 10초 끝나기 전 시가\n",
    "        ending_orderbook_midprice = 0\n",
    "        \n",
    "        # 새롭게 추가?\n",
    "        wap = 0\n",
    "        ba_spread = 0\n",
    "        \n",
    "        trading_volume = 0\n",
    "        volume_power = 0  ## 체결강도 (BID인 volume / ASK인 volume) ## bid(매수)가 더 많은지 ask(매도)가 더 많은지 비율로 \n",
    "        trade_volume_bid = 0 ## voluem power 계산용\n",
    "        trade_volume_ask = 0 ## voluem power 계산용\n",
    "        ending_orderbook_total_ask_size = 0\n",
    "        ending_orderbook_total_bid_size = 0\n",
    "        ending_orderbook_ap_0 = 0\n",
    "        ending_orderbook_as_0 = 0\n",
    "        ending_orderbook_ap_1 = 0\n",
    "        ending_orderbook_as_1 = 0\n",
    "        ending_orderbook_ap_2 = 0\n",
    "        ending_orderbook_as_2 = 0\n",
    "        ending_orderbook_ap_3 = 0\n",
    "        ending_orderbook_as_3 = 0\n",
    "        ending_orderbook_ap_4 = 0\n",
    "        ending_orderbook_as_4 = 0\n",
    "        ending_orderbook_ap_5 = 0\n",
    "        ending_orderbook_as_5 = 0\n",
    "        ending_orderbook_ap_6 = 0\n",
    "        ending_orderbook_as_6 = 0\n",
    "        ending_orderbook_ap_7 = 0\n",
    "        ending_orderbook_as_7 = 0\n",
    "        ending_orderbook_ap_8 = 0\n",
    "        ending_orderbook_as_8 = 0\n",
    "        ending_orderbook_ap_9 = 0\n",
    "        ending_orderbook_as_9 = 0\n",
    "        ending_orderbook_ap_10 = 0\n",
    "        ending_orderbook_as_10 = 0\n",
    "        ending_orderbook_ap_11 = 0\n",
    "        ending_orderbook_as_11 = 0\n",
    "        ending_orderbook_ap_12 = 0\n",
    "        ending_orderbook_as_12 = 0\n",
    "        ending_orderbook_ap_13 = 0\n",
    "        ending_orderbook_as_13 = 0\n",
    "        ending_orderbook_ap_14 = 0\n",
    "        ending_orderbook_as_14 = 0\n",
    "\n",
    "        ending_orderbook_bp_0 = 0\n",
    "        ending_orderbook_bs_0 = 0\n",
    "        ending_orderbook_bp_1 = 0\n",
    "        ending_orderbook_bs_1 = 0\n",
    "        ending_orderbook_bp_2 = 0\n",
    "        ending_orderbook_bs_2 = 0\n",
    "        ending_orderbook_bp_3 = 0\n",
    "        ending_orderbook_bs_3 = 0\n",
    "        ending_orderbook_bp_4 = 0\n",
    "        ending_orderbook_bs_4 = 0\n",
    "        ending_orderbook_bp_5 = 0\n",
    "        ending_orderbook_bs_5 = 0\n",
    "        ending_orderbook_bp_6 = 0\n",
    "        ending_orderbook_bs_6 = 0\n",
    "        ending_orderbook_bp_7 = 0\n",
    "        ending_orderbook_bs_7 = 0\n",
    "        ending_orderbook_bp_8 = 0\n",
    "        ending_orderbook_bs_8 = 0\n",
    "        ending_orderbook_bp_9 = 0\n",
    "        ending_orderbook_bs_9 = 0\n",
    "        ending_orderbook_bp_10 = 0\n",
    "        ending_orderbook_bs_10 = 0\n",
    "        ending_orderbook_bp_11 = 0\n",
    "        ending_orderbook_bs_11 = 0\n",
    "        ending_orderbook_bp_12 = 0\n",
    "        ending_orderbook_bs_12 = 0\n",
    "        ending_orderbook_bp_13 = 0\n",
    "        ending_orderbook_bs_13 = 0\n",
    "        ending_orderbook_bp_14 = 0\n",
    "        ending_orderbook_bs_14 = 0\n",
    "\n",
    "\n",
    "        ## 데이터 첫 row 및 last row의 timestamp 측정:\n",
    "        df2['datetime'] = pd.to_datetime(df2['datetime'])\n",
    "        # initial_timestamp = '' + str(df2['datetime'].iloc[0].year) + str(df2['datetime'].iloc[0].month) + str(df2['datetime'].iloc[0].day) + str(df2['datetime'].iloc[0].hour) + str(df2['datetime'].iloc[0].minute) + str(df2['datetime'].iloc[0].second//10)\n",
    "        # last_timestamp = '' + str(df2['datetime'].iloc[df2.shape[0]-1].year) + str(df2['datetime'].iloc[df2.shape[0]-1].month) + str(df2['datetime'].iloc[df2.shape[0]-1].day) + str(df2['datetime'].iloc[df2.shape[0]-1].hour) + str(df2['datetime'].iloc[df2.shape[0]-1].minute) + str(df2['datetime'].iloc[df2.shape[0]-1].second//10)\n",
    "\n",
    "        # 1분당 1라인 만들기:\n",
    "        initial_timestamp = '' + str(df2['datetime'].iloc[0].year) + str(df2['datetime'].iloc[0].month) + str(df2['datetime'].iloc[0].day) + str(df2['datetime'].iloc[0].hour) + str(df2['datetime'].iloc[0].minute)\n",
    "        last_timestamp = '' + str(df2['datetime'].iloc[df2.shape[0]-1].year) + str(df2['datetime'].iloc[df2.shape[0]-1].month) + str(df2['datetime'].iloc[df2.shape[0]-1].day) + str(df2['datetime'].iloc[df2.shape[0]-1].hour) + str(df2['datetime'].iloc[df2.shape[0]-1].minute)\n",
    "\n",
    "        data_len = df2.shape[0]\n",
    "        print(\"데이터{}_{}의 길이: {}\".format(coin, partition_list[p], data_len))\n",
    "\n",
    "        # new_length = 0\n",
    "        for i in range(data_len):  \n",
    "\n",
    "            # 현재 타임스탬프 확인\n",
    "            # current_timestamp = '' + str(df2['datetime'].iloc[i].year) + str(df2['datetime'].iloc[i].month) + str(df2['datetime'].iloc[i].day) + str(df2['datetime'].iloc[i].hour) + str(df2['datetime'].iloc[i].minute) + str(df2['datetime'].iloc[i].second//10)\n",
    "            current_timestamp = '' + str(df2['datetime'].iloc[i].year) + str(df2['datetime'].iloc[i].month) + str(df2['datetime'].iloc[i].day) + str(df2['datetime'].iloc[i].hour) + str(df2['datetime'].iloc[i].minute)\n",
    "\n",
    "            # 처음구간은 10초를 채우기 어렵기 때문에 자른다. 마지막 10초도 마찬가지로 자른다    \n",
    "            if  current_timestamp == initial_timestamp or current_timestamp == last_timestamp : ## 초가 같은 부분에 자른다. \n",
    "                print(\"--이 파일의 처음 10초 구간 혹은 이 파일의 마지막 10초 구간--\")\n",
    "                continue ## 따라서 초의 2번째 자리(49초 -> 50초)가 달라지면 else로 넘어간다. \n",
    "            else:\n",
    "\n",
    "                # 직전 타임스탬프(10초 단위)와 현재 타임스탬프(10초 단위)를 비교해서, 만약 다르다면? 신규 10초가 시작된다.\n",
    "                # previous_timestamp = '' + str(df2['datetime'].iloc[i-1].year) + str(df2['datetime'].iloc[i-1].month) + str(df2['datetime'].iloc[i-1].day) + str(df2['datetime'].iloc[i-1].hour) + str(df2['datetime'].iloc[i-1].minute) + str(df2['datetime'].iloc[i-1].second//10)\n",
    "                # previous_timestamp = '' + str(df2['datetime'].iloc[i-1].year) + str(df2['datetime'].iloc[i-1].month) + str(df2['datetime'].iloc[i-1].day) + str(df2['datetime'].iloc[i-1].hour) + str(df2['datetime'].iloc[i-1].minute) + str(df2['datetime'].iloc[i-1].second//10)\n",
    "                previous_timestamp = '' + str(df2['datetime'].iloc[i-1].year) + str(df2['datetime'].iloc[i-1].month) + str(df2['datetime'].iloc[i-1].day) + str(df2['datetime'].iloc[i-1].hour) + str(df2['datetime'].iloc[i-1].minute)\n",
    "\n",
    "                if previous_timestamp != current_timestamp: \n",
    "                    # 스타팅 인덱스 찍어두고,\n",
    "                    index_start_m = i\n",
    "                    # previous_timestamp와 current_timestamp와 다를 경우. ex) previous가 49초이고 current가 50초일 때\n",
    "                    # 이 때, 모든 값을 초기화 한다. \n",
    "                    \n",
    "                    # datetime_10s = df2['datetime'].iloc[i]\n",
    "                    datetime_m = df2['datetime'].iloc[i]\n",
    "                    beginning_orderbook_midprice = 0\n",
    "                    beginning_price = 0\n",
    "                    highest_price = 0\n",
    "                    lowest_price = 0\n",
    "                    ending_price = 0\n",
    "                    ending_orderbook_midprice = 0\n",
    "                    trading_volume = 0\n",
    "                    volume_power = 0  \n",
    "                    trade_volume_bid = 0 \n",
    "                    trade_volume_ask = 0 \n",
    "                    ending_orderbook_total_ask_size = 0\n",
    "                    ending_orderbook_total_bid_size = 0\n",
    "                    ending_orderbook_ap_0 = 0\n",
    "                    ending_orderbook_as_0 = 0\n",
    "                    ending_orderbook_ap_1 = 0\n",
    "                    ending_orderbook_as_1 = 0\n",
    "                    ending_orderbook_ap_2 = 0\n",
    "                    ending_orderbook_as_2 = 0\n",
    "                    ending_orderbook_ap_3 = 0\n",
    "                    ending_orderbook_as_3 = 0\n",
    "                    ending_orderbook_ap_4 = 0\n",
    "                    ending_orderbook_as_4 = 0\n",
    "                    ending_orderbook_ap_5 = 0\n",
    "                    ending_orderbook_as_5 = 0\n",
    "                    ending_orderbook_ap_6 = 0\n",
    "                    ending_orderbook_as_6 = 0\n",
    "                    ending_orderbook_ap_7 = 0\n",
    "                    ending_orderbook_as_7 = 0\n",
    "                    ending_orderbook_ap_8 = 0\n",
    "                    ending_orderbook_as_8 = 0\n",
    "                    ending_orderbook_ap_9 = 0\n",
    "                    ending_orderbook_as_9 = 0\n",
    "                    ending_orderbook_ap_10 = 0\n",
    "                    ending_orderbook_as_10 = 0\n",
    "                    ending_orderbook_ap_11 = 0\n",
    "                    ending_orderbook_as_11 = 0\n",
    "                    ending_orderbook_ap_12 = 0\n",
    "                    ending_orderbook_as_12 = 0\n",
    "                    ending_orderbook_ap_13 = 0\n",
    "                    ending_orderbook_as_13 = 0\n",
    "                    ending_orderbook_ap_14 = 0\n",
    "                    ending_orderbook_as_14 = 0\n",
    "                    ending_orderbook_bp_0 = 0\n",
    "                    ending_orderbook_bs_0 = 0\n",
    "                    ending_orderbook_bp_1 = 0\n",
    "                    ending_orderbook_bs_1 = 0\n",
    "                    ending_orderbook_bp_2 = 0\n",
    "                    ending_orderbook_bs_2 = 0\n",
    "                    ending_orderbook_bp_3 = 0\n",
    "                    ending_orderbook_bs_3 = 0\n",
    "                    ending_orderbook_bp_4 = 0\n",
    "                    ending_orderbook_bs_4 = 0\n",
    "                    ending_orderbook_bp_5 = 0\n",
    "                    ending_orderbook_bs_5 = 0\n",
    "                    ending_orderbook_bp_6 = 0\n",
    "                    ending_orderbook_bs_6 = 0\n",
    "                    ending_orderbook_bp_7 = 0\n",
    "                    ending_orderbook_bs_7 = 0\n",
    "                    ending_orderbook_bp_8 = 0\n",
    "                    ending_orderbook_bs_8 = 0\n",
    "                    ending_orderbook_bp_9 = 0\n",
    "                    ending_orderbook_bs_9 = 0\n",
    "                    ending_orderbook_bp_10 = 0\n",
    "                    ending_orderbook_bs_10 = 0\n",
    "                    ending_orderbook_bp_11 = 0\n",
    "                    ending_orderbook_bs_11 = 0\n",
    "                    ending_orderbook_bp_12 = 0\n",
    "                    ending_orderbook_bs_12 = 0\n",
    "                    ending_orderbook_bp_13 = 0\n",
    "                    ending_orderbook_bs_13 = 0\n",
    "                    ending_orderbook_bp_14 = 0\n",
    "                    ending_orderbook_bs_14 = 0\n",
    "\n",
    "\n",
    "                ## orderbook 타입\n",
    "                if df2['type_websocket'].iloc[i]  == 'orderbook':\n",
    "                    if beginning_orderbook_midprice == 0: ## 아직 업데이트가 안 되었다면? (만약을 대비)\n",
    "                        beginning_orderbook_midprice = (df2['orderbook_ap_0'].iloc[i] + df2['orderbook_bp_0'].iloc[i])/2\n",
    "                                    ## ask price 0과 bid price 0의 평균 \n",
    "\n",
    "                ## trade/ticker 타입 (거래발생)\n",
    "                elif df2['type_websocket'].iloc[i]  == 'trade' or df2['type_websocket'].iloc[i]  == 'ticker':\n",
    "                    if beginning_price==0: ## 아직 업데이트가 안 되었다면?\n",
    "                        beginning_price = df2['trade_price'].iloc[i]\n",
    "\n",
    "                    ## 고가 수정 및 저가 수정\n",
    "                    if highest_price <  df2['trade_price'].iloc[i]: \n",
    "                        highest_price = df2['trade_price'].iloc[i]\n",
    "                    if lowest_price == 0 or lowest_price > df2['trade_price'].iloc[i]:\n",
    "                        lowest_price = df2['trade_price'].iloc[i]\n",
    "\n",
    "                    ## 거래량 더해주기\n",
    "                    trading_volume += df2['trade_volume'].iloc[i] \n",
    "\n",
    "                    # ask_bid => 매수/매도 구분 한 후에, 체결강도 계산용 BID/ASK 거래량 더해주기\n",
    "                    if df2['ask_bid'].iloc[i] =='BID': \n",
    "                        trade_volume_bid += df2['trade_volume'].iloc[i]\n",
    "                    elif df2['ask_bid'].iloc[i] =='ASK':\n",
    "                        trade_volume_ask += df2['trade_volume'].iloc[i]\n",
    "\n",
    "                ## 10초의 마지막 줄임을 발견\n",
    "                # next_timestamp = '' + str(df2['datetime'].iloc[i+1].year) + str(df2['datetime'].iloc[i+1].month) + str(df2['datetime'].iloc[i+1].day) + str(df2['datetime'].iloc[i+1].hour) + str(df2['datetime'].iloc[i+1].minute) + str(df2['datetime'].iloc[i+1].second//10)\n",
    "                next_timestamp = '' + str(df2['datetime'].iloc[i+1].year) + str(df2['datetime'].iloc[i+1].month) + str(df2['datetime'].iloc[i+1].day) + str(df2['datetime'].iloc[i+1].hour) + str(df2['datetime'].iloc[i+1].minute) \n",
    "                \n",
    "                if current_timestamp != next_timestamp: ## if current_timestamp이 49초이고 next가 50초 이면 print\n",
    "\n",
    "                    # print(\"--10초의 마지막줄처리--\")\n",
    "                    # print(\"현재 index: {}\".format(i)) ## 현재 index\n",
    "                    # print(current_timestamp)\n",
    "                    # print(next_timestamp)\n",
    "                    # # if current_timestamp_plus_10s != next_timestamp: raise Exception(\"10초 동안 아무런 데이터가 없음.\")\n",
    "\n",
    "                    j = 0\n",
    "                    while ending_price == 0 or ending_orderbook_midprice==0: ## 둘 중에 하나라도 0이면 계속 while\n",
    "                        # print(\"i-j:\"+ str(i-j))\n",
    "                        ## 여기서 i = 193이라고 치면 j가 1씩 추가될 때 마다 i가 줄어드니까 하나 씩 전으로 돌아간다. \n",
    "\n",
    "                        if (df2['type_websocket'].iloc[i-j]  == 'trade' or df2['type_websocket'].iloc[i-j]  == 'ticker') and (ending_price==0):\n",
    "                            ending_price = df2['trade_price'].iloc[i-j]\n",
    "                            if beginning_price==0: \n",
    "                                beginning_price = ending_price\n",
    "                                highest_price = ending_price\n",
    "                                lowest_price = ending_price\n",
    "\n",
    "                        # i = 193 j= 0 인덱스:193이 trade or ticker면 그 값을 ending price로\n",
    "\n",
    "                        elif (df2['type_websocket'].iloc[i-j]  == 'orderbook') & (ending_orderbook_midprice==0):\n",
    "                            ending_orderbook_midprice = (df2['orderbook_ap_0'].iloc[i-j] +  df2['orderbook_bp_0'].iloc[i-j])/2\n",
    "                            ending_orderbook_total_ask_size = df2['total_ask_size'].iloc[i-j]\n",
    "                            ending_orderbook_total_bid_size = df2['total_bid_size'].iloc[i-j]\n",
    "                            ending_orderbook_ap_0 = df2['orderbook_ap_0'].iloc[i-j]/ending_orderbook_midprice  ## 나누는 이유: orderbook 평균가 기준 얼마나 높은지 비율로 따지기 위해 \n",
    "                            ending_orderbook_as_0 = df2['orderbook_as_0'].iloc[i-j]\n",
    "                            ending_orderbook_ap_1 = df2['orderbook_ap_1'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_1 = df2['orderbook_as_1'].iloc[i-j]\n",
    "                            ending_orderbook_ap_2 = df2['orderbook_ap_2'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_2 = df2['orderbook_as_2'].iloc[i-j]\n",
    "                            ending_orderbook_ap_3 = df2['orderbook_ap_3'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_3 = df2['orderbook_as_3'].iloc[i-j]\n",
    "                            ending_orderbook_ap_4 = df2['orderbook_ap_4'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_4 = df2['orderbook_as_4'].iloc[i-j]\n",
    "                            ending_orderbook_ap_5 = df2['orderbook_ap_5'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_5 = df2['orderbook_as_5'].iloc[i-j]\n",
    "                            ending_orderbook_ap_6 = df2['orderbook_ap_6'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_6 = df2['orderbook_as_6'].iloc[i-j]\n",
    "                            ending_orderbook_ap_7 = df2['orderbook_ap_7'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_7 = df2['orderbook_as_7'].iloc[i-j]\n",
    "                            ending_orderbook_ap_8 = df2['orderbook_ap_8'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_8 = df2['orderbook_as_8'].iloc[i-j]\n",
    "                            ending_orderbook_ap_9 = df2['orderbook_ap_9'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_9 = df2['orderbook_as_9'].iloc[i-j]\n",
    "                            ending_orderbook_ap_10 = df2['orderbook_ap_10'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_10 = df2['orderbook_as_10'].iloc[i-j]\n",
    "                            ending_orderbook_ap_11 = df2['orderbook_ap_11'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_11 = df2['orderbook_as_11'].iloc[i-j]\n",
    "                            ending_orderbook_ap_12 = df2['orderbook_ap_12'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_12 = df2['orderbook_as_12'].iloc[i-j]\n",
    "                            ending_orderbook_ap_13 = df2['orderbook_ap_13'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_13 = df2['orderbook_as_13'].iloc[i-j]\n",
    "                            ending_orderbook_ap_14 = df2['orderbook_ap_14'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_14 = df2['orderbook_as_14'].iloc[i-j]\n",
    "                            ending_orderbook_bp_0 = df2['orderbook_bp_0'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_0 = df2['orderbook_bs_0'].iloc[i-j]\n",
    "                            ending_orderbook_bp_1 = df2['orderbook_bp_1'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_1 = df2['orderbook_bs_1'].iloc[i-j]\n",
    "                            ending_orderbook_bp_2 = df2['orderbook_bp_2'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_2 = df2['orderbook_bs_2'].iloc[i-j]\n",
    "                            ending_orderbook_bp_3 = df2['orderbook_bp_3'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_3 = df2['orderbook_bs_3'].iloc[i-j]\n",
    "                            ending_orderbook_bp_4 = df2['orderbook_bp_4'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_4 = df2['orderbook_bs_4'].iloc[i-j]\n",
    "                            ending_orderbook_bp_5 = df2['orderbook_bp_5'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_5 = df2['orderbook_bs_5'].iloc[i-j]\n",
    "                            ending_orderbook_bp_6 = df2['orderbook_bp_6'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_6 = df2['orderbook_bs_6'].iloc[i-j]\n",
    "                            ending_orderbook_bp_7 = df2['orderbook_bp_7'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_7 = df2['orderbook_bs_7'].iloc[i-j]\n",
    "                            ending_orderbook_bp_8 = df2['orderbook_bp_8'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_8 = df2['orderbook_bs_8'].iloc[i-j]\n",
    "                            ending_orderbook_bp_9 = df2['orderbook_bp_9'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_9 = df2['orderbook_bs_9'].iloc[i-j]\n",
    "                            ending_orderbook_bp_10 = df2['orderbook_bp_10'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_10 = df2['orderbook_bs_10'].iloc[i-j]\n",
    "                            ending_orderbook_bp_11 = df2['orderbook_bp_11'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_11 = df2['orderbook_bs_11'].iloc[i-j]\n",
    "                            ending_orderbook_bp_12 = df2['orderbook_bp_12'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_12 = df2['orderbook_bs_12'].iloc[i-j]\n",
    "                            ending_orderbook_bp_13 = df2['orderbook_bp_13'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_13 = df2['orderbook_bs_13'].iloc[i-j]\n",
    "                            ending_orderbook_bp_14 = df2['orderbook_bp_14'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_14 = df2['orderbook_bs_14'].iloc[i-j]                 \n",
    "\n",
    "                            ## 10초 동안 orderbook 변화가 없었다면, 가장 최근 orderbook 정보가 beginning에 있어야. \n",
    "                            if beginning_orderbook_midprice == 0 :  beginning_orderbook_midprice = ending_orderbook_midprice \n",
    "\n",
    "                        j += 1\n",
    "\n",
    "                    volume_power = trade_volume_bid-trade_volume_ask\n",
    "\n",
    "                    # data_to_insert = pd.Series({'datetime_10s':datetime_10s, 'beginning_orderbook_midprice':beginning_orderbook_midprice,\n",
    "                    data_to_insert = pd.Series({'datetime_m':datetime_m, 'beginning_orderbook_midprice':beginning_orderbook_midprice,\n",
    "                                     'beginning_price':beginning_price, 'highest_price':highest_price,\n",
    "                                     'highest_price':highest_price, 'lowest_price':lowest_price,\n",
    "                                     'ending_price':ending_price, 'ending_orderbook_midprice':ending_orderbook_midprice,\n",
    "                                     'trading_volume':trading_volume, 'volume_power':volume_power,\n",
    "                                      'trade_volume_bid':trade_volume_bid, 'trade_volume_ask':trade_volume_ask,\n",
    "                                     'ending_orderbook_total_ask_size':ending_orderbook_total_ask_size, 'ending_orderbook_total_bid_size':ending_orderbook_total_bid_size,\n",
    "                                     'ending_orderbook_ap_0':ending_orderbook_ap_0, 'ending_orderbook_as_0':ending_orderbook_as_0,\n",
    "                                     'ending_orderbook_ap_1':ending_orderbook_ap_1, 'ending_orderbook_as_1':ending_orderbook_as_1,\n",
    "                                     'ending_orderbook_ap_2':ending_orderbook_ap_2, 'ending_orderbook_as_2':ending_orderbook_as_2,\n",
    "                                     'ending_orderbook_ap_3':ending_orderbook_ap_3, 'ending_orderbook_as_3':ending_orderbook_as_3,\n",
    "                                      'ending_orderbook_ap_4':ending_orderbook_ap_4, 'ending_orderbook_as_4':ending_orderbook_as_4,\n",
    "                                     'ending_orderbook_ap_5':ending_orderbook_ap_5, 'ending_orderbook_as_5':ending_orderbook_as_5,\n",
    "                                      'ending_orderbook_ap_6':ending_orderbook_ap_6, 'ending_orderbook_as_6':ending_orderbook_as_6,\n",
    "                                     'ending_orderbook_ap_7':ending_orderbook_ap_7, 'ending_orderbook_as_7':ending_orderbook_as_7,\n",
    "                                      'ending_orderbook_ap_8':ending_orderbook_ap_8, 'ending_orderbook_as_8':ending_orderbook_as_8,\n",
    "                                     'ending_orderbook_ap_9':ending_orderbook_ap_9, 'ending_orderbook_as_9':ending_orderbook_as_9,\n",
    "                                      'ending_orderbook_ap_10':ending_orderbook_ap_10, 'ending_orderbook_as_10':ending_orderbook_as_10,\n",
    "                                     'ending_orderbook_ap_11':ending_orderbook_ap_11, 'ending_orderbook_as_11':ending_orderbook_as_11,\n",
    "                                      'ending_orderbook_ap_12':ending_orderbook_ap_12, 'ending_orderbook_as_12':ending_orderbook_as_12,\n",
    "                                     'ending_orderbook_ap_13':ending_orderbook_ap_13, 'ending_orderbook_as_13':ending_orderbook_as_13,\n",
    "                                      'ending_orderbook_ap_14':ending_orderbook_ap_14, 'ending_orderbook_as_14':ending_orderbook_as_14,\n",
    "                                     'ending_orderbook_bp_0':ending_orderbook_bp_0, 'ending_orderbook_bs_0':ending_orderbook_bs_0,\n",
    "                                     'ending_orderbook_bp_1':ending_orderbook_bp_1, 'ending_orderbook_bs_1':ending_orderbook_bs_1,\n",
    "                                     'ending_orderbook_bp_2':ending_orderbook_bp_2, 'ending_orderbook_bs_2':ending_orderbook_bs_2,\n",
    "                                     'ending_orderbook_bp_3':ending_orderbook_bp_3, 'ending_orderbook_bs_3':ending_orderbook_bs_3,\n",
    "                                     'ending_orderbook_bp_4':ending_orderbook_bp_4, 'ending_orderbook_bs_4':ending_orderbook_bs_4,\n",
    "                                     'ending_orderbook_bp_5':ending_orderbook_bp_5, 'ending_orderbook_bs_5':ending_orderbook_bs_5,\n",
    "                                     'ending_orderbook_bp_6':ending_orderbook_bp_6, 'ending_orderbook_bs_6':ending_orderbook_bs_6,\n",
    "                                     'ending_orderbook_bp_7':ending_orderbook_bp_7, 'ending_orderbook_bs_7':ending_orderbook_bs_7,\n",
    "                                     'ending_orderbook_bp_8':ending_orderbook_bp_8, 'ending_orderbook_bs_8':ending_orderbook_bs_8,\n",
    "                                     'ending_orderbook_bp_9':ending_orderbook_bp_9, 'ending_orderbook_bs_9':ending_orderbook_bs_9,\n",
    "                                     'ending_orderbook_bp_10':ending_orderbook_bp_10, 'ending_orderbook_bs_10':ending_orderbook_bs_10,\n",
    "                                     'ending_orderbook_bp_11':ending_orderbook_bp_11, 'ending_orderbook_bs_11':ending_orderbook_bs_11,\n",
    "                                     'ending_orderbook_bp_12':ending_orderbook_bp_12, 'ending_orderbook_bs_12':ending_orderbook_bs_12,\n",
    "                                     'ending_orderbook_bp_13':ending_orderbook_bp_13, 'ending_orderbook_bs_13':ending_orderbook_bs_13,\n",
    "                                     'ending_orderbook_bp_14':ending_orderbook_bp_14, 'ending_orderbook_bs_14':ending_orderbook_bs_14})\n",
    "\n",
    "                    # print(data_to_insert)\n",
    "                    my_data = pd.concat([my_data, data_to_insert.to_frame().T], ignore_index=True)\n",
    "                    # new_length += 1\n",
    "                    # print('현재 {}째 줄이 생성되었습니다.'.format(new_length))\n",
    "                    # my_data = my_data.append(data_to_insert, ignore_index=True)\n",
    "\n",
    "        # pd.set_option('display.max_columns', None)\n",
    "        # my_data        \n",
    "        # my_data.to_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\bigdata_{}_{}_10s.csv\".format(coin, partition_list[p]))\n",
    "        # print('Complete: bigdata_{}_{}_10s'.format(coin,partition_list[p]))\n",
    "\n",
    "        my_data.to_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\bigdata_{}_{}_{}_m.csv\".format(coin, data_num, partition_list[p]))\n",
    "        print('Complete: bigdata_{}_{}_{}_m'.format(coin, data_num, partition_list[p]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443d567e-6842-4c87-a056-b0944686d3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c1e4ed-71d6-4d19-b9ed-cb71614cd0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d85fdbf-a7b1-42b7-ac5d-be533011ddc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d03ed39-cb68-4628-b9f8-a65b6608c84a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8895e441-d644-4034-b323-b39b75126af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097bb09-0fc1-4fba-83bc-85e040b8c69c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a39bcfd8-d965-47f0-99d8-9ec33f659469",
   "metadata": {},
   "source": [
    "## 1-3: 10초씩/1분 계산된 파일을 코인별로 하나로 concat 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60bfbaa-2551-4693-a582-bd60ce588039",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_list = list(range(0, 5000000, 1000000))\n",
    "# num_list = list(range(0, 79000000, 1000000))\n",
    "\n",
    "coin_list = ['BTC', 'ETH', 'DOGE', 'XRP']\n",
    "data_num = 2\n",
    "\n",
    "for coin in coin_list:\n",
    "\n",
    "    partition_list = os.listdir(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\big_data_{}_{}.csv\\\\\".format(coin, data_num)) \n",
    "    coin_concat = pd.DataFrame()\n",
    "    for p in range(len(partition_list)):\n",
    "\n",
    "        df = pd.read_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\bigdata_{}_{}_{}_m.csv\".format(coin, data_num, partition_list[p]))    \n",
    "        # bigdata_BTC_102.part_10s\n",
    "        coin_concat = pd.concat([coin_concat, df])\n",
    "\n",
    "    # coin_concat.to_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\concat_bigdata_{}_10s.csv\".format(coin))\n",
    "    coin_concat.to_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\concat_bigdata_{}_{}_m.csv\".format(coin, data_num))\n",
    "\n",
    "    \n",
    "    print(\"Successful concatnation for {}\".format(coin))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee950b8-a1e4-45c5-8d51-9df2b3c1a58b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c87613-c59a-4a8c-bdf6-48f89040e597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb681d46-6939-43c7-aaca-290022425ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdce2bf-8c0c-450d-99b2-b939909c3b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59015059-d018-4d9d-a82c-91cbbc9e4fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c027a435-494e-4484-b661-bce2d0826e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359d2147-708a-443e-8b85-11653694169a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e2073-92d4-47ee-bf40-0b7f9edcbfa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ce9c4b-8bba-476b-bb84-35019fa1fa49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab81002c-a0fd-474f-91b3-1f72fc36352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d24ade-b953-4da1-ae6a-1ce8fa21a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of lines of the csv file to be read\n",
    "number_lines = sum(1 for row in (open(in_csv)))\n",
    "print(number_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee751b-f477-4dd7-a7ad-da942e5210c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change the row size according to your need\n",
    "rowsize = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b77f2d4-2c26-456f-89e4-0e78a81de923",
   "metadata": {},
   "outputs": [],
   "source": [
    "range(0, number_lines, rowsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666aa4bd-de56-492a-96ac-1be91801b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(in_csv,\n",
    "                     nrows=3,  # number of rows to read at each loop\n",
    "                     skiprows=0)  # skip rows that have been read\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07383b66-5102-4bce-8ba3-9a2fd267d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_header = list(df.columns.values)\n",
    "my_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157cb61f-4df4-424c-838d-7fa2278d7766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subdir maker\n",
    "directory = 'output'\n",
    "parent_dir = 'D:\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트'\n",
    "# parent_dir = 'path\\\\rows'\n",
    "path = os.path.join(parent_dir, directory)\n",
    "print(path)\n",
    "os.mkdir(path)\n",
    "print('Directory created.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2f759f-2d2a-4133-8428-4f4d9dfdabd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start looping through data writing it to a new file for each set\n",
    "for i in range(0, number_lines, rowsize):\n",
    "    \n",
    "    df = pd.read_csv(in_csv, names = my_header,\n",
    "                     nrows=rowsize,  # number of rows to read at each loop\n",
    "                     skiprows=i)  # skip rows that have been read\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    # csv to write data to a new file with indexed name. input_1.csv etc.\n",
    "    out_csv = os.path.join(path, 'NewFile' + str(i) + '.csv')\n",
    "\n",
    "    print('Initiating slicer. Step:')\n",
    "    print(str(i))\n",
    "    print('-----------------------')\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    df.to_csv(out_csv,\n",
    "              index=False,\n",
    "              header=True,\n",
    "              mode='a',  # append data to csv file\n",
    "              chunksize=rowsize)  # size of data to append for each loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6fafb7-3e84-4cca-bb69-f07fb34ecdc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9860a4-4483-4c0b-8f37-f1387b474e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6ed5fb0-edac-46f7-a7b0-44accf265c3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1-2: chunked 된 파일을 4개의 코인별로 나누자. 그리고 시간별로 sorting까지 해서 저장하되, 라인 수가 적절하다면 concatenate까지 진행하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceb0930-26f5-4e93-b1f0-12fe9519d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517b5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621dc2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_list = list(range(0, 5000000, 1000000))\n",
    "# num_list = list(range(0, 79000000, 1000000))\n",
    "\n",
    "coin_list = ['BTC', 'ETH', 'DOGE', 'XRP']\n",
    "for i in range(0, 16): ## 0~16\n",
    "    for coin in coin_list:\n",
    "        coin_concat = pd.DataFrame()\n",
    "\n",
    "        num_list = list(range(0+5000000*i, 5000000+5000000*i, 1000000))\n",
    "        print(num_list)\n",
    "        \n",
    "        for num in num_list:\n",
    "            # df = pd.read_csv(\"data/NewFile{}.csv\".format(num))    \n",
    "            df = pd.read_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\NewFile{}.csv\".format(num))    \n",
    "\n",
    "            \"\"\"\n",
    "            if type(df.loc[0, 'opening_price']) == str:\n",
    "                df = df.drop([0])\n",
    "            \"\"\"\n",
    "\n",
    "            globals()[\"df_{}\".format(num)] = df\n",
    "\n",
    "            if coin == 'BTC':\n",
    "                globals()[\"df_BTC_{}\".format(num)] = globals()[\"df_{}\".format(num)][globals()[\"df_{}\".format(num)].code == 'KRW-BTC']\n",
    "            elif coin == 'ETH':\n",
    "                globals()[\"df_ETH_{}\".format(num)] = globals()[\"df_{}\".format(num)][globals()[\"df_{}\".format(num)].code == 'KRW-ETH']\n",
    "            elif coin == 'DOGE':\n",
    "                globals()[\"df_DOGE_{}\".format(num)] = globals()[\"df_{}\".format(num)][globals()[\"df_{}\".format(num)].code == 'KRW-DOGE']\n",
    "            else:\n",
    "                globals()[\"df_XRP_{}\".format(num)] = globals()[\"df_{}\".format(num)][globals()[\"df_{}\".format(num)].code == 'KRW-XRP']\n",
    "\n",
    "            # concatenating \n",
    "            coin_concat = pd.concat([coin_concat, globals()[\"df_{}_{}\".format(coin, num)]])\n",
    "\n",
    "        # object에서 datetime64[ns] 으로 변경\n",
    "        coin_concat.sys_datetime = pd.to_datetime(coin_concat['sys_datetime'], format='%Y-%m-%d %H:%M:%S.%f', errors='raise')\n",
    "        # sorting\n",
    "        coin_concat = coin_concat.sort_values('sys_datetime', ascending = True, na_position = 'last')\n",
    "\n",
    "        # 중복되는 행 제거\n",
    "        coin_concat = coin_concat.drop_duplicates(subset = ['trade_price','trade_volume','timestamp'])\n",
    "\n",
    "        # saving\n",
    "        coin_concat.to_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\{}_{}.csv\".format(coin, num))\n",
    "        print('Complete: {}_{}'.format(coin,num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1691589e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad06319f-b32f-4e36-b110-ebf4e4b2350a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23275357-cf5a-443c-8047-cc4c0154d7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "116edc1c-6ac8-4d91-8d09-97ad24a1e545",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1-3: 각 파일을 10초씩 묶는 전처리 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0971b2ae-a334-46bc-86ef-3b0c599d59fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일을 하나씩 열어서 10초씩 묶는 전처리를 수행하자.\n",
    "# 이후 6*100개(100분)를 look back 함으로써 단기 분석을 수행하고 향후 (10초는 뛰어넘고), 5분 혹은 10분간 수익률이 어떻게 될 것인지 추정한다.\n",
    "# 또다른 프로젝트로는 변동성을 예측. Ex) 최근 10시간을 분석한 후 향후 1시간의 변동성을 예측하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062e0fa3-45a0-4655-b2aa-6982c8d39696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc551aa2-4ac5-4dbb-896e-f8793496b794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일을 for문 안에서 하나씩 열기\n",
    "coin_list = ['BTC', 'ETH', 'DOGE', 'XRP']\n",
    "num_list = list(range(0, 79000000, 1000000))\n",
    "\n",
    "for num in range(0, 16):\n",
    "    for coin in coin_list:\n",
    "        df2 = pd.read_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\{}_{}.csv\".format(coin, num*5000000+4000000))\n",
    "        print('Just opened: {}_{}.csv'.format(coin,num*5000000+4000000))\n",
    "\n",
    "        # 새롭게 데이터를 작성할 틀 만들기 (my_data)\n",
    "        my_data = pd.DataFrame(columns=['datetime_10s', 'beginning_orderbook_midprice','beginning_price', 'highest_price',\n",
    "                                   'lowest_price', 'ending_price','ending_orderbook_midprice',\n",
    "                                   'trading_volume', 'volume_power','trade_volume_bid', 'trade_volume_ask',\n",
    "                                   'ending_orderbook_total_ask_size', 'ending_orderbook_total_bid_size', \n",
    "                                    'ending_orderbook_ap_0','ending_orderbook_as_0',\n",
    "                                    'ending_orderbook_ap_1','ending_orderbook_as_1',\n",
    "                                   'ending_orderbook_ap_2','ending_orderbook_as_2',\n",
    "                                   'ending_orderbook_ap_3','ending_orderbook_as_3',\n",
    "                                   'ending_orderbook_ap_4','ending_orderbook_as_4',\n",
    "                                   'ending_orderbook_ap_5','ending_orderbook_as_5',\n",
    "                                   'ending_orderbook_ap_6','ending_orderbook_as_6',\n",
    "                                   'ending_orderbook_ap_7','ending_orderbook_as_7',\n",
    "                                   'ending_orderbook_ap_8','ending_orderbook_as_8',\n",
    "                                   'ending_orderbook_ap_9','ending_orderbook_as_9',\n",
    "                                   'ending_orderbook_ap_10','ending_orderbook_as_10',\n",
    "                                   'ending_orderbook_ap_11','ending_orderbook_as_11',\n",
    "                                   'ending_orderbook_ap_12','ending_orderbook_as_12',\n",
    "                                   'ending_orderbook_ap_13','ending_orderbook_as_13',\n",
    "                                   'ending_orderbook_ap_14','ending_orderbook_as_14',\n",
    "                                    'ending_orderbook_bp_0','ending_orderbook_bs_0',\n",
    "                                    'ending_orderbook_bp_1','ending_orderbook_bs_1',\n",
    "                                   'ending_orderbook_bp_2','ending_orderbook_bs_2',\n",
    "                                   'ending_orderbook_bp_3','ending_orderbook_bs_3',\n",
    "                                   'ending_orderbook_bp_4','ending_orderbook_bs_4',\n",
    "                                   'ending_orderbook_bp_5','ending_orderbook_bs_5',\n",
    "                                   'ending_orderbook_bp_6','ending_orderbook_bs_6',\n",
    "                                   'ending_orderbook_bp_7','ending_orderbook_bs_7',\n",
    "                                   'ending_orderbook_bp_8','ending_orderbook_bs_8',\n",
    "                                   'ending_orderbook_bp_9','ending_orderbook_bs_9',\n",
    "                                   'ending_orderbook_bp_10','ending_orderbook_bs_10',\n",
    "                                   'ending_orderbook_bp_11','ending_orderbook_bs_11',\n",
    "                                   'ending_orderbook_bp_12','ending_orderbook_bs_12',\n",
    "                                   'ending_orderbook_bp_13','ending_orderbook_bs_13',\n",
    "                                   'ending_orderbook_bp_14','ending_orderbook_bs_14'])\n",
    "        \n",
    "        ## 10초 동안 다음의 변수값을 도출 할 예정 (일단 모두 0으로 세팅)\n",
    "        datetime_10s=0 \n",
    "        beginning_orderbook_midprice = 0 ## ask price 0과 bid price 0의 평균 \n",
    "        beginning_price = 0 ## 10초 시작하고 시가\n",
    "        highest_price = 0 ## 고가\n",
    "        lowest_price = 0 ## 저가\n",
    "        ending_price = 0 ## 10초 끝나기 전 시가\n",
    "        ending_orderbook_midprice = 0\n",
    "\n",
    "        trading_volume = 0\n",
    "        volume_power = 0  ## 체결강도 (BID인 volume / ASK인 volume) ## bid(매수)가 더 많은지 ask(매도)가 더 많은지 비율로 \n",
    "\n",
    "        trade_volume_bid = 0 ## voluem power 계산용\n",
    "        trade_volume_ask = 0 ## voluem power 계산용\n",
    "\n",
    "        ending_orderbook_total_ask_size = 0\n",
    "        ending_orderbook_total_bid_size = 0\n",
    "        ending_orderbook_ap_0 = 0\n",
    "        ending_orderbook_as_0 = 0\n",
    "        ending_orderbook_ap_1 = 0\n",
    "        ending_orderbook_as_1 = 0\n",
    "        ending_orderbook_ap_2 = 0\n",
    "        ending_orderbook_as_2 = 0\n",
    "        ending_orderbook_ap_3 = 0\n",
    "        ending_orderbook_as_3 = 0\n",
    "        ending_orderbook_ap_4 = 0\n",
    "        ending_orderbook_as_4 = 0\n",
    "        ending_orderbook_ap_5 = 0\n",
    "        ending_orderbook_as_5 = 0\n",
    "        ending_orderbook_ap_6 = 0\n",
    "        ending_orderbook_as_6 = 0\n",
    "        ending_orderbook_ap_7 = 0\n",
    "        ending_orderbook_as_7 = 0\n",
    "        ending_orderbook_ap_8 = 0\n",
    "        ending_orderbook_as_8 = 0\n",
    "        ending_orderbook_ap_9 = 0\n",
    "        ending_orderbook_as_9 = 0\n",
    "        ending_orderbook_ap_10 = 0\n",
    "        ending_orderbook_as_10 = 0\n",
    "        ending_orderbook_ap_11 = 0\n",
    "        ending_orderbook_as_11 = 0\n",
    "        ending_orderbook_ap_12 = 0\n",
    "        ending_orderbook_as_12 = 0\n",
    "        ending_orderbook_ap_13 = 0\n",
    "        ending_orderbook_as_13 = 0\n",
    "        ending_orderbook_ap_14 = 0\n",
    "        ending_orderbook_as_14 = 0\n",
    "\n",
    "        ending_orderbook_bp_0 = 0\n",
    "        ending_orderbook_bs_0 = 0\n",
    "        ending_orderbook_bp_1 = 0\n",
    "        ending_orderbook_bs_1 = 0\n",
    "        ending_orderbook_bp_2 = 0\n",
    "        ending_orderbook_bs_2 = 0\n",
    "        ending_orderbook_bp_3 = 0\n",
    "        ending_orderbook_bs_3 = 0\n",
    "        ending_orderbook_bp_4 = 0\n",
    "        ending_orderbook_bs_4 = 0\n",
    "        ending_orderbook_bp_5 = 0\n",
    "        ending_orderbook_bs_5 = 0\n",
    "        ending_orderbook_bp_6 = 0\n",
    "        ending_orderbook_bs_6 = 0\n",
    "        ending_orderbook_bp_7 = 0\n",
    "        ending_orderbook_bs_7 = 0\n",
    "        ending_orderbook_bp_8 = 0\n",
    "        ending_orderbook_bs_8 = 0\n",
    "        ending_orderbook_bp_9 = 0\n",
    "        ending_orderbook_bs_9 = 0\n",
    "        ending_orderbook_bp_10 = 0\n",
    "        ending_orderbook_bs_10 = 0\n",
    "        ending_orderbook_bp_11 = 0\n",
    "        ending_orderbook_bs_11 = 0\n",
    "        ending_orderbook_bp_12 = 0\n",
    "        ending_orderbook_bs_12 = 0\n",
    "        ending_orderbook_bp_13 = 0\n",
    "        ending_orderbook_bs_13 = 0\n",
    "        ending_orderbook_bp_14 = 0\n",
    "        ending_orderbook_bs_14 = 0\n",
    "\n",
    "\n",
    "        ## 데이터 첫 row 및 last row의 timestamp 측정:\n",
    "        df2['datetime'] = pd.to_datetime(df2['datetime'])\n",
    "        initial_timestamp = '' + str(df2['datetime'].iloc[0].year) + str(df2['datetime'].iloc[0].month) + str(df2['datetime'].iloc[0].day) + str(df2['datetime'].iloc[0].hour) + str(df2['datetime'].iloc[0].minute) + str(df2['datetime'].iloc[0].second//10)\n",
    "        last_timestamp = '' + str(df2['datetime'].iloc[df2.shape[0]-1].year) + str(df2['datetime'].iloc[df2.shape[0]-1].month) + str(df2['datetime'].iloc[df2.shape[0]-1].day) + str(df2['datetime'].iloc[df2.shape[0]-1].hour) + str(df2['datetime'].iloc[df2.shape[0]-1].minute) + str(df2['datetime'].iloc[df2.shape[0]-1].second//10)\n",
    "\n",
    "        data_len = df2.shape[0]\n",
    "        print(\"데이터{}_{}의 길이: {}\".format(coin, num, data_len))\n",
    "        \n",
    "        # new_length = 0\n",
    "        for i in range(data_len):  \n",
    "\n",
    "            ## 처음구간은 10초를 채우기 어렵기 때문에 자른다. 마지막 10초도 마찬가지로 자른다    \n",
    "            current_timestamp = '' + str(df2['datetime'].iloc[i].year) + str(df2['datetime'].iloc[i].month) + str(df2['datetime'].iloc[i].day) + str(df2['datetime'].iloc[i].hour) + str(df2['datetime'].iloc[i].minute) + str(df2['datetime'].iloc[i].second//10)\n",
    "           \n",
    "            if  current_timestamp == initial_timestamp or current_timestamp == last_timestamp : ## 초가 같은 부분에 자른다. \n",
    "                print(\"--이 파일의 처음 10초 구간 혹은 이 파일의 마지막 10초 구간--\")\n",
    "                continue ## 따라서 초의 2번째 자리(49초 -> 50초)가 달라지면 else로 넘어간다. \n",
    "            else:\n",
    "\n",
    "                previous_timestamp = '' + str(df2['datetime'].iloc[i-1].year) + str(df2['datetime'].iloc[i-1].month) + str(df2['datetime'].iloc[i-1].day) + str(df2['datetime'].iloc[i-1].hour) + str(df2['datetime'].iloc[i-1].minute) + str(df2['datetime'].iloc[i-1].second//10)\n",
    "                if previous_timestamp != current_timestamp: ## 10초 단위의 첫번째 줄이라면? 도출해야할 값들 초기화.\n",
    "                    # 스타팅 인덱스 찍어두고,\n",
    "                    index_start_10s = i\n",
    "                    # previous_timestamp와 current_timestamp와 다를 경우. ex) previous가 49초이고 current가 50초일 때\n",
    "                    # 이 때만 모든 값을 초기화 한다. \n",
    "\n",
    "                    datetime_10s = df2['datetime'].iloc[i]\n",
    "                    beginning_orderbook_midprice = 0\n",
    "                    beginning_price = 0\n",
    "                    highest_price = 0\n",
    "                    lowest_price = 0\n",
    "                    ending_price = 0\n",
    "                    ending_orderbook_midprice = 0\n",
    "                    trading_volume = 0\n",
    "                    volume_power = 0  \n",
    "                    trade_volume_bid = 0 \n",
    "                    trade_volume_ask = 0 \n",
    "                    ending_orderbook_total_ask_size = 0\n",
    "                    ending_orderbook_total_bid_size = 0\n",
    "                    ending_orderbook_ap_0 = 0\n",
    "                    ending_orderbook_as_0 = 0\n",
    "                    ending_orderbook_ap_1 = 0\n",
    "                    ending_orderbook_as_1 = 0\n",
    "                    ending_orderbook_ap_2 = 0\n",
    "                    ending_orderbook_as_2 = 0\n",
    "                    ending_orderbook_ap_3 = 0\n",
    "                    ending_orderbook_as_3 = 0\n",
    "                    ending_orderbook_ap_4 = 0\n",
    "                    ending_orderbook_as_4 = 0\n",
    "                    ending_orderbook_ap_5 = 0\n",
    "                    ending_orderbook_as_5 = 0\n",
    "                    ending_orderbook_ap_6 = 0\n",
    "                    ending_orderbook_as_6 = 0\n",
    "                    ending_orderbook_ap_7 = 0\n",
    "                    ending_orderbook_as_7 = 0\n",
    "                    ending_orderbook_ap_8 = 0\n",
    "                    ending_orderbook_as_8 = 0\n",
    "                    ending_orderbook_ap_9 = 0\n",
    "                    ending_orderbook_as_9 = 0\n",
    "                    ending_orderbook_ap_10 = 0\n",
    "                    ending_orderbook_as_10 = 0\n",
    "                    ending_orderbook_ap_11 = 0\n",
    "                    ending_orderbook_as_11 = 0\n",
    "                    ending_orderbook_ap_12 = 0\n",
    "                    ending_orderbook_as_12 = 0\n",
    "                    ending_orderbook_ap_13 = 0\n",
    "                    ending_orderbook_as_13 = 0\n",
    "                    ending_orderbook_ap_14 = 0\n",
    "                    ending_orderbook_as_14 = 0\n",
    "                    ending_orderbook_bp_0 = 0\n",
    "                    ending_orderbook_bs_0 = 0\n",
    "                    ending_orderbook_bp_1 = 0\n",
    "                    ending_orderbook_bs_1 = 0\n",
    "                    ending_orderbook_bp_2 = 0\n",
    "                    ending_orderbook_bs_2 = 0\n",
    "                    ending_orderbook_bp_3 = 0\n",
    "                    ending_orderbook_bs_3 = 0\n",
    "                    ending_orderbook_bp_4 = 0\n",
    "                    ending_orderbook_bs_4 = 0\n",
    "                    ending_orderbook_bp_5 = 0\n",
    "                    ending_orderbook_bs_5 = 0\n",
    "                    ending_orderbook_bp_6 = 0\n",
    "                    ending_orderbook_bs_6 = 0\n",
    "                    ending_orderbook_bp_7 = 0\n",
    "                    ending_orderbook_bs_7 = 0\n",
    "                    ending_orderbook_bp_8 = 0\n",
    "                    ending_orderbook_bs_8 = 0\n",
    "                    ending_orderbook_bp_9 = 0\n",
    "                    ending_orderbook_bs_9 = 0\n",
    "                    ending_orderbook_bp_10 = 0\n",
    "                    ending_orderbook_bs_10 = 0\n",
    "                    ending_orderbook_bp_11 = 0\n",
    "                    ending_orderbook_bs_11 = 0\n",
    "                    ending_orderbook_bp_12 = 0\n",
    "                    ending_orderbook_bs_12 = 0\n",
    "                    ending_orderbook_bp_13 = 0\n",
    "                    ending_orderbook_bs_13 = 0\n",
    "                    ending_orderbook_bp_14 = 0\n",
    "                    ending_orderbook_bs_14 = 0\n",
    "\n",
    "\n",
    "                ## orderbook 타입\n",
    "                if df2['type_websocket'].iloc[i]  == 'orderbook':\n",
    "                    if beginning_orderbook_midprice == 0: ## 아직 업데이트가 안 되었다면? (만약을 대비)\n",
    "                        beginning_orderbook_midprice = (df2['orderbook_ap_0'].iloc[i] + df2['orderbook_bp_0'].iloc[i])/2\n",
    "                                    ## ask price 0과 bid price 0의 평균 \n",
    "\n",
    "                ## trade/ticker 타입 (거래발생)\n",
    "                elif df2['type_websocket'].iloc[i]  == 'trade' or df2['type_websocket'].iloc[i]  == 'ticker':\n",
    "                    if beginning_price==0: ## 아직 업데이트가 안 되었다면?\n",
    "                        beginning_price = df2['trade_price'].iloc[i]\n",
    "\n",
    "                    if highest_price <  df2['trade_price'].iloc[i]: ## 고가 수정\n",
    "                        highest_price = df2['trade_price'].iloc[i]\n",
    "                    if lowest_price == 0 or lowest_price > df2['trade_price'].iloc[i]: ## 저가 수정\n",
    "                        lowest_price = df2['trade_price'].iloc[i]\n",
    "\n",
    "                    trading_volume += df2['trade_volume'].iloc[i] ## 거래량 더해주기\n",
    "\n",
    "                    # ask_bid => 매수/매도 구분\n",
    "                    if df2['ask_bid'].iloc[i] =='BID': ## 체결강도 계산용 BID/ASK 거래량 더해주기\n",
    "                        trade_volume_bid += df2['trade_volume'].iloc[i]\n",
    "                    elif df2['ask_bid'].iloc[i] =='ASK':\n",
    "                        trade_volume_ask += df2['trade_volume'].iloc[i]\n",
    "\n",
    "                ## 10초의 마지막 줄임을 발견\n",
    "\n",
    "                next_timestamp = '' + str(df2['datetime'].iloc[i+1].year) + str(df2['datetime'].iloc[i+1].month) + str(df2['datetime'].iloc[i+1].day) + str(df2['datetime'].iloc[i+1].hour) + str(df2['datetime'].iloc[i+1].minute) + str(df2['datetime'].iloc[i+1].second//10)\n",
    "                if current_timestamp != next_timestamp: ## if current_timestamp이 49초이고 next가 50초 이면 print\n",
    "                    \n",
    "                    # print(\"--10초의 마지막줄처리--\")\n",
    "                    # print(\"현재 index: {}\".format(i)) ## 현재 index\n",
    "                    # print(current_timestamp)\n",
    "                    # print(next_timestamp)\n",
    "                    # # if current_timestamp_plus_10s != next_timestamp: raise Exception(\"10초 동안 아무런 데이터가 없음.\")\n",
    "                    \n",
    "                    j = 0\n",
    "                    while ending_price == 0 or ending_orderbook_midprice==0: ## 둘 중에 하나라도 0이면 계속 while\n",
    "                        # print(\"i-j:\"+ str(i-j))\n",
    "                        ## 여기서 i = 193이라고 치면 j가 1씩 추가될 때 마다 i가 줄어드니까 하나 씩 전으로 돌아간다. \n",
    "\n",
    "                        if (df2['type_websocket'].iloc[i-j]  == 'trade' or df2['type_websocket'].iloc[i-j]  == 'ticker') and (ending_price==0):\n",
    "                            ending_price = df2['trade_price'].iloc[i-j]\n",
    "                            if beginning_price==0: \n",
    "                                beginning_price = ending_price\n",
    "                                highest_price = ending_price\n",
    "                                lowest_price = ending_price\n",
    "                                \n",
    "                        # i = 193 j= 0 인덱스:193이 trade or ticker면 그 값을 ending price로\n",
    "\n",
    "                        elif (df2['type_websocket'].iloc[i-j]  == 'orderbook') & (ending_orderbook_midprice==0):\n",
    "                            ending_orderbook_midprice = (df2['orderbook_ap_0'].iloc[i-j] +  df2['orderbook_bp_0'].iloc[i-j])/2\n",
    "                            ending_orderbook_total_ask_size = df2['total_ask_size'].iloc[i-j]\n",
    "                            ending_orderbook_total_bid_size = df2['total_bid_size'].iloc[i-j]\n",
    "                            ending_orderbook_ap_0 = df2['orderbook_ap_0'].iloc[i-j]/ending_orderbook_midprice  ## 나누는 이유: orderbook 평균가 기준 얼마나 높은지 비율로 따지기 위해 \n",
    "                            ending_orderbook_as_0 = df2['orderbook_as_0'].iloc[i-j]\n",
    "                            ending_orderbook_ap_1 = df2['orderbook_ap_1'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_1 = df2['orderbook_as_1'].iloc[i-j]\n",
    "                            ending_orderbook_ap_2 = df2['orderbook_ap_2'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_2 = df2['orderbook_as_2'].iloc[i-j]\n",
    "                            ending_orderbook_ap_3 = df2['orderbook_ap_3'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_3 = df2['orderbook_as_3'].iloc[i-j]\n",
    "                            ending_orderbook_ap_4 = df2['orderbook_ap_4'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_4 = df2['orderbook_as_4'].iloc[i-j]\n",
    "                            ending_orderbook_ap_5 = df2['orderbook_ap_5'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_5 = df2['orderbook_as_5'].iloc[i-j]\n",
    "                            ending_orderbook_ap_6 = df2['orderbook_ap_6'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_6 = df2['orderbook_as_6'].iloc[i-j]\n",
    "                            ending_orderbook_ap_7 = df2['orderbook_ap_7'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_7 = df2['orderbook_as_7'].iloc[i-j]\n",
    "                            ending_orderbook_ap_8 = df2['orderbook_ap_8'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_8 = df2['orderbook_as_8'].iloc[i-j]\n",
    "                            ending_orderbook_ap_9 = df2['orderbook_ap_9'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_9 = df2['orderbook_as_9'].iloc[i-j]\n",
    "                            ending_orderbook_ap_10 = df2['orderbook_ap_10'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_10 = df2['orderbook_as_10'].iloc[i-j]\n",
    "                            ending_orderbook_ap_11 = df2['orderbook_ap_11'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_11 = df2['orderbook_as_11'].iloc[i-j]\n",
    "                            ending_orderbook_ap_12 = df2['orderbook_ap_12'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_12 = df2['orderbook_as_12'].iloc[i-j]\n",
    "                            ending_orderbook_ap_13 = df2['orderbook_ap_13'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_13 = df2['orderbook_as_13'].iloc[i-j]\n",
    "                            ending_orderbook_ap_14 = df2['orderbook_ap_14'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_as_14 = df2['orderbook_as_14'].iloc[i-j]\n",
    "                            ending_orderbook_bp_0 = df2['orderbook_bp_0'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_0 = df2['orderbook_bs_0'].iloc[i-j]\n",
    "                            ending_orderbook_bp_1 = df2['orderbook_bp_1'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_1 = df2['orderbook_bs_1'].iloc[i-j]\n",
    "                            ending_orderbook_bp_2 = df2['orderbook_bp_2'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_2 = df2['orderbook_bs_2'].iloc[i-j]\n",
    "                            ending_orderbook_bp_3 = df2['orderbook_bp_3'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_3 = df2['orderbook_bs_3'].iloc[i-j]\n",
    "                            ending_orderbook_bp_4 = df2['orderbook_bp_4'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_4 = df2['orderbook_bs_4'].iloc[i-j]\n",
    "                            ending_orderbook_bp_5 = df2['orderbook_bp_5'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_5 = df2['orderbook_bs_5'].iloc[i-j]\n",
    "                            ending_orderbook_bp_6 = df2['orderbook_bp_6'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_6 = df2['orderbook_bs_6'].iloc[i-j]\n",
    "                            ending_orderbook_bp_7 = df2['orderbook_bp_7'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_7 = df2['orderbook_bs_7'].iloc[i-j]\n",
    "                            ending_orderbook_bp_8 = df2['orderbook_bp_8'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_8 = df2['orderbook_bs_8'].iloc[i-j]\n",
    "                            ending_orderbook_bp_9 = df2['orderbook_bp_9'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_9 = df2['orderbook_bs_9'].iloc[i-j]\n",
    "                            ending_orderbook_bp_10 = df2['orderbook_bp_10'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_10 = df2['orderbook_bs_10'].iloc[i-j]\n",
    "                            ending_orderbook_bp_11 = df2['orderbook_bp_11'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_11 = df2['orderbook_bs_11'].iloc[i-j]\n",
    "                            ending_orderbook_bp_12 = df2['orderbook_bp_12'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_12 = df2['orderbook_bs_12'].iloc[i-j]\n",
    "                            ending_orderbook_bp_13 = df2['orderbook_bp_13'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_13 = df2['orderbook_bs_13'].iloc[i-j]\n",
    "                            ending_orderbook_bp_14 = df2['orderbook_bp_14'].iloc[i-j]/ending_orderbook_midprice\n",
    "                            ending_orderbook_bs_14 = df2['orderbook_bs_14'].iloc[i-j]                 \n",
    "                              \n",
    "                            ## 10초 동안 orderbook 변화가 없었다면, 가장 최근 orderbook 정보가 beginning에 있어야. \n",
    "                            if beginning_orderbook_midprice == 0 :  beginning_orderbook_midprice = ending_orderbook_midprice \n",
    "                            \n",
    "                        j += 1\n",
    "\n",
    "                    volume_power = trade_volume_bid-trade_volume_ask\n",
    "\n",
    "                    data_to_insert = pd.Series({'datetime_10s':datetime_10s, 'beginning_orderbook_midprice':beginning_orderbook_midprice,\n",
    "                                     'beginning_price':beginning_price, 'highest_price':highest_price,\n",
    "                                     'highest_price':highest_price, 'lowest_price':lowest_price,\n",
    "                                     'ending_price':ending_price, 'ending_orderbook_midprice':ending_orderbook_midprice,\n",
    "                                     'trading_volume':trading_volume, 'volume_power':volume_power,\n",
    "                                      'trade_volume_bid':trade_volume_bid, 'trade_volume_ask':trade_volume_ask,\n",
    "                                     'ending_orderbook_total_ask_size':ending_orderbook_total_ask_size, 'ending_orderbook_total_bid_size':ending_orderbook_total_bid_size,\n",
    "                                     'ending_orderbook_ap_0':ending_orderbook_ap_0, 'ending_orderbook_as_0':ending_orderbook_as_0,\n",
    "                                     'ending_orderbook_ap_1':ending_orderbook_ap_1, 'ending_orderbook_as_1':ending_orderbook_as_1,\n",
    "                                     'ending_orderbook_ap_2':ending_orderbook_ap_2, 'ending_orderbook_as_2':ending_orderbook_as_2,\n",
    "                                     'ending_orderbook_ap_3':ending_orderbook_ap_3, 'ending_orderbook_as_3':ending_orderbook_as_3,\n",
    "                                      'ending_orderbook_ap_4':ending_orderbook_ap_4, 'ending_orderbook_as_4':ending_orderbook_as_4,\n",
    "                                     'ending_orderbook_ap_5':ending_orderbook_ap_5, 'ending_orderbook_as_5':ending_orderbook_as_5,\n",
    "                                      'ending_orderbook_ap_6':ending_orderbook_ap_6, 'ending_orderbook_as_6':ending_orderbook_as_6,\n",
    "                                     'ending_orderbook_ap_7':ending_orderbook_ap_7, 'ending_orderbook_as_7':ending_orderbook_as_7,\n",
    "                                      'ending_orderbook_ap_8':ending_orderbook_ap_8, 'ending_orderbook_as_8':ending_orderbook_as_8,\n",
    "                                     'ending_orderbook_ap_9':ending_orderbook_ap_9, 'ending_orderbook_as_9':ending_orderbook_as_9,\n",
    "                                      'ending_orderbook_ap_10':ending_orderbook_ap_10, 'ending_orderbook_as_10':ending_orderbook_as_10,\n",
    "                                     'ending_orderbook_ap_11':ending_orderbook_ap_11, 'ending_orderbook_as_11':ending_orderbook_as_11,\n",
    "                                      'ending_orderbook_ap_12':ending_orderbook_ap_12, 'ending_orderbook_as_12':ending_orderbook_as_12,\n",
    "                                     'ending_orderbook_ap_13':ending_orderbook_ap_13, 'ending_orderbook_as_13':ending_orderbook_as_13,\n",
    "                                      'ending_orderbook_ap_14':ending_orderbook_ap_14, 'ending_orderbook_as_14':ending_orderbook_as_14,\n",
    "                                     'ending_orderbook_bp_0':ending_orderbook_bp_0, 'ending_orderbook_bs_0':ending_orderbook_bs_0,\n",
    "                                     'ending_orderbook_bp_1':ending_orderbook_bp_1, 'ending_orderbook_bs_1':ending_orderbook_bs_1,\n",
    "                                     'ending_orderbook_bp_2':ending_orderbook_bp_2, 'ending_orderbook_bs_2':ending_orderbook_bs_2,\n",
    "                                     'ending_orderbook_bp_3':ending_orderbook_bp_3, 'ending_orderbook_bs_3':ending_orderbook_bs_3,\n",
    "                                     'ending_orderbook_bp_4':ending_orderbook_bp_4, 'ending_orderbook_bs_4':ending_orderbook_bs_4,\n",
    "                                     'ending_orderbook_bp_5':ending_orderbook_bp_5, 'ending_orderbook_bs_5':ending_orderbook_bs_5,\n",
    "                                     'ending_orderbook_bp_6':ending_orderbook_bp_6, 'ending_orderbook_bs_6':ending_orderbook_bs_6,\n",
    "                                     'ending_orderbook_bp_7':ending_orderbook_bp_7, 'ending_orderbook_bs_7':ending_orderbook_bs_7,\n",
    "                                     'ending_orderbook_bp_8':ending_orderbook_bp_8, 'ending_orderbook_bs_8':ending_orderbook_bs_8,\n",
    "                                     'ending_orderbook_bp_9':ending_orderbook_bp_9, 'ending_orderbook_bs_9':ending_orderbook_bs_9,\n",
    "                                     'ending_orderbook_bp_10':ending_orderbook_bp_10, 'ending_orderbook_bs_10':ending_orderbook_bs_10,\n",
    "                                     'ending_orderbook_bp_11':ending_orderbook_bp_11, 'ending_orderbook_bs_11':ending_orderbook_bs_11,\n",
    "                                     'ending_orderbook_bp_12':ending_orderbook_bp_12, 'ending_orderbook_bs_12':ending_orderbook_bs_12,\n",
    "                                     'ending_orderbook_bp_13':ending_orderbook_bp_13, 'ending_orderbook_bs_13':ending_orderbook_bs_13,\n",
    "                                     'ending_orderbook_bp_14':ending_orderbook_bp_14, 'ending_orderbook_bs_14':ending_orderbook_bs_14})\n",
    "                    \n",
    "                    # print(data_to_insert)\n",
    "                    my_data = pd.concat([my_data, data_to_insert.to_frame().T], ignore_index=True)\n",
    "                    # new_length += 1\n",
    "                    # print('현재 {}째 줄이 생성되었습니다.'.format(new_length))\n",
    "                    # my_data = my_data.append(data_to_insert, ignore_index=True)\n",
    "\n",
    "        # pd.set_option('display.max_columns', None)\n",
    "        # my_data        \n",
    "        my_data.to_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\{}_{}_10s.csv\".format(coin, num*5000000+4000000))\n",
    "        print('Complete: {}_{}_10s'.format(coin,num*5000000+4000000))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64597195-ece8-45f4-8c7b-7df24d060d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 매 line이 10초씩 넘어가고 있는지 확인해야. 끝부분에 문제가 있다.\n",
    "## 연속적이지 않은 파일을 어떻게 concatenate 한단 말인가?\n",
    "## 학습 데이터 한줄을 3 dimension으로 만들어야. 한줄에 100분 데이터가 다 들어가도록. y를 그에 맞추어 만들어야 한다.\n",
    "\n",
    "## 한줄에서 다음줄로 넘어갈 때 시간이 20초 가 넘는다.라고 하면, 전후로 100줄 씩 없애기. 이런 작업을 해야 할 것 같다. 그래야 안전.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe01029b-ccaf-43f1-92ea-080534c5fe7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3473b4e-5436-4b73-9a94-d6b5ed77835e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca88f59-f192-4618-9fdb-deb468c113a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86028ea-5c29-4b52-9954-e2066b330d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb919e95-f1fe-4295-bbbc-46b96a1579d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f68b9a7-8ffa-49b2-ad76-b631e5b9230b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1663ccf-02d1-4385-af08-772a3c22c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btc = pd.read_csv(\"test/BTC.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd4d020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_btc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9271cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btc['datetime'] = pd.to_datetime(df_btc['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b0536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btc['datetime'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de57991",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initial_datetime = df_btc['datetime'][0]\n",
    "initial_datetime_yeartosecond = '' + str(df_btc['datetime'][0].year) + str(df_btc['datetime'][0].month) + str(df_btc['datetime'][0].day) + str(df_btc['datetime'][0].hour) + str(df_btc['datetime'][0].minute) + str(df_btc['datetime'][0].second)\n",
    "initial_datetime_yeartosecond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45caf874",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.DataFrame(columns=['datetime_10s', 'beginning_orderbook_midprice','beginning_price', 'highest_price',\n",
    "                               'lowest_price', 'ending_price','ending_orderbook_midprice',\n",
    "                               'trading_volume', 'volume_power','trade_volume_bid', 'trade_volume_ask',\n",
    "                               'ending_orderbook_total_ask_size', 'ending_orderbook_total_bid_size', \n",
    "                                'ending_orderbook_ap_0','ending_orderbook_as_0',\n",
    "                                'ending_orderbook_ap_1','ending_orderbook_as_1',\n",
    "                               'ending_orderbook_ap_2','ending_orderbook_as_2',\n",
    "                               'ending_orderbook_ap_3','ending_orderbook_as_3',\n",
    "                               'ending_orderbook_ap_4','ending_orderbook_as_4',\n",
    "                               'ending_orderbook_ap_5','ending_orderbook_as_5',\n",
    "                               'ending_orderbook_ap_6','ending_orderbook_as_6',\n",
    "                               'ending_orderbook_ap_7','ending_orderbook_as_7',\n",
    "                               'ending_orderbook_ap_8','ending_orderbook_as_8',\n",
    "                               'ending_orderbook_ap_9','ending_orderbook_as_9',\n",
    "                               'ending_orderbook_ap_10','ending_orderbook_as_10',\n",
    "                               'ending_orderbook_ap_11','ending_orderbook_as_11',\n",
    "                               'ending_orderbook_ap_12','ending_orderbook_as_12',\n",
    "                               'ending_orderbook_ap_13','ending_orderbook_as_13',\n",
    "                               'ending_orderbook_ap_14','ending_orderbook_as_14',\n",
    "                                'ending_orderbook_bp_0','ending_orderbook_bs_0',\n",
    "                                'ending_orderbook_bp_1','ending_orderbook_bs_1',\n",
    "                               'ending_orderbook_bp_2','ending_orderbook_bs_2',\n",
    "                               'ending_orderbook_bp_3','ending_orderbook_bs_3',\n",
    "                               'ending_orderbook_bp_4','ending_orderbook_bs_4',\n",
    "                               'ending_orderbook_bp_5','ending_orderbook_bs_5',\n",
    "                               'ending_orderbook_bp_6','ending_orderbook_bs_6',\n",
    "                               'ending_orderbook_bp_7','ending_orderbook_bs_7',\n",
    "                               'ending_orderbook_bp_8','ending_orderbook_bs_8',\n",
    "                               'ending_orderbook_bp_9','ending_orderbook_bs_9',\n",
    "                               'ending_orderbook_bp_10','ending_orderbook_bs_10',\n",
    "                               'ending_orderbook_bp_11','ending_orderbook_bs_11',\n",
    "                               'ending_orderbook_bp_12','ending_orderbook_bs_12',\n",
    "                               'ending_orderbook_bp_13','ending_orderbook_bs_13',\n",
    "                               'ending_orderbook_bp_14','ending_orderbook_bs_14'\n",
    "                               ])\n",
    "my_data\n",
    "# df = pd.DataFrame(data=my_data)\n",
    "# ##df_sum_10s = \n",
    "# df\n",
    "\n",
    "# data_to_insert = {'datetime':1234, '국어점수':60}\n",
    "\n",
    "# df = df.append(data_to_insert, ignore_index=True)\n",
    "# df.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e527e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_datetime_yeartosecond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7873e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_timestamp = '' + str(df_btc['datetime'].iloc[0].year) + str(df_btc['datetime'].iloc[0].month) + str(df_btc['datetime'].iloc[0].day) + str(df_btc['datetime'].iloc[0].hour) + str(df_btc['datetime'].iloc[0].minute) + str(df_btc['datetime'].iloc[0].second//10)\n",
    "initial_timestamp ## 초 단위를 10으로 나눈 몫으로 해서 0초 10초 ... 50초가 될 때로 나눈다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe6016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btc['type_websocket'].iloc[0]\n",
    "# df_btc['trade_price'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad41e767",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 10초 동안 다음의 변수값을 도출 할 예정\n",
    "\n",
    "datetime_10s=0 \n",
    "beginning_orderbook_midprice = 0 ## ask price 0과 bid price 0의 평균 \n",
    "beginning_price = 0 ## 10초 시작하고 시가\n",
    "highest_price = 0 ## 고가\n",
    "lowest_price = 0 ## 저가\n",
    "ending_price = 0 ## 10초 끝나기 전 시가\n",
    "ending_orderbook_midprice = 0\n",
    "\n",
    "trading_volume = 0\n",
    "volume_power = 0  ## 체결강도 (BID인 volume / ASK인 volume) ## bid(매수)가 더 많은지 ask(매도)가 더 많은지 비율로 \n",
    "\n",
    "trade_volume_bid = 0 ## voluem power 계산용\n",
    "trade_volume_ask = 0 ## voluem power 계산용\n",
    "\n",
    "ending_orderbook_total_ask_size = 0\n",
    "ending_orderbook_total_bid_size = 0\n",
    "ending_orderbook_ap_0 = 0\n",
    "ending_orderbook_as_0 = 0\n",
    "ending_orderbook_ap_1 = 0\n",
    "ending_orderbook_as_1 = 0\n",
    "ending_orderbook_ap_2 = 0\n",
    "ending_orderbook_as_2 = 0\n",
    "ending_orderbook_ap_3 = 0\n",
    "ending_orderbook_as_3 = 0\n",
    "ending_orderbook_ap_4 = 0\n",
    "ending_orderbook_as_4 = 0\n",
    "ending_orderbook_ap_5 = 0\n",
    "ending_orderbook_as_5 = 0\n",
    "ending_orderbook_ap_6 = 0\n",
    "ending_orderbook_as_6 = 0\n",
    "ending_orderbook_ap_7 = 0\n",
    "ending_orderbook_as_7 = 0\n",
    "ending_orderbook_ap_8 = 0\n",
    "ending_orderbook_as_8 = 0\n",
    "ending_orderbook_ap_9 = 0\n",
    "ending_orderbook_as_9 = 0\n",
    "ending_orderbook_ap_10 = 0\n",
    "ending_orderbook_as_10 = 0\n",
    "ending_orderbook_ap_11 = 0\n",
    "ending_orderbook_as_11 = 0\n",
    "ending_orderbook_ap_12 = 0\n",
    "ending_orderbook_as_12 = 0\n",
    "ending_orderbook_ap_13 = 0\n",
    "ending_orderbook_as_13 = 0\n",
    "ending_orderbook_ap_14 = 0\n",
    "ending_orderbook_as_14 = 0\n",
    "\n",
    "ending_orderbook_bp_0 = 0\n",
    "ending_orderbook_bs_0 = 0\n",
    "ending_orderbook_bp_1 = 0\n",
    "ending_orderbook_bs_1 = 0\n",
    "ending_orderbook_bp_2 = 0\n",
    "ending_orderbook_bs_2 = 0\n",
    "ending_orderbook_bp_3 = 0\n",
    "ending_orderbook_bs_3 = 0\n",
    "ending_orderbook_bp_4 = 0\n",
    "ending_orderbook_bs_4 = 0\n",
    "ending_orderbook_bp_5 = 0\n",
    "ending_orderbook_bs_5 = 0\n",
    "ending_orderbook_bp_6 = 0\n",
    "ending_orderbook_bs_6 = 0\n",
    "ending_orderbook_bp_7 = 0\n",
    "ending_orderbook_bs_7 = 0\n",
    "ending_orderbook_bp_8 = 0\n",
    "ending_orderbook_bs_8 = 0\n",
    "ending_orderbook_bp_9 = 0\n",
    "ending_orderbook_bs_9 = 0\n",
    "ending_orderbook_bp_10 = 0\n",
    "ending_orderbook_bs_10 = 0\n",
    "ending_orderbook_bp_11 = 0\n",
    "ending_orderbook_bs_11 = 0\n",
    "ending_orderbook_bp_12 = 0\n",
    "ending_orderbook_bs_12 = 0\n",
    "ending_orderbook_bp_13 = 0\n",
    "ending_orderbook_bs_13 = 0\n",
    "ending_orderbook_bp_14 = 0\n",
    "ending_orderbook_bs_14 = 0\n",
    "\n",
    "\n",
    "## 데이터 첫 row의 timestamp 측정:\n",
    "initial_timestamp = '' + str(df_btc['datetime'].iloc[0].year) + str(df_btc['datetime'].iloc[0].month) + str(df_btc['datetime'].iloc[0].day) + str(df_btc['datetime'].iloc[0].hour) + str(df_btc['datetime'].iloc[0].minute) + str(df_btc['datetime'].iloc[0].second//10)\n",
    "last_timestamp = '' + str(df_btc['datetime'].iloc[df_btc.shape[0]-1].year) + str(df_btc['datetime'].iloc[df_btc.shape[0]-1].month) + str(df_btc['datetime'].iloc[df_btc.shape[0]-1].day) + str(df_btc['datetime'].iloc[df_btc.shape[0]-1].hour) + str(df_btc['datetime'].iloc[df_btc.shape[0]-1].minute) + str(df_btc['datetime'].iloc[df_btc.shape[0]-1].second//10)\n",
    "\n",
    "for i in range(20000): ## range(df_btc.shape[0]): \n",
    "\n",
    "    ## 처음구간은 10초를 채우기 어렵기 때문에 자른다. 마지막 10초도 마찬가지로 자른다    \n",
    "    current_timestamp = '' + str(df_btc['datetime'].iloc[i].year) + str(df_btc['datetime'].iloc[i].month) + str(df_btc['datetime'].iloc[i].day) + str(df_btc['datetime'].iloc[i].hour) + str(df_btc['datetime'].iloc[i].minute) + str(df_btc['datetime'].iloc[i].second//10)\n",
    "    \n",
    "    if  current_timestamp == initial_timestamp or current_timestamp == last_timestamp : ## 초가 같은 부분에 자른다. \n",
    "        print(\"--처음구간--\")\n",
    "        continue ## 따라서 초의 2번째 자리(49초 -> 50초)가 달라지면 else로 넘어간다. \n",
    "    else:\n",
    "        \n",
    "        previous_timestamp = '' + str(df_btc['datetime'].iloc[i-1].year) + str(df_btc['datetime'].iloc[i-1].month) + str(df_btc['datetime'].iloc[i-1].day) + str(df_btc['datetime'].iloc[i-1].hour) + str(df_btc['datetime'].iloc[i-1].minute) + str(df_btc['datetime'].iloc[i-1].second//10)\n",
    "        if previous_timestamp != current_timestamp: ## 10초의 첫번째 줄이라면? 도출해야할 값들 초기화.\n",
    "            # previous_timestamp와 current_timestamp와 다를 경우는 ex) previous가 49초이고 current가 50초일 때\n",
    "            # 이 때만 모든 값을 초기화 한다. \n",
    "            \n",
    "            datetime_10s = df_btc['datetime'].iloc[i]\n",
    "            beginning_orderbook_midprice = 0\n",
    "            beginning_price = 0\n",
    "            highest_price = 0\n",
    "            lowest_price = 0\n",
    "            ending_price = 0\n",
    "            ending_orderbook_midprice = 0\n",
    "            trading_volume = 0\n",
    "            volume_power = 0  \n",
    "            trade_volume_bid = 0 \n",
    "            trade_volume_ask = 0 \n",
    "            ending_orderbook_total_ask_size = 0\n",
    "            ending_orderbook_total_bid_size = 0\n",
    "            ending_orderbook_ap_0 = 0\n",
    "            ending_orderbook_as_0 = 0\n",
    "            ending_orderbook_ap_1 = 0\n",
    "            ending_orderbook_as_1 = 0\n",
    "            ending_orderbook_ap_2 = 0\n",
    "            ending_orderbook_as_2 = 0\n",
    "            ending_orderbook_ap_3 = 0\n",
    "            ending_orderbook_as_3 = 0\n",
    "            ending_orderbook_ap_4 = 0\n",
    "            ending_orderbook_as_4 = 0\n",
    "            ending_orderbook_ap_5 = 0\n",
    "            ending_orderbook_as_5 = 0\n",
    "            ending_orderbook_ap_6 = 0\n",
    "            ending_orderbook_as_6 = 0\n",
    "            ending_orderbook_ap_7 = 0\n",
    "            ending_orderbook_as_7 = 0\n",
    "            ending_orderbook_ap_8 = 0\n",
    "            ending_orderbook_as_8 = 0\n",
    "            ending_orderbook_ap_9 = 0\n",
    "            ending_orderbook_as_9 = 0\n",
    "            ending_orderbook_ap_10 = 0\n",
    "            ending_orderbook_as_10 = 0\n",
    "            ending_orderbook_ap_11 = 0\n",
    "            ending_orderbook_as_11 = 0\n",
    "            ending_orderbook_ap_12 = 0\n",
    "            ending_orderbook_as_12 = 0\n",
    "            ending_orderbook_ap_13 = 0\n",
    "            ending_orderbook_as_13 = 0\n",
    "            ending_orderbook_ap_14 = 0\n",
    "            ending_orderbook_as_14 = 0\n",
    "            ending_orderbook_bp_0 = 0\n",
    "            ending_orderbook_bs_0 = 0\n",
    "            ending_orderbook_bp_1 = 0\n",
    "            ending_orderbook_bs_1 = 0\n",
    "            ending_orderbook_bp_2 = 0\n",
    "            ending_orderbook_bs_2 = 0\n",
    "            ending_orderbook_bp_3 = 0\n",
    "            ending_orderbook_bs_3 = 0\n",
    "            ending_orderbook_bp_4 = 0\n",
    "            ending_orderbook_bs_4 = 0\n",
    "            ending_orderbook_bp_5 = 0\n",
    "            ending_orderbook_bs_5 = 0\n",
    "            ending_orderbook_bp_6 = 0\n",
    "            ending_orderbook_bs_6 = 0\n",
    "            ending_orderbook_bp_7 = 0\n",
    "            ending_orderbook_bs_7 = 0\n",
    "            ending_orderbook_bp_8 = 0\n",
    "            ending_orderbook_bs_8 = 0\n",
    "            ending_orderbook_bp_9 = 0\n",
    "            ending_orderbook_bs_9 = 0\n",
    "            ending_orderbook_bp_10 = 0\n",
    "            ending_orderbook_bs_10 = 0\n",
    "            ending_orderbook_bp_11 = 0\n",
    "            ending_orderbook_bs_11 = 0\n",
    "            ending_orderbook_bp_12 = 0\n",
    "            ending_orderbook_bs_12 = 0\n",
    "            ending_orderbook_bp_13 = 0\n",
    "            ending_orderbook_bs_13 = 0\n",
    "            ending_orderbook_bp_14 = 0\n",
    "            ending_orderbook_bs_14 = 0\n",
    "\n",
    "\n",
    "        ## orderbook 타입\n",
    "        if df_btc['type_websocket'].iloc[i]  == 'orderbook':\n",
    "            if beginning_orderbook_midprice == 0: ## 아직 업데이트가 안 되었다면? (만약을 대비)\n",
    "                beginning_orderbook_midprice = (df_btc['orderbook_ap_0'].iloc[i] + df_btc['orderbook_bp_0'].iloc[i])/2\n",
    "                            ## ask price 0과 bid price 0의 평균 \n",
    "        \n",
    "        ## trade/ticker 타입 (거래발생)\n",
    "        elif df_btc['type_websocket'].iloc[i]  == 'trade' or df_btc['type_websocket'].iloc[i]  == 'ticker':\n",
    "            if beginning_price==0: ## 아직 업데이트가 안 되었다면?\n",
    "                beginning_price = df_btc['trade_price'].iloc[i]\n",
    "\n",
    "            if highest_price <  df_btc['trade_price'].iloc[i]: ## 고가 수정\n",
    "                highest_price = df_btc['trade_price'].iloc[i]\n",
    "            if lowest_price == 0 or lowest_price > df_btc['trade_price'].iloc[i]: ## 저가 수정\n",
    "                lowest_price = df_btc['trade_price'].iloc[i]\n",
    "\n",
    "            trading_volume += df_btc['trade_volume'].iloc[i] ## 거래량 더해주기\n",
    "            \n",
    "            # ask_bid => 매수/매도 구분\n",
    "            if df_btc['ask_bid'].iloc[i] =='BID': ## 체결강도 계산용 BID/ASK 거래량 더해주기\n",
    "                trade_volume_bid += df_btc['trade_volume'].iloc[i]\n",
    "            elif df_btc['ask_bid'].iloc[i] =='ASK':\n",
    "                trade_volume_ask += df_btc['trade_volume'].iloc[i]\n",
    "\n",
    "        ## 10초의 마지막 줄임을 발견\n",
    "        \n",
    "        next_timestamp = '' + str(df_btc['datetime'].iloc[i+1].year) + str(df_btc['datetime'].iloc[i+1].month) + str(df_btc['datetime'].iloc[i+1].day) + str(df_btc['datetime'].iloc[i+1].hour) + str(df_btc['datetime'].iloc[i+1].minute) + str(df_btc['datetime'].iloc[i+1].second//10)\n",
    "        if current_timestamp != next_timestamp: ## if current_timestamp이 49초이고 next가 50초 이면 print\n",
    "            \n",
    "            print(\"--마지막줄처리--\")\n",
    "            print(\"현재 index: {}\".format(i)) ## 현재 index\n",
    "            print(current_timestamp)\n",
    "            print(next_timestamp)\n",
    "\n",
    "            j = 0\n",
    "            while ending_price == 0 or ending_orderbook_midprice==0: ## 둘 중에 하나라도 0이면 계속 while\n",
    "                print(\"i-j:\"+ str(i-j))\n",
    "                \n",
    "                \n",
    "                ## 여기서 i = 193이라고 치면 j가 1씩 추가될 때 마다 i가 줄어드니까 하나 씩 전으로 돌아간다. \n",
    "                \n",
    "                if (df_btc['type_websocket'].iloc[i-j]  == 'trade' or df_btc['type_websocket'].iloc[i-j]  == 'ticker'):\n",
    "                    ending_price = df_btc['trade_price'].iloc[i-j]\n",
    "                # i = 193 j= 0 인덱스:193이 trade or ticker면 그 값을 ending price로\n",
    "                \n",
    "                elif df_btc['type_websocket'].iloc[i-j]  == 'orderbook':\n",
    "                    ending_orderbook_midprice = (df_btc['orderbook_ap_0'].iloc[i-j] +  df_btc['orderbook_bp_0'].iloc[i-j])/2\n",
    "                    ending_orderbook_total_ask_size = df_btc['total_ask_size'].iloc[i-j]\n",
    "                    ending_orderbook_total_bid_size = df_btc['total_bid_size'].iloc[i-j]\n",
    "                    ending_orderbook_ap_0 = df_btc['orderbook_ap_0'].iloc[i-j]/ending_orderbook_midprice  ## 나누는 이유: orderbook 평균가 기준 얼마나 높은지 비율로 따지기 위해 \n",
    "                    ending_orderbook_as_0 = df_btc['orderbook_as_0'].iloc[i-j]\n",
    "                    ending_orderbook_ap_1 = df_btc['orderbook_ap_1'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_as_1 = df_btc['orderbook_as_1'].iloc[i-j]\n",
    "                    ending_orderbook_ap_2 = df_btc['orderbook_ap_2'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_as_2 = df_btc['orderbook_as_2'].iloc[i-j]\n",
    "                    ending_orderbook_ap_3 = df_btc['orderbook_ap_3'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_as_3 = df_btc['orderbook_as_3'].iloc[i-j]\n",
    "                    ending_orderbook_ap_4 = df_btc['orderbook_ap_4'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_as_4 = df_btc['orderbook_as_4'].iloc[i-j]\n",
    "                    ending_orderbook_ap_5 = df_btc['orderbook_ap_5'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_as_5 = df_btc['orderbook_as_5'].iloc[i-j]\n",
    "                    ending_orderbook_ap_6 = df_btc['orderbook_ap_6'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_as_6 = df_btc['orderbook_as_6'].iloc[i-j]\n",
    "                    ending_orderbook_ap_7 = df_btc['orderbook_ap_7'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_as_7 = df_btc['orderbook_as_7'].iloc[i-j]\n",
    "                    ending_orderbook_ap_8 = df_btc['orderbook_ap_8'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_as_8 = df_btc['orderbook_as_8'].iloc[i-j]\n",
    "                    ending_orderbook_ap_9 = df_btc['orderbook_ap_9'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_as_9 = df_btc['orderbook_as_9'].iloc[i-j]\n",
    "                    ending_orderbook_ap_10 = df_btc['orderbook_ap_10'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_as_10 = df_btc['orderbook_as_10'].iloc[i-j]\n",
    "                    ending_orderbook_ap_11 = df_btc['orderbook_ap_11'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_as_11 = df_btc['orderbook_as_11'].iloc[i-j]\n",
    "                    ending_orderbook_ap_12 = df_btc['orderbook_ap_12'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_as_12 = df_btc['orderbook_as_12'].iloc[i-j]\n",
    "                    ending_orderbook_ap_13 = df_btc['orderbook_ap_13'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_as_13 = df_btc['orderbook_as_13'].iloc[i-j]\n",
    "                    ending_orderbook_ap_14 = df_btc['orderbook_ap_14'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_as_14 = df_btc['orderbook_as_14'].iloc[i-j]\n",
    "                    ending_orderbook_bp_0 = df_btc['orderbook_bp_0'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_bs_0 = df_btc['orderbook_bs_0'].iloc[i-j]\n",
    "                    ending_orderbook_bp_1 = df_btc['orderbook_bp_1'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_bs_1 = df_btc['orderbook_bs_1'].iloc[i-j]\n",
    "                    ending_orderbook_bp_2 = df_btc['orderbook_bp_2'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_bs_2 = df_btc['orderbook_bs_2'].iloc[i-j]\n",
    "                    ending_orderbook_bp_3 = df_btc['orderbook_bp_3'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_bs_3 = df_btc['orderbook_bs_3'].iloc[i-j]\n",
    "                    ending_orderbook_bp_4 = df_btc['orderbook_bp_4'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_bs_4 = df_btc['orderbook_bs_4'].iloc[i-j]\n",
    "                    ending_orderbook_bp_5 = df_btc['orderbook_bp_5'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_bs_5 = df_btc['orderbook_bs_5'].iloc[i-j]\n",
    "                    ending_orderbook_bp_6 = df_btc['orderbook_bp_6'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_bs_6 = df_btc['orderbook_bs_6'].iloc[i-j]\n",
    "                    ending_orderbook_bp_7 = df_btc['orderbook_bp_7'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_bs_7 = df_btc['orderbook_bs_7'].iloc[i-j]\n",
    "                    ending_orderbook_bp_8 = df_btc['orderbook_bp_8'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_bs_8 = df_btc['orderbook_bs_8'].iloc[i-j]\n",
    "                    ending_orderbook_bp_9 = df_btc['orderbook_bp_9'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_bs_9 = df_btc['orderbook_bs_9'].iloc[i-j]\n",
    "                    ending_orderbook_bp_10 = df_btc['orderbook_bp_10'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_bs_10 = df_btc['orderbook_bs_10'].iloc[i-j]\n",
    "                    ending_orderbook_bp_11 = df_btc['orderbook_bp_11'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_bs_11 = df_btc['orderbook_bs_11'].iloc[i-j]\n",
    "                    ending_orderbook_bp_12 = df_btc['orderbook_bp_12'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_bs_12 = df_btc['orderbook_bs_12'].iloc[i-j]\n",
    "                    ending_orderbook_bp_13 = df_btc['orderbook_bp_13'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_bs_13 = df_btc['orderbook_bs_13'].iloc[i-j]\n",
    "                    ending_orderbook_bp_14 = df_btc['orderbook_bp_14'].iloc[i-j]/ending_orderbook_midprice\n",
    "                    ending_orderbook_bs_14 = df_btc['orderbook_bs_14'].iloc[i-j]                 \n",
    "                       \n",
    "                j += 1\n",
    "                            \n",
    "            volume_power = trade_volume_bid/trade_volume_ask\n",
    "            \n",
    "            data_to_insert = {'datetime_10s':datetime_10s, 'beginning_orderbook_midprice':beginning_orderbook_midprice,\n",
    "                             'beginning_price':beginning_price, 'highest_price':highest_price,\n",
    "                             'highest_price':highest_price, 'lowest_price':lowest_price,\n",
    "                             'ending_price':ending_price, 'ending_orderbook_midprice':ending_orderbook_midprice,\n",
    "                             'trading_volume':trading_volume, 'volume_power':volume_power,\n",
    "                              'trade_volume_bid':trade_volume_bid, 'trade_volume_ask':trade_volume_ask,\n",
    "                             'ending_orderbook_total_ask_size':ending_orderbook_total_ask_size, 'ending_orderbook_total_bid_size':ending_orderbook_total_bid_size,\n",
    "                             'ending_orderbook_ap_0':ending_orderbook_ap_0, 'ending_orderbook_as_0':ending_orderbook_as_0,\n",
    "                             'ending_orderbook_ap_1':ending_orderbook_ap_1, 'ending_orderbook_as_1':ending_orderbook_as_1,\n",
    "                             'ending_orderbook_ap_2':ending_orderbook_ap_2, 'ending_orderbook_as_2':ending_orderbook_as_2,\n",
    "                             'ending_orderbook_ap_3':ending_orderbook_ap_3, 'ending_orderbook_as_3':ending_orderbook_as_3,\n",
    "                              'ending_orderbook_ap_4':ending_orderbook_ap_4, 'ending_orderbook_as_4':ending_orderbook_as_4,\n",
    "                             'ending_orderbook_ap_5':ending_orderbook_ap_5, 'ending_orderbook_as_5':ending_orderbook_as_5,\n",
    "                              'ending_orderbook_ap_6':ending_orderbook_ap_6, 'ending_orderbook_as_6':ending_orderbook_as_6,\n",
    "                             'ending_orderbook_ap_7':ending_orderbook_ap_7, 'ending_orderbook_as_7':ending_orderbook_as_7,\n",
    "                              'ending_orderbook_ap_8':ending_orderbook_ap_8, 'ending_orderbook_as_8':ending_orderbook_as_8,\n",
    "                             'ending_orderbook_ap_9':ending_orderbook_ap_9, 'ending_orderbook_as_9':ending_orderbook_as_9,\n",
    "                              'ending_orderbook_ap_10':ending_orderbook_ap_10, 'ending_orderbook_as_10':ending_orderbook_as_10,\n",
    "                             'ending_orderbook_ap_11':ending_orderbook_ap_11, 'ending_orderbook_as_11':ending_orderbook_as_11,\n",
    "                              'ending_orderbook_ap_12':ending_orderbook_ap_12, 'ending_orderbook_as_12':ending_orderbook_as_12,\n",
    "                             'ending_orderbook_ap_13':ending_orderbook_ap_13, 'ending_orderbook_as_13':ending_orderbook_as_13,\n",
    "                              'ending_orderbook_ap_14':ending_orderbook_ap_14, 'ending_orderbook_as_14':ending_orderbook_as_14,\n",
    "                             'ending_orderbook_bp_0':ending_orderbook_bp_0, 'ending_orderbook_bs_0':ending_orderbook_bs_0,\n",
    "                             'ending_orderbook_bp_1':ending_orderbook_bp_1, 'ending_orderbook_bs_1':ending_orderbook_bs_1,\n",
    "                             'ending_orderbook_bp_2':ending_orderbook_bp_2, 'ending_orderbook_bs_2':ending_orderbook_bs_2,\n",
    "                             'ending_orderbook_bp_3':ending_orderbook_bp_3, 'ending_orderbook_bs_3':ending_orderbook_bs_3,\n",
    "                             'ending_orderbook_bp_4':ending_orderbook_bp_4, 'ending_orderbook_bs_4':ending_orderbook_bs_4,\n",
    "                             'ending_orderbook_bp_5':ending_orderbook_bp_5, 'ending_orderbook_bs_5':ending_orderbook_bs_5,\n",
    "                             'ending_orderbook_bp_6':ending_orderbook_bp_6, 'ending_orderbook_bs_6':ending_orderbook_bs_6,\n",
    "                             'ending_orderbook_bp_7':ending_orderbook_bp_7, 'ending_orderbook_bs_7':ending_orderbook_bs_7,\n",
    "                             'ending_orderbook_bp_8':ending_orderbook_bp_8, 'ending_orderbook_bs_8':ending_orderbook_bs_8,\n",
    "                             'ending_orderbook_bp_9':ending_orderbook_bp_9, 'ending_orderbook_bs_9':ending_orderbook_bs_9,\n",
    "                             'ending_orderbook_bp_10':ending_orderbook_bp_10, 'ending_orderbook_bs_10':ending_orderbook_bs_10,\n",
    "                             'ending_orderbook_bp_11':ending_orderbook_bp_11, 'ending_orderbook_bs_11':ending_orderbook_bs_11,\n",
    "                             'ending_orderbook_bp_12':ending_orderbook_bp_12, 'ending_orderbook_bs_12':ending_orderbook_bs_12,\n",
    "                             'ending_orderbook_bp_13':ending_orderbook_bp_13, 'ending_orderbook_bs_13':ending_orderbook_bs_13,\n",
    "                             'ending_orderbook_bp_14':ending_orderbook_bp_14, 'ending_orderbook_bs_14':ending_orderbook_bs_14}\n",
    "             \n",
    "            my_data = my_data.append(data_to_insert, ignore_index=True)\n",
    "                 \n",
    "pd.set_option('display.max_columns', None)\n",
    "my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf3df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb80671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7032f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x_frames, y_frames):    # x_frames, y_frames는 x,y의 sequence length를 얼마 줄건지 x_frames에는 최근 n일의 데이터를 주고, y_frames는 이후 n일의 데이터를 넣어준다\n",
    "        ## 동시에 t의 다음날 그그 다음날 등등 t+1 부터 t+5까지 동시에 예측하도록 모델 학습\n",
    "        # t+1 만 예측하게 되면, 전날 가격과 똑같이 예측을 하게 된다는 경향\n",
    "        # model이 편법을 배운다. \n",
    "\n",
    "        self.x_frames = x_frames  ## 이것은 look-back 즉 sequence의 길이, input_dim과는 아무 관계 없다. input_dim은 여기서 open, low, high 등 6\n",
    "        self.y_frames = y_frames\n",
    "        \n",
    "\n",
    "        # self.data = pdr.DataReader(self.symbol, 'yahoo', self.start, self.end)\n",
    "        # self.data = data\n",
    "        # print(self.data.isna().sum())\n",
    "        \n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):     ## 데이터 셋의 길이\n",
    "        return len(self.data) - (self.x_frames + self.y_frames) + 1\n",
    "        ## 만약 1,2,3,4,5 일치를 갖고 있을 때 x에 2일 y에 그 후의 2일을 준다고 하면 \n",
    "        # 총 가능 frame은 1,2 - 3,4\n",
    "                        # 2,3 - 4,5   2개 이다.\n",
    "    \n",
    "    def __getitem__(self, idx):  ## i번째 요청이 왔을 때, i번째를 건네주는 역할\n",
    "        idx += self.x_frames  # idx = 0 이고 x_frames가 10이면 idx = 10\n",
    "        data = self.data.iloc[idx-self.x_frames:idx+self.y_frames] ## idx 기준으로 이전 x개, 이후 y개를 select 하겠다.  ex) x_frames =10, y_frames 5 라면 [10 - 10: 10 + 5] = [0:15]\n",
    "        data = data[['datetime_10s', 'beginning_orderbook_midprice','beginning_price', 'highest_price',\n",
    "                               'lowest_price', 'ending_price','ending_orderbook_midprice',\n",
    "                               'trading_volume', 'volume_power','trade_volume_bid', 'trade_volume_ask',\n",
    "                               'ending_orderbook_total_ask_size', 'ending_orderbook_total_bid_size', \n",
    "                                'ending_orderbook_ap_0','ending_orderbook_as_0',\n",
    "                                'ending_orderbook_ap_1','ending_orderbook_as_1',\n",
    "                               'ending_orderbook_ap_2','ending_orderbook_as_2',\n",
    "                               'ending_orderbook_ap_3','ending_orderbook_as_3',\n",
    "                               'ending_orderbook_ap_4','ending_orderbook_as_4',\n",
    "                               'ending_orderbook_ap_5','ending_orderbook_as_5',\n",
    "                               'ending_orderbook_ap_6','ending_orderbook_as_6',\n",
    "                               'ending_orderbook_ap_7','ending_orderbook_as_7',\n",
    "                               'ending_orderbook_ap_8','ending_orderbook_as_8',\n",
    "                               'ending_orderbook_ap_9','ending_orderbook_as_9',\n",
    "                               'ending_orderbook_ap_10','ending_orderbook_as_10',\n",
    "                               'ending_orderbook_ap_11','ending_orderbook_as_11',\n",
    "                               'ending_orderbook_ap_12','ending_orderbook_as_12',\n",
    "                               'ending_orderbook_ap_13','ending_orderbook_as_13',\n",
    "                               'ending_orderbook_ap_14','ending_orderbook_as_14',\n",
    "                                'ending_orderbook_bp_0','ending_orderbook_bs_0',\n",
    "                                'ending_orderbook_bp_1','ending_orderbook_bs_1',\n",
    "                               'ending_orderbook_bp_2','ending_orderbook_bs_2',\n",
    "                               'ending_orderbook_bp_3','ending_orderbook_bs_3',\n",
    "                               'ending_orderbook_bp_4','ending_orderbook_bs_4',\n",
    "                               'ending_orderbook_bp_5','ending_orderbook_bs_5',\n",
    "                               'ending_orderbook_bp_6','ending_orderbook_bs_6',\n",
    "                               'ending_orderbook_bp_7','ending_orderbook_bs_7',\n",
    "                               'ending_orderbook_bp_8','ending_orderbook_bs_8',\n",
    "                               'ending_orderbook_bp_9','ending_orderbook_bs_9',\n",
    "                               'ending_orderbook_bp_10','ending_orderbook_bs_10',\n",
    "                               'ending_orderbook_bp_11','ending_orderbook_bs_11',\n",
    "                               'ending_orderbook_bp_12','ending_orderbook_bs_12',\n",
    "                               'ending_orderbook_bp_13','ending_orderbook_bs_13',\n",
    "                               'ending_orderbook_bp_14','ending_orderbook_bs_14']]\n",
    "        # data = data.apply(lambda x: np.log(x+1) - np.log(x[self.x_frames-1]+1))  ## log 수익률로 표현하기 위함. +1을 해서 log 0을 피하기 위함\n",
    "        data = data.values ## data frame을 ndarray로 바꿔준다.\n",
    "        X = data[:self.x_frames]  ## x_frame(3일) + y_frame(2일) 합쳐서 5개(5일)가 주어졌다면 X에 3개 y에 2개\n",
    "        y = data[self.x_frames:]\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "StockDataset(200, 20).__getitem__(0)[0]\n",
    "\n",
    "# StockDataset(200, 20).__getitem__(0)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be6a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = StockDataset(200, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
