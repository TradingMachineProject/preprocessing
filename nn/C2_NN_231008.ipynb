{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBRARY IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "from contextlib import contextmanager\n",
    "import time  \n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "from sklearn.preprocessing import minmax_scale  \n",
    "from typing import Dict, List, Optional, Tuple  \n",
    "import seaborn as sns \n",
    "import gc\n",
    "import traceback \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "import os\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "working_directory = 'D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'  ## 서로 다른 환경에서는 이곳을 수정해야 함.\n",
    "# working_directory = 'C:\\\\Users\\\\user\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'\n",
    "\n",
    "os.chdir(working_directory)\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Early morning period & CHECK NULL VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_result_df = pd.read_csv(\"./DB/professor_BTC_sum_both_10m.csv\")\n",
    "combined_result_df_raw = pd.read_csv(\"./output/BTC_sum_both_10m.csv\")\n",
    "print(\"# of rows of combined_Result_Df:\", combined_result_df_raw.shape[0])\n",
    "\n",
    "combined_result_df_raw['window_start'] = pd.to_datetime(combined_result_df_raw['window_start'])  # Convert to datetime\n",
    "\n",
    "# Define the time range\n",
    "start_time = pd.to_datetime('00:00:00').time()\n",
    "end_time = pd.to_datetime('06:00:00').time()\n",
    "\n",
    "# Filter and drop rows (새벽시간 삭제하기)\n",
    "filtered_df = combined_result_df_raw[~combined_result_df_raw['window_start'].apply(lambda x: start_time <= x.time() <= end_time)]\n",
    "print(\"# of rows of filtered_df:\", filtered_df.shape[0])\n",
    "\n",
    "# Set the target variable:\n",
    "target_var = 'dv5_realized_volatility_mean0'\n",
    "target_var_3 = target_var[:3]\n",
    "print(\"target_var_3:\", target_var_3)\n",
    "\n",
    "# 사용하지 않을 변수(column)들 미리 제거 (dropna 효과를 최소화하기 위해.)\n",
    "remove_varlist0 = [\n",
    "                  'window_start', \n",
    "                  'window_end',\n",
    "                  'prices_30s_for_NN',\n",
    "                  'window_end_150_ticker',\n",
    "                  'window_end_300_ticker',\n",
    "                  'window_end_450_ticker',\n",
    "                  'window_end_150_orderbook',\n",
    "                  'window_end_300_orderbook',\n",
    "                  'window_end_450_orderbook',\n",
    "                  'volume_power',\n",
    "                  'volume_power_150',\n",
    "                  'volume_power_300',\n",
    "                  'volume_power_450',\n",
    "                  'end_price',\n",
    "\n",
    "                #   'dv1_realized_volatility',\n",
    "                  'dv2_lowest_return',\n",
    "                  'dv3_highest_return',\n",
    "                  'dv4_realized_volatility_30s',\n",
    "                  'dv5_realized_volatility_mean0',\n",
    "\n",
    "                  'prices_30s_for_NN_onlyprices',\n",
    "                  ]\n",
    "remove_varlist0.remove(target_var)\n",
    "\n",
    "main_feature_list = list(filtered_df.columns)\n",
    "\n",
    "for i in range(len(remove_varlist0)):\n",
    "    # print(remove_varlist0[i])\n",
    "    main_feature_list.remove(remove_varlist0[i])\n",
    "\n",
    "filtered_df = filtered_df[main_feature_list]\n",
    "\n",
    "combined_result_df = filtered_df.dropna()\n",
    "print(\"# of rows of filtered_df:\", combined_result_df.shape[0])\n",
    "\n",
    "# 평균이 아닌 이전 값으로 Null 채우기\n",
    "# combined_result_df = combined_result_df.fillna(method='ffill') \n",
    "\n",
    "combined_result_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating Features => Log transformation of some liquidity measures (Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_result_df['liq_last_1'] = np.log10(combined_result_df['liq_last_1'] + 0.00001)\n",
    "combined_result_df['liq_last_2'] = np.log10(combined_result_df['liq_last_2'] + 0.00001)\n",
    "combined_result_df['liq_last_5'] = np.log10(combined_result_df['liq_last_5'] + 0.00001)\n",
    "combined_result_df['liq_last_10'] = np.log10(combined_result_df['liq_last_10'] + 0.00001)\n",
    "combined_result_df['liq_last_15'] = np.log10(combined_result_df['liq_last_15'] + 0.00001)\n",
    "combined_result_df['trade_vol'] = np.log10(combined_result_df['trade_vol'] + 0.00001)\n",
    "combined_result_df['num_trades'] = np.log10(combined_result_df['num_trades'] + 0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUMERIC FEATURES & CALCULATE CORR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_feature_list = list(combined_result_df.columns)\n",
    "feature_list_for_corr = list(combined_result_df.columns)\n",
    "\n",
    "remove_varlist = [\n",
    "                  'time_id'\n",
    "                  ]\n",
    "\n",
    "for i in range(len(remove_varlist)):\n",
    "    # print(remove_varlist[i])\n",
    "    feature_list_for_corr.remove(remove_varlist[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combined_result_df\n",
    "correlation_matrix = data[feature_list_for_corr].corr(method=lambda x, y: kendalltau(x, y).correlation)\n",
    "correlation_matrix_series = correlation_matrix[target_var].copy() # .sort_values(ascending=False)\n",
    "correlation_matrix_series.sort_values(ascending=False, inplace=True)\n",
    "correlation_matrix_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_result_df_mfl = combined_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_for_finding_NN = []\n",
    "feature_list_for_finding_NN.append('realized_volatility_mean0')\n",
    "feature_list_for_finding_NN.append('bidask_spread_0')\n",
    "feature_list_for_finding_NN.append('bidask_spread_1')\n",
    "feature_list_for_finding_NN.append('highest_return')\n",
    "feature_list_for_finding_NN.append('lowest_return')\n",
    "feature_list_for_finding_NN.append('num_trades')\n",
    "feature_list_for_finding_NN.append('high_low_gap')\n",
    "feature_list_for_finding_NN.append('BB_width_w10')\n",
    "feature_list_for_finding_NN.append('BB_width_w20')\n",
    "feature_list_for_finding_NN.append('liq_last_1')\n",
    "feature_list_for_finding_NN.append('liq_last_5')\n",
    "feature_list_for_finding_NN.append('ep_liq_1')\n",
    "feature_list_for_finding_NN.append('ep_liq_5')\n",
    "feature_list_for_finding_NN.append('tvpl1')\n",
    "feature_list_for_finding_NN.append('tvpl5')\n",
    "feature_list_for_finding_NN.append('tvpl_ep1')\n",
    "feature_list_for_finding_NN.append('tvpl_ep5')\n",
    "feature_list_for_finding_NN.append('trade_vol')\n",
    "feature_list_for_finding_NN.append('trade.tau')\n",
    "# feature_list_for_finding_NN.append('volume_power')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILD NEIGHBORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NEIGHBORS_MAX = 65 \n",
    "\n",
    "class Neighbors:\n",
    "    def __init__(self, \n",
    "                 name: str, \n",
    "                 pivot: pd.DataFrame, \n",
    "                 p: float, \n",
    "                 metric: str = 'minkowski', \n",
    "                 metric_params: object = None, \n",
    "                 exclude_self: bool = True,\n",
    "                 ):\n",
    "        self.name = name\n",
    "        self.exclude_self = exclude_self\n",
    "        self.p = p\n",
    "        self.metric = metric\n",
    "        self.neighbors = np.empty((0, 65), dtype=int)  # 빈 2D 배열로 초기화\n",
    "\n",
    "        nn = NearestNeighbors(\n",
    "            n_neighbors=N_NEIGHBORS_MAX, \n",
    "            p=p, \n",
    "            metric=metric, \n",
    "            metric_params=metric_params\n",
    "        )\n",
    "        \n",
    "        # nn.fit(pivot)\n",
    "        # _, self.neighbors = nn.kneighbors(pivot, return_distance=True)\n",
    "\n",
    "        # 이웃을 찾을 이전 window 벙뮈\n",
    "        window_size = 10000\n",
    "        col_names = pivot.columns\n",
    "        index_name = pivot.index.name\n",
    "        \n",
    "        for t in range(len(pivot)) :\n",
    "            # window size 까지의 데이터는 random NN 설정\n",
    "            # 1 ~ window 범위의 데이터는 추후 버려야 함\n",
    "            if t < window_size :\n",
    "                update_array = np.random.permutation(np.arange(1, 66))\n",
    "                self.neighbors = np.append(self.neighbors, [update_array], axis = 0)\n",
    "                \n",
    "            else :\n",
    "                pvdf = pd.DataFrame(pivot.iloc[t-window_size:t])\n",
    "                pvdf.columns = [list(col_names)]\n",
    "                pvdf = pvdf.rename_axis(index_name)\n",
    "                nn.fit(pvdf)\n",
    "                \n",
    "                update_array = nn.kneighbors(pivot.iloc[t].values.reshape(1, -1), return_distance=False)\n",
    "                update_array = update_array.reshape(-1)\n",
    "                self.neighbors = np.append(self.neighbors, [update_array], axis = 0)\n",
    "                # if t // 10000 == 0 :\n",
    "                #    print(t,self.neighbors)\n",
    "\n",
    "        self.columns = self.index = self.feature_values = self.feature_col = None\n",
    "\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def make_nn_feature(self, n=5, agg=np.mean) -> pd.DataFrame:\n",
    "        assert self.feature_values is not None, \"should call rearrange_feature_values beforehand\"\n",
    "\n",
    "        start = 1 if self.exclude_self else 0\n",
    "\n",
    "        pivot_aggs = pd.DataFrame(\n",
    "            agg(self.feature_values[start:n+start,:,0], axis=0), \n",
    "            columns=self.columns, \n",
    "            index=self.index\n",
    "        )\n",
    "\n",
    "        dst = pivot_aggs.reset_index() # unstack().\n",
    "        # print(\"dst.shape:\", dst.shape)\n",
    "        new_column_names = ['time_id', f'{self.feature_col}_nn{n}_{self.name}_{agg.__name__}'] # 3개를 예측했는데 2개만 들어왔다??\n",
    "        dst.columns = new_column_names \n",
    "        return dst\n",
    "    \n",
    "\n",
    "class TimeIdNeighbors(Neighbors):\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        # feature_pivot = df.pivot(index='time_id', values=feature_col)\n",
    "        # feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "\n",
    "        feature_df = df[['time_id', feature_col]]\n",
    "        feature_df.set_index('time_id', inplace=True)\n",
    "        feature_df = feature_df.fillna(feature_df.mean())\n",
    "\n",
    "        feature_values = np.zeros((N_NEIGHBORS_MAX, feature_df.shape[0], 1))\n",
    "\n",
    "        for i in range(N_NEIGHBORS_MAX):\n",
    "            feature_values[i, :, 0] += feature_df.values[self.neighbors[:, i], 0]\n",
    "\n",
    "        self.columns = list(feature_df.columns)\n",
    "        self.index = list(feature_df.index)\n",
    "        self.feature_values = feature_values\n",
    "        self.feature_col = feature_col\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"time-id NN (name={self.name}, metric={self.metric}, p={self.p})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROGRESS CHECK FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - s\n",
    "    print(f'[{name}] {elapsed: .3f}초')\n",
    "\n",
    "def print_trace(name: str = ''):\n",
    "    print(f'{name or \"익명\"}에서 에러가 발생했습니다.')\n",
    "    print(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET NN CLASS. Output: {}_sum_plus_nn_features.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# StandardScaler 객체 생성\n",
    "scaler = StandardScaler()\n",
    "\n",
    "USE_ONE_FEATURE_C = True\n",
    "USE_ONE_FEATURE_M_1 = True\n",
    "USE_ONE_FEATURE_M_2 = True\n",
    "\n",
    "USE_TWO_FEATURES = True\n",
    "\n",
    "USE_ALL_FEATURES = True\n",
    "USE_SEVALRAL_FEATURES = True\n",
    "\n",
    "# Top 5 Related Feature\n",
    "top_5_high_feat = list(correlation_matrix_series.keys())[1:6]\n",
    "top_5_low_feat = list(correlation_matrix_series.keys())[-5:]\n",
    "\n",
    "# Top 5 Absolute Related Feature\n",
    "sorted_data = correlation_matrix_series.abs().sort_values(ascending=False)\n",
    "top_5_high_abs_feat = list(sorted_data.head(6).keys())[1:]\n",
    "top_5_low_abs_feat = list(sorted_data.tail(5).keys())\n",
    "\n",
    "# time_id_neighbors List \n",
    "time_id_neighbors: List[Neighbors] = []\n",
    "\n",
    "with timer('knn fit'):\n",
    "    df_pv = combined_result_df_mfl.copy()\n",
    "    # df_pv = df_pv.drop(['window_start', 'window_end','volume_power'], axis=1)\n",
    "    \n",
    "    # Standard All Feature\n",
    "    # df_pv[feature_list_for_finding_NN] = scaler.fit_transform(df_pv[feature_list_for_finding_NN])\n",
    "\n",
    "    feature_list = list(df_pv.columns)\n",
    "    feature_list.remove('time_id') # Can't be standardized\n",
    "    # feature_list.remove('prices_30s_for_NN_onlyprices') # Can't be standardized\n",
    "    # feature_list.remove('time_id')\n",
    "    df_pv[feature_list] = scaler.fit_transform(df_pv[feature_list])\n",
    "    # combined_result_df_mfl_scaled.head(3)\n",
    "\n",
    "\n",
    "\n",
    "    #### USE ONLY ONE FACTOR ####\n",
    "\n",
    "    # ## Canberra Distance\n",
    "    # if USE_ONE_FEATURE_C :\n",
    "    #     for feat in feature_list_for_finding_NN :\n",
    "    #         gc.collect()\n",
    "    #         df_nn = df_pv[['time_id',feat]]\n",
    "    #         df_nn.set_index('time_id', inplace=True)\n",
    "    #         df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "    #         time_id_neighbors.append(\n",
    "    #             TimeIdNeighbors(\n",
    "    #                 feat + '_c', \n",
    "    #                 df_nn, \n",
    "    #                 p=2, \n",
    "    #                 metric='canberra', \n",
    "    #                 exclude_self=True\n",
    "    #             )\n",
    "    #         )\n",
    "\n",
    "    ## Manhattan Distance\n",
    "    \n",
    "    if USE_ONE_FEATURE_M_1:\n",
    "        for feat in feature_list_for_finding_NN :\n",
    "            gc.collect()\n",
    "            df_nn = df_pv[['time_id',feat]]\n",
    "            df_nn.set_index('time_id', inplace=True)\n",
    "            # df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "            time_id_neighbors.append(\n",
    "                TimeIdNeighbors(feat + '_m_p1', df_nn, p=1, exclude_self=True)\n",
    "            )\n",
    "\n",
    "    # ## Euclidean Distance\n",
    "    # if USE_ONE_FEATURE_M_2:\n",
    "    #     for feat in feature_list_for_finding_NN :\n",
    "    #         gc.collect()\n",
    "    #         df_nn = df_pv[['time_id',feat]]\n",
    "    #         df_nn.set_index('time_id', inplace=True)\n",
    "    #         df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "    #         time_id_neighbors.append(\n",
    "    #             TimeIdNeighbors(feat + '_m_p2', df_nn, p=2, exclude_self=True)\n",
    "    #         )\n",
    "\n",
    "\n",
    "    # #### TWO FACTOR ####\n",
    "\n",
    "    # if USE_TWO_FEATURES:\n",
    "    #     feature_list = ['time_id','realized_volatility_mean0','bidask_spread_0'] # target variable이 바뀌면 2번/3번 변수 수정해주어야.\n",
    "    #     df_nn = df_pv[feature_list]\n",
    "    #     df_nn.set_index('time_id', inplace=True)\n",
    "    #     df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "    #     ## Canberra\n",
    "\n",
    "    #     time_id_neighbors.append(\n",
    "    #             TimeIdNeighbors(\n",
    "    #                 'two_c',  # before correction: feat + 'two_c', \n",
    "    #                 df_nn, \n",
    "    #                 p=2, \n",
    "    #                 metric='canberra', \n",
    "    #                 exclude_self=True\n",
    "    #             )\n",
    "    #         )\n",
    "    #     ## Euclidean Distance\n",
    "    #     time_id_neighbors.append(\n",
    "    #         TimeIdNeighbors(\n",
    "    #             'two_m', \n",
    "    #             df_nn, \n",
    "    #             p=2, \n",
    "    #             exclude_self=True\n",
    "    #         )\n",
    "    #     )\n",
    "\n",
    "\n",
    "    #### USE SEVALRAL FACTOR ####\n",
    "\n",
    "    if USE_SEVALRAL_FEATURES:\n",
    "        ## High Related Feature \n",
    "        feature_list = ['time_id']\n",
    "        feature_list += top_5_high_feat\n",
    "        df_nn = df_pv[feature_list]\n",
    "        df_nn.set_index('time_id', inplace=True)\n",
    "        # df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "        ### Euclidean Distance\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'high5_nn_m', \n",
    "                df_nn, \n",
    "                p=1, \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ## Low Related Feature\n",
    "\n",
    "        feature_list = ['time_id']\n",
    "        feature_list += top_5_low_feat\n",
    "        df_nn = df_pv[feature_list]\n",
    "        df_nn.set_index('time_id', inplace=True)\n",
    "        # df_nn = df_nn.fillna(df_nn.mean())        \n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'low5_nn_m', \n",
    "                df_nn, \n",
    "                p=1, \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ## High Abs Related Feature\n",
    "\n",
    "        feature_list = ['time_id']\n",
    "        feature_list += top_5_high_abs_feat\n",
    "        df_nn = df_pv[feature_list]\n",
    "        df_nn.set_index('time_id', inplace=True)\n",
    "        # df_nn = df_nn.fillna(df_nn.mean())\n",
    "        \n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'high5_abs_nn_m', \n",
    "                df_nn, \n",
    "                p=1, \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ## Low Abs Related Feature\n",
    "\n",
    "        feature_list = ['time_id']\n",
    "        feature_list += top_5_low_abs_feat\n",
    "        df_nn = df_pv[feature_list]\n",
    "        df_nn.set_index('time_id', inplace=True)\n",
    "        # df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'low5_abs_nn_m', \n",
    "                df_nn, \n",
    "                p=1, \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    #### USE ALL FACTOR ####\n",
    "\n",
    "    if USE_ALL_FEATURES:\n",
    "        df_nn = df_pv.copy()\n",
    "        # df_nn = df_nn.drop(['dv1_realized_volatility'], axis=1)\n",
    "        df_nn.set_index('time_id', inplace=True)\n",
    "\n",
    "        # Standard All Feature\n",
    "        # df_nn[feature_list_for_finding_NN] = scaler.fit_transform(df_nn[feature_list_for_finding_NN])\n",
    "\n",
    "        df_nn = df_nn[feature_list_for_finding_NN]\n",
    "        # df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'all_nn_m_p1', \n",
    "                df_nn, \n",
    "                p=1, \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # time_id_neighbors.append(\n",
    "        #     TimeIdNeighbors(\n",
    "        #         'all_nn_m_p2', \n",
    "        #         df_nn, \n",
    "        #         p=2, \n",
    "        #         exclude_self=True\n",
    "        #     )\n",
    "        # )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate Features With NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nearest_neighbor_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df2 = combined_result_df_mfl.copy()\n",
    "    print(df2.shape)\n",
    "\n",
    "    ### time_id를 기준으로 얻어진 neighbor를 대상으로 feature 만들기\n",
    "    feature_cols = {\n",
    "        # 'realized_volatility': [np.mean], #, np.min, np.max, np.std\n",
    "        'realized_volatility_mean0': [np.mean, np.median],\n",
    "        'realized_volatility_30s': [np.mean, np.median],\n",
    "\n",
    "        # 'dv1_realized_volatility': [np.mean],\n",
    "        # 'dv2_lowest_return': [np.mean, np.median],\n",
    "        # 'dv3_highest_return': [np.mean, np.median],\n",
    "        # 'dv4_realized_volatility_30s': [np.mean, np.median],\n",
    "        'dv5_realized_volatility_mean0': [np.mean, np.median],\n",
    "\n",
    "        'num_trades': [np.mean, np.median],\n",
    "        'lowest_return': [np.mean, np.median], # , np.mean, np.min\n",
    "        'highest_return': [np.mean, np.median], # , np.mean, np.min\n",
    "        'high_low_gap': [np.mean, np.median],\n",
    "        'trade_vol': [np.mean, np.median],\n",
    "        # 'volume_power': [np.mean, np.median],\n",
    "        'BB_width_w10': [np.mean, np.median],\n",
    "        'BB_width_w20': [np.mean, np.median],\n",
    "\n",
    "        'liq_last_1': [np.mean, np.median],\n",
    "        'liq_last_5': [np.mean, np.median],\n",
    "        'ep_liq_1': [np.mean, np.median],\n",
    "        'ep_liq_5': [np.mean, np.median],\n",
    "        'bidask_spread_0': [np.mean, np.median],\n",
    "        'bidask_spread_1': [np.mean, np.median],\n",
    "\n",
    "        'tvpl1': [np.mean, np.median],\n",
    "        'tvpl5': [np.mean, np.median],\n",
    "        'tvpl_ep1': [np.mean, np.median],\n",
    "        'tvpl_ep5': [np.mean, np.median],\n",
    "        'trade.tau': [np.mean, np.median],\n",
    "    }\n",
    "\n",
    "    time_id_neigbor_sizes = [2, 4, 8, 16, 32, 48, 64] # 메모리 부족으로 계속 오류가 나는 것 같아 이웃 계산 숫자를 줄임.\n",
    "\n",
    "    ndf: Optional[pd.DataFrame] = None\n",
    "    \n",
    "    # 새로운 feature를 기존 df에 추가하는 함수\n",
    "    def _add_ndf(ndf: Optional[pd.DataFrame], dst: pd.DataFrame) -> pd.DataFrame:\n",
    "        if ndf is None:\n",
    "            return dst\n",
    "        else:\n",
    "            ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
    "            #columns_to_convert = [dst.columns[-1]]  # 열 변환 대상을 선택하거나 여러 열을 지정할 수 있음\n",
    "            #converted_columns = dst[columns_to_convert].astype(np.float32)\n",
    "            #ndf = pd.concat([ndf, converted_columns], axis=1)\n",
    "\n",
    "            return ndf\n",
    "\n",
    "    # neighbor time_id\n",
    "    for feature_col in feature_cols.keys():\n",
    "        gc.collect()\n",
    "        try: \n",
    "            for nn in time_id_neighbors:\n",
    "                nn.rearrange_feature_values(df2, feature_col)\n",
    "\n",
    "\n",
    "            time_id_ns = time_id_neigbor_sizes\n",
    "\n",
    "            for agg in feature_cols[feature_col]:\n",
    "                for n in time_id_ns:\n",
    "                    try:\n",
    "                        for nn in time_id_neighbors:\n",
    "                            gc.collect()\n",
    "                            dst = nn.make_nn_feature(n, agg)\n",
    "                            ndf = _add_ndf(ndf, dst)\n",
    "                    except Exception:\n",
    "                        WHERE_ERROR = feature_col\n",
    "                        print_trace('time-id nn')\n",
    "                        pass\n",
    "        except Exception:\n",
    "            print_trace('time-id nn')\n",
    "\n",
    "    if ndf is not None:\n",
    "        df2 = pd.merge(df2, ndf, on=['time_id'], how='left')\n",
    "    \n",
    "    print(df2.shape)\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "combined_result_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75169, 89)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hhkim\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75169, 7145)\n",
      "[make nearest neighbor feature]  548.499초\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "with timer('make nearest neighbor feature'):\n",
    "    df3 = make_nearest_neighbor_feature(combined_result_df_mfl)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the first 'window_size' (10,000) rows. <= These rows do not have appropriate nearest neighbors.\n",
    "df3 = df3.drop(index=range(window_size))\n",
    "\n",
    "coin = 'BTC'\n",
    "df3.to_csv(working_directory + \"output\\\\{}_sum_plus_nn_features_for_{}.csv\".format(coin, target_var_3), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP code\n",
    "\n",
    "import gc\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "working_directory = 'D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'  ## 서로 다른 환경에서는 이곳을 수정해야 함.\n",
    "# working_directory = 'C:\\\\Users\\\\user\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'\n",
    "\n",
    "os.chdir(working_directory)\n",
    "gc.collect()\n",
    "\n",
    "# combined_result_df = pd.read_csv(\"./DB/professor_BTC_sum_both_10m.csv\")\n",
    "coin = 'BTC'\n",
    "target_var = 'dv5_realized_volatility_mean0'\n",
    "target_var_3 = target_var[:3]\n",
    "df3 = pd.read_csv(\"./output/{}_sum_plus_nn_features_for_{}.csv\".format(coin, target_var_3))\n",
    "\n",
    "window_size=10000\n",
    "df3 = df3.drop(index=range(window_size))\n",
    "df3.to_csv(working_directory + \"output\\\\{}_sum_plus_nn_features_for_{}.csv\".format(coin, target_var_3), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the final outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows: 75169\n",
      "# of columns: 7145\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "working_directory = 'D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'  ## 서로 다른 환경에서는 이곳을 수정해야 함.\n",
    "# working_directory = 'C:\\\\Users\\\\user\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'\n",
    "\n",
    "os.chdir(working_directory)\n",
    "gc.collect()\n",
    "\n",
    "# combined_result_df = pd.read_csv(\"./DB/professor_BTC_sum_both_10m.csv\")\n",
    "coin = 'BTC'\n",
    "target_var = 'dv5_realized_volatility_mean0'\n",
    "target_var_3 = target_var[:3]\n",
    "df3 = pd.read_csv(\"./output/{}_sum_plus_nn_features_for_{}.csv\".format(coin, target_var_3))\n",
    "\n",
    "df3.head(3)\n",
    "print(\"# of rows:\", df3.shape[0])\n",
    "print(\"# of columns:\", df3.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['realized_volatility_mean0', 'num_trades', 'lowest_return',\n",
       "       'highest_return', 'high_low_gap', 'trade_vol', 'time_id',\n",
       "       'BB_width_w20', 'BB_width_w40', 'BB_width_w10',\n",
       "       ...\n",
       "       'trade.tau_nn64_tvpl5_m_p1_median',\n",
       "       'trade.tau_nn64_tvpl_ep1_m_p1_median',\n",
       "       'trade.tau_nn64_tvpl_ep5_m_p1_median',\n",
       "       'trade.tau_nn64_trade_vol_m_p1_median',\n",
       "       'trade.tau_nn64_trade.tau_m_p1_median',\n",
       "       'trade.tau_nn64_high5_nn_m_median', 'trade.tau_nn64_low5_nn_m_median',\n",
       "       'trade.tau_nn64_high5_abs_nn_m_median',\n",
       "       'trade.tau_nn64_low5_abs_nn_m_median',\n",
       "       'trade.tau_nn64_all_nn_m_p1_median'],\n",
       "      dtype='object', length=7145)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
