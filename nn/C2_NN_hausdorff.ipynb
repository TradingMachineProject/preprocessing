{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBRARY IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "from contextlib import contextmanager\n",
    "import time  \n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "from sklearn.preprocessing import minmax_scale  \n",
    "from typing import Dict, List, Optional, Tuple  \n",
    "import seaborn as sns \n",
    "import gc\n",
    "import traceback \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import kendalltau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_result_df_raw = pd.read_csv(\"./DB/BTC_sum_both_10m.csv\")\n",
    "print(\"# of rows of combined_Result_Df:\", combined_result_df_raw.shape[0])\n",
    "\n",
    "combined_result_df_raw['window_start'] = pd.to_datetime(combined_result_df_raw['window_start'])  # Convert to datetime\n",
    "\n",
    "# Define the time range\n",
    "start_time = pd.to_datetime('00:00:00').time()\n",
    "end_time = pd.to_datetime('06:00:00').time()\n",
    "\n",
    "# Filter and drop rows (새벽시간 삭제하기)\n",
    "filtered_df = combined_result_df_raw[~combined_result_df_raw['window_start'].apply(lambda x: start_time <= x.time() <= end_time)]\n",
    "print(\"# of rows of filtered_df:\", filtered_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_floats_from_series(df):\n",
    "    def extract_floats(s):\n",
    "        return [float(num) for num in re.findall(r\"(-?\\d+\\.\\d+)\", s)]\n",
    "\n",
    "    df['only_prices_30s_for_NN'] = df['prices_30s_for_NN_onlyprices'].apply(extract_floats)\n",
    "    \n",
    "    # 데이터 포인트가 20개가 아닌 행을 삭제\n",
    "    mask = df['only_prices_30s_for_NN'].apply(len) == 20\n",
    "    df = df[mask].reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Early morning period & CHECK NULL VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the target variable:\n",
    "target_var = 'dv5_realized_volatility_mean0'\n",
    "target_var_3 = target_var[:3]\n",
    "print(\"target_var_3:\", target_var_3)\n",
    "\n",
    "# 사용하지 않을 변수(column)들 미리 제거 (dropna 효과를 최소화하기 위해.)\n",
    "remove_varlist0 = [\n",
    "                  'window_start', \n",
    "                  'window_end',\n",
    "                  'prices_30s_for_NN',\n",
    "                  'window_end_150_ticker',\n",
    "                  'window_end_300_ticker',\n",
    "                  'window_end_450_ticker',\n",
    "                  'window_end_150_orderbook',\n",
    "                  'window_end_300_orderbook',\n",
    "                  'window_end_450_orderbook',\n",
    "                  'volume_power',\n",
    "                  'volume_power_150',\n",
    "                  'volume_power_300',\n",
    "                  'volume_power_450',\n",
    "                  'end_price',\n",
    "\n",
    "                #   'dv1_realized_volatility',\n",
    "                  'dv2_lowest_return',\n",
    "                  'dv3_highest_return',\n",
    "                  'dv4_realized_volatility_30s',\n",
    "                  'dv5_realized_volatility_mean0',\n",
    "\n",
    "                  #'prices_30s_for_NN_onlyprices',\n",
    "                  #'only_prices_30s_for_NN'\n",
    "                  ]\n",
    "remove_varlist0.remove(target_var)\n",
    "\n",
    "main_feature_list = list(filtered_df.columns)\n",
    "\n",
    "for i in range(len(remove_varlist0)):\n",
    "    main_feature_list.remove(remove_varlist0[i])\n",
    "\n",
    "filtered_df = filtered_df[main_feature_list]\n",
    "\n",
    "combined_result_df = filtered_df.dropna()\n",
    "print(\"# of rows of filtered_df:\", combined_result_df.shape[0])\n",
    "\n",
    "combined_result_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_result_df = extract_floats_from_series(combined_result_df)\n",
    "listed_time_series_df = combined_result_df['only_prices_30s_for_NN'].copy()\n",
    "combined_result_df = combined_result_df.drop('prices_30s_for_NN_onlyprices', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_time_series_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUMERIC FEATURES & CALCULATE CORR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_feature_list = list(combined_result_df.columns)\n",
    "feature_list_for_corr = list(combined_result_df.columns)\n",
    "\n",
    "remove_varlist = [\n",
    "                  'time_id',\n",
    "                  'only_prices_30s_for_NN'\n",
    "                  ]\n",
    "\n",
    "for i in range(len(remove_varlist)):\n",
    "    # print(remove_varlist[i])\n",
    "    feature_list_for_corr.remove(remove_varlist[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combined_result_df\n",
    "correlation_matrix = data[feature_list_for_corr].corr(method=lambda x, y: kendalltau(x, y).correlation)\n",
    "correlation_matrix_series = correlation_matrix[target_var].copy() # .sort_values(ascending=False)\n",
    "correlation_matrix_series.sort_values(ascending=False, inplace=True)\n",
    "correlation_matrix_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_result_df_mfl = combined_result_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_for_finding_NN = []\n",
    "feature_list_for_finding_NN.append('realized_volatility_mean0')\n",
    "feature_list_for_finding_NN.append('bidask_spread_0')\n",
    "feature_list_for_finding_NN.append('bidask_spread_1')\n",
    "feature_list_for_finding_NN.append('highest_return')\n",
    "feature_list_for_finding_NN.append('lowest_return')\n",
    "feature_list_for_finding_NN.append('num_trades')\n",
    "feature_list_for_finding_NN.append('high_low_gap')\n",
    "feature_list_for_finding_NN.append('BB_width_w10')\n",
    "feature_list_for_finding_NN.append('BB_width_w20')\n",
    "feature_list_for_finding_NN.append('liq_last_1')\n",
    "feature_list_for_finding_NN.append('liq_last_5')\n",
    "feature_list_for_finding_NN.append('ep_liq_1')\n",
    "feature_list_for_finding_NN.append('ep_liq_5')\n",
    "feature_list_for_finding_NN.append('tvpl1')\n",
    "feature_list_for_finding_NN.append('tvpl5')\n",
    "feature_list_for_finding_NN.append('tvpl_ep1')\n",
    "feature_list_for_finding_NN.append('tvpl_ep5')\n",
    "feature_list_for_finding_NN.append('trade_vol')\n",
    "feature_list_for_finding_NN.append('trade.tau')\n",
    "# feature_list_for_finding_NN.append('volume_power')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "from numba import jit\n",
    "\n",
    "# Numba를 사용하여 최적화된 Hausdorff 거리 계산 함수\n",
    "@jit(nopython=True)\n",
    "def compute_hausdorff_modified_numba(x, y):\n",
    "    min_distances = np.abs(np.subtract(x, y))\n",
    "    return np.max(min_distances)\n",
    "\n",
    "# 주어진 시계열 대상과 다른 모든 시계열 간의 거리를 계산하는 함수\n",
    "def compute_distances(target_series, series_list):\n",
    "    return [compute_hausdorff_modified_numba(target_series, s) for s in series_list]\n",
    "\n",
    "# 병렬 처리를 사용하여 특정 시계열과 가장 유사한 시계열의 인덱스를 찾는 함수\n",
    "def parallel_similarity(df, idx, num_similar, window_size = 10000):\n",
    "    if window_size > idx :\n",
    "        return np.array([0] * num_similar)\n",
    "    \n",
    "    target_series = np.array(df.iloc[idx])\n",
    "    \n",
    "    other_series = []\n",
    "    \n",
    "    for series in df.iloc[idx-window_size:idx] :\n",
    "        other_series.append(np.array(series))\n",
    "            \n",
    "    distances = compute_distances(target_series, other_series)\n",
    "    similar_indices = list(np.argsort(distances)[:num_similar])\n",
    "    \n",
    "    similar_indices = [idx - window_size + x for x in similar_indices]\n",
    "    \n",
    "    return similar_indices\n",
    "\n",
    "# 병렬 처리를 사용하여 모든 시계열 간의 유사도를 계산하는 함수\n",
    "def compute_similarity_parallel(df, num_similar=10, window_size = 10000):\n",
    "    # 빈 2D 배열 생성\n",
    "    results = np.array([]).reshape(0, num_similar)  # 2D 배열, 0행 2열\n",
    "    \n",
    "    for idx in range(len(df)):\n",
    "        if (idx + 1) % (len(df) // 10) == 0:\n",
    "            print(f\"Processed {idx + 1} rows ({((idx + 1) / len(df)) * 100:.1f}%)\")\n",
    "        result = parallel_similarity(df, idx, num_similar, window_size)\n",
    "        results = np.append(results, [result], axis=0)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILD NEIGHBORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NEIGHBORS_MAX = 65 \n",
    "\n",
    "class Neighbors:\n",
    "    def __init__(self, \n",
    "                 name: str, \n",
    "                 pivot: pd.DataFrame, \n",
    "                 p: float, \n",
    "                 metric: str = 'minkowski', \n",
    "                 metric_params: object = None, \n",
    "                 exclude_self: bool = True,\n",
    "                 house_metric = False\n",
    "                 ):\n",
    "        self.name = name\n",
    "        self.exclude_self = exclude_self\n",
    "        self.p = p\n",
    "        self.metric = metric\n",
    "        self.neighbors = np.empty((0, N_NEIGHBORS_MAX), dtype=int)  # 빈 2D 배열로 초기화\n",
    "\n",
    "        nn = NearestNeighbors(\n",
    "            n_neighbors=N_NEIGHBORS_MAX, \n",
    "            p=p, \n",
    "            metric=metric, \n",
    "            metric_params=metric_params\n",
    "        )\n",
    "        \n",
    "        # nn.fit(pivot)\n",
    "        # _, self.neighbors = nn.kneighbors(pivot, return_distance=True)\n",
    "\n",
    "        # 이웃을 찾을 이전 window 벙뮈\n",
    "        window_size = 10000\n",
    "        \n",
    "        \n",
    "        if house_metric == True : \n",
    "            self.neighbors = compute_similarity_parallel(listed_time_series_df, num_similar=N_NEIGHBORS_MAX, window_size = window_size)\n",
    "            self.neighbors = self.neighbors.astype(int)\n",
    "        else :\n",
    "            col_names = pivot.columns\n",
    "            index_name = pivot.index.name\n",
    "            \n",
    "            for t in range(len(pivot)) :\n",
    "                # window size 까지의 데이터는 random NN 설정\n",
    "                # 1 ~ window 범위의 데이터는 추후 버려야 함\n",
    "                if t < window_size :\n",
    "                    update_array = np.random.permutation(np.arange(1, N_NEIGHBORS_MAX+1))\n",
    "                    self.neighbors = np.append(self.neighbors, [update_array], axis = 0)\n",
    "\n",
    "                else :\n",
    "                    pvdf = pd.DataFrame(pivot.iloc[t-window_size:t])\n",
    "                    pvdf.columns = [list(col_names)]\n",
    "                    pvdf = pvdf.rename_axis(index_name)\n",
    "                    nn.fit(pvdf)\n",
    "\n",
    "                    update_array = nn.kneighbors(pivot.iloc[t].values.reshape(1, -1), return_distance=False)\n",
    "                    update_array = update_array.reshape(-1)\n",
    "                    self.neighbors = np.append(self.neighbors, [update_array], axis = 0)\n",
    "                    # if t // 10000 == 0 :\n",
    "                    #    print(t,self.neighbors)\n",
    "\n",
    "        self.columns = self.index = self.feature_values = self.feature_col = None\n",
    "\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def make_nn_feature(self, n=5, agg=np.mean) -> pd.DataFrame:\n",
    "        assert self.feature_values is not None, \"should call rearrange_feature_values beforehand\"\n",
    "\n",
    "        start = 1 if self.exclude_self else 0\n",
    "\n",
    "        pivot_aggs = pd.DataFrame(\n",
    "            agg(self.feature_values[start:n+start,:,0], axis=0), \n",
    "            columns=self.columns, \n",
    "            index=self.index\n",
    "        )\n",
    "\n",
    "        dst = pivot_aggs.reset_index() # unstack().\n",
    "        # print(\"dst.shape:\", dst.shape)\n",
    "        new_column_names = ['time_id', f'{self.feature_col}_nn{n}_{self.name}_{agg.__name__}'] # 3개를 예측했는데 2개만 들어왔다??\n",
    "        dst.columns = new_column_names \n",
    "        return dst\n",
    "    \n",
    "\n",
    "class TimeIdNeighbors(Neighbors):\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        # feature_pivot = df.pivot(index='time_id', values=feature_col)\n",
    "        # feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "\n",
    "        feature_df = df[['time_id', feature_col]]\n",
    "        feature_df.set_index('time_id', inplace=True)\n",
    "        feature_df = feature_df.fillna(feature_df.mean())\n",
    "\n",
    "        feature_values = np.zeros((N_NEIGHBORS_MAX, feature_df.shape[0], 1))\n",
    "\n",
    "        for i in range(N_NEIGHBORS_MAX):\n",
    "            feature_values[i, :, 0] += feature_df.values[self.neighbors[:, i], 0]\n",
    "\n",
    "        self.columns = list(feature_df.columns)\n",
    "        self.index = list(feature_df.index)\n",
    "        self.feature_values = feature_values\n",
    "        self.feature_col = feature_col\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"time-id NN (name={self.name}, metric={self.metric}, p={self.p})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROGRESS CHECK FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - s\n",
    "    print(f'[{name}] {elapsed: .3f}초')\n",
    "\n",
    "def print_trace(name: str = ''):\n",
    "    print(f'{name or \"익명\"}에서 에러가 발생했습니다.')\n",
    "    print(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET NN CLASS. Output: {}_sum_plus_nn_features.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# StandardScaler 객체 생성\n",
    "scaler = StandardScaler()\n",
    "\n",
    "USE_ONE_FEATURE_C = False\n",
    "USE_ONE_FEATURE_M_1 = False\n",
    "USE_ONE_FEATURE_M_2 = False\n",
    "\n",
    "USE_TWO_FEATURES = False\n",
    "\n",
    "USE_ALL_FEATURES = False\n",
    "USE_SEVALRAL_FEATURES = False\n",
    "\n",
    "USE_HOUSE_FEATURES = True\n",
    "\n",
    "# Top 5 Related Feature\n",
    "top_5_high_feat = list(correlation_matrix_series.keys())[1:6]\n",
    "top_5_low_feat = list(correlation_matrix_series.keys())[-5:]\n",
    "\n",
    "# Top 5 Absolute Related Feature\n",
    "sorted_data = correlation_matrix_series.abs().sort_values(ascending=False)\n",
    "top_5_high_abs_feat = list(sorted_data.head(6).keys())[1:]\n",
    "top_5_low_abs_feat = list(sorted_data.tail(5).keys())\n",
    "\n",
    "# time_id_neighbors List \n",
    "time_id_neighbors: List[Neighbors] = []\n",
    "\n",
    "with timer('knn fit'):\n",
    "    df_pv = combined_result_df_mfl.copy()\n",
    "    # df_pv = df_pv.drop(['window_start', 'window_end','volume_power'], axis=1)\n",
    "    \n",
    "    # Standard All Feature\n",
    "    # df_pv[feature_list_for_finding_NN] = scaler.fit_transform(df_pv[feature_list_for_finding_NN])\n",
    "\n",
    "    feature_list = list(df_pv.columns)\n",
    "    feature_list.remove('time_id') # Can't be standardized\n",
    "    feature_list.remove('only_prices_30s_for_NN') # Can't be standardized\n",
    "    \n",
    "    # feature_list.remove('prices_30s_for_NN_onlyprices') # Can't be standardized\n",
    "    # feature_list.remove('time_id')\n",
    "    df_pv[feature_list] = scaler.fit_transform(df_pv[feature_list])\n",
    "    # combined_result_df_mfl_scaled.head(3)\n",
    "\n",
    "\n",
    "\n",
    "    #### USE ONLY ONE FACTOR ####\n",
    "\n",
    "    # ## Canberra Distance\n",
    "    # if USE_ONE_FEATURE_C :\n",
    "    #     for feat in feature_list_for_finding_NN :\n",
    "    #         gc.collect()\n",
    "    #         df_nn = df_pv[['time_id',feat]]\n",
    "    #         df_nn.set_index('time_id', inplace=True)\n",
    "    #         df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "    #         time_id_neighbors.append(\n",
    "    #             TimeIdNeighbors(\n",
    "    #                 feat + '_c', \n",
    "    #                 df_nn, \n",
    "    #                 p=2, \n",
    "    #                 metric='canberra', \n",
    "    #                 exclude_self=True\n",
    "    #             )\n",
    "    #         )\n",
    "\n",
    "    ## Manhattan Distance\n",
    "    i = 0\n",
    "    if USE_ONE_FEATURE_M_1:\n",
    "        for feat in feature_list_for_finding_NN :\n",
    "            gc.collect()\n",
    "            df_nn = df_pv[['time_id',feat]]\n",
    "            df_nn.set_index('time_id', inplace=True)\n",
    "            # df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "            time_id_neighbors.append(\n",
    "                TimeIdNeighbors(feat + '_m_p1', df_nn, p=1, exclude_self=True)\n",
    "            )\n",
    "\n",
    "    # ## Euclidean Distance\n",
    "    # if USE_ONE_FEATURE_M_2:\n",
    "    #     for feat in feature_list_for_finding_NN :\n",
    "    #         gc.collect()\n",
    "    #         df_nn = df_pv[['time_id',feat]]\n",
    "    #         df_nn.set_index('time_id', inplace=True)\n",
    "    #         df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "    #         time_id_neighbors.append(\n",
    "    #             TimeIdNeighbors(feat + '_m_p2', df_nn, p=2, exclude_self=True)\n",
    "    #         )\n",
    "\n",
    "\n",
    "    # #### TWO FACTOR ####\n",
    "\n",
    "    # if USE_TWO_FEATURES:\n",
    "    #     feature_list = ['time_id','realized_volatility_mean0','bidask_spread_0'] # target variable이 바뀌면 2번/3번 변수 수정해주어야.\n",
    "    #     df_nn = df_pv[feature_list]\n",
    "    #     df_nn.set_index('time_id', inplace=True)\n",
    "    #     df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "    #     ## Canberra\n",
    "\n",
    "    #     time_id_neighbors.append(\n",
    "    #             TimeIdNeighbors(\n",
    "    #                 'two_c',  # before correction: feat + 'two_c', \n",
    "    #                 df_nn, \n",
    "    #                 p=2, \n",
    "    #                 metric='canberra', \n",
    "    #                 exclude_self=True\n",
    "    #             )\n",
    "    #         )\n",
    "    #     ## Euclidean Distance\n",
    "    #     time_id_neighbors.append(\n",
    "    #         TimeIdNeighbors(\n",
    "    #             'two_m', \n",
    "    #             df_nn, \n",
    "    #             p=2, \n",
    "    #             exclude_self=True\n",
    "    #         )\n",
    "    #     )\n",
    "\n",
    "\n",
    "    #### USE SEVALRAL FACTOR ####\n",
    "\n",
    "    if USE_SEVALRAL_FEATURES:\n",
    "        ## High Related Feature \n",
    "        feature_list = ['time_id']\n",
    "        feature_list += top_5_high_feat\n",
    "        df_nn = df_pv[feature_list]\n",
    "        df_nn.set_index('time_id', inplace=True)\n",
    "        # df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "        ### Euclidean Distance\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'high5_nn_m', \n",
    "                df_nn, \n",
    "                p=1, \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ## Low Related Feature\n",
    "\n",
    "        feature_list = ['time_id']\n",
    "        feature_list += top_5_low_feat\n",
    "        df_nn = df_pv[feature_list]\n",
    "        df_nn.set_index('time_id', inplace=True)\n",
    "        # df_nn = df_nn.fillna(df_nn.mean())        \n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'low5_nn_m', \n",
    "                df_nn, \n",
    "                p=1, \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ## High Abs Related Feature\n",
    "\n",
    "        feature_list = ['time_id']\n",
    "        feature_list += top_5_high_abs_feat\n",
    "        df_nn = df_pv[feature_list]\n",
    "        df_nn.set_index('time_id', inplace=True)\n",
    "        # df_nn = df_nn.fillna(df_nn.mean())\n",
    "        \n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'high5_abs_nn_m', \n",
    "                df_nn, \n",
    "                p=1, \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ## Low Abs Related Feature\n",
    "\n",
    "        feature_list = ['time_id']\n",
    "        feature_list += top_5_low_abs_feat\n",
    "        df_nn = df_pv[feature_list]\n",
    "        df_nn.set_index('time_id', inplace=True)\n",
    "        # df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'low5_abs_nn_m', \n",
    "                df_nn, \n",
    "                p=1, \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    #### USE ALL FACTOR ####\n",
    "\n",
    "    if USE_ALL_FEATURES:\n",
    "        df_nn = df_pv.copy()\n",
    "        # df_nn = df_nn.drop(['dv1_realized_volatility'], axis=1)\n",
    "        df_nn.set_index('time_id', inplace=True)\n",
    "\n",
    "        # Standard All Feature\n",
    "        # df_nn[feature_list_for_finding_NN] = scaler.fit_transform(df_nn[feature_list_for_finding_NN])\n",
    "\n",
    "        df_nn = df_nn[feature_list_for_finding_NN]\n",
    "        # df_nn = df_nn.fillna(df_nn.mean())\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'all_nn_m_p1', \n",
    "                df_nn, \n",
    "                p=1, \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # time_id_neighbors.append(\n",
    "        #     TimeIdNeighbors(\n",
    "        #         'all_nn_m_p2', \n",
    "        #         df_nn, \n",
    "        #         p=2, \n",
    "        #         exclude_self=True\n",
    "        #     )\n",
    "        # )\n",
    "        \n",
    "    if USE_HOUSE_FEATURES:\n",
    "        time_id_neighbors.append(\n",
    "                TimeIdNeighbors('hausdorff', None, p=0, exclude_self=True, house_metric = True)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_id_neighbors[0].neighbors[9999:10001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate Features With NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nearest_neighbor_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df2 = combined_result_df_mfl.copy()\n",
    "    print(df2.shape)\n",
    "\n",
    "    ### time_id를 기준으로 얻어진 neighbor를 대상으로 feature 만들기\n",
    "    feature_cols = {\n",
    "        # 'realized_volatility': [np.mean], #, np.min, np.max, np.std\n",
    "        'realized_volatility_mean0': [np.mean, np.median],\n",
    "        'realized_volatility_30s': [np.mean, np.median],\n",
    "\n",
    "        # 'dv1_realized_volatility': [np.mean],\n",
    "        # 'dv2_lowest_return': [np.mean, np.median],\n",
    "        # 'dv3_highest_return': [np.mean, np.median],\n",
    "        # 'dv4_realized_volatility_30s': [np.mean, np.median],\n",
    "        'dv5_realized_volatility_mean0': [np.mean, np.median],\n",
    "\n",
    "        'num_trades': [np.mean, np.median],\n",
    "        'lowest_return': [np.mean, np.median], # , np.mean, np.min\n",
    "        'highest_return': [np.mean, np.median], # , np.mean, np.min\n",
    "        'high_low_gap': [np.mean, np.median],\n",
    "        'trade_vol': [np.mean, np.median],\n",
    "        # 'volume_power': [np.mean, np.median],\n",
    "        'BB_width_w10': [np.mean, np.median],\n",
    "        'BB_width_w20': [np.mean, np.median],\n",
    "\n",
    "        'liq_last_1': [np.mean, np.median],\n",
    "        'liq_last_5': [np.mean, np.median],\n",
    "        'ep_liq_1': [np.mean, np.median],\n",
    "        'ep_liq_5': [np.mean, np.median],\n",
    "        'bidask_spread_0': [np.mean, np.median],\n",
    "        'bidask_spread_1': [np.mean, np.median],\n",
    "\n",
    "        'tvpl1': [np.mean, np.median],\n",
    "        'tvpl5': [np.mean, np.median],\n",
    "        'tvpl_ep1': [np.mean, np.median],\n",
    "        'tvpl_ep5': [np.mean, np.median],\n",
    "        'trade.tau': [np.mean, np.median],\n",
    "    }\n",
    "\n",
    "    time_id_neigbor_sizes = [2, 4, 8, 16, 32, 48, 64] # 메모리 부족으로 계속 오류가 나는 것 같아 이웃 계산 숫자를 줄임.\n",
    "\n",
    "    ndf: Optional[pd.DataFrame] = None\n",
    "    \n",
    "    # 새로운 feature를 기존 df에 추가하는 함수\n",
    "    def _add_ndf(ndf: Optional[pd.DataFrame], dst: pd.DataFrame) -> pd.DataFrame:\n",
    "        if ndf is None:\n",
    "            return dst\n",
    "        else:\n",
    "            ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
    "            #columns_to_convert = [dst.columns[-1]]  # 열 변환 대상을 선택하거나 여러 열을 지정할 수 있음\n",
    "            #converted_columns = dst[columns_to_convert].astype(np.float32)\n",
    "            #ndf = pd.concat([ndf, converted_columns], axis=1)\n",
    "\n",
    "            return ndf\n",
    "\n",
    "    # neighbor time_id\n",
    "    for feature_col in feature_cols.keys():\n",
    "        gc.collect()\n",
    "        try: \n",
    "            for nn in time_id_neighbors:\n",
    "                nn.rearrange_feature_values(df2, feature_col)\n",
    "\n",
    "\n",
    "            time_id_ns = time_id_neigbor_sizes\n",
    "\n",
    "            for agg in feature_cols[feature_col]:\n",
    "                for n in time_id_ns:\n",
    "                    try:\n",
    "                        for nn in time_id_neighbors:\n",
    "                            gc.collect()\n",
    "                            dst = nn.make_nn_feature(n, agg)\n",
    "                            ndf = _add_ndf(ndf, dst)\n",
    "                    except Exception:\n",
    "                        WHERE_ERROR = feature_col\n",
    "                        print_trace('time-id nn')\n",
    "                        pass\n",
    "        except Exception:\n",
    "            print_trace('time-id nn')\n",
    "\n",
    "    if ndf is not None:\n",
    "        df2 = pd.merge(df2, ndf, on=['time_id'], how='left')\n",
    "    \n",
    "    print(df2.shape)\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "combined_result_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "with timer('make nearest neighbor feature'):\n",
    "    df3 = make_nearest_neighbor_feature(combined_result_df_mfl)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the first 'window_size' (10,000) rows. <= These rows do not have appropriate nearest neighbors.\n",
    "window_size = 10000\n",
    "df3 = df3.drop(index=range(window_size))\n",
    "\n",
    "coin = 'BTC'\n",
    "df3.to_csv(working_directory + \"output\\\\{}_sum_plus_nn_features_for_{}.csv\".format(coin, target_var_3), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP code\n",
    "\n",
    "import gc\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "working_directory = 'D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'  ## 서로 다른 환경에서는 이곳을 수정해야 함.\n",
    "# working_directory = 'C:\\\\Users\\\\user\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'\n",
    "\n",
    "os.chdir(working_directory)\n",
    "gc.collect()\n",
    "\n",
    "# combined_result_df = pd.read_csv(\"./DB/professor_BTC_sum_both_10m.csv\")\n",
    "coin = 'BTC'\n",
    "target_var = 'dv5_realized_volatility_mean0'\n",
    "target_var_3 = target_var[:3]\n",
    "df3 = pd.read_csv(\"./output/{}_sum_plus_nn_features_for_{}.csv\".format(coin, target_var_3))\n",
    "\n",
    "window_size=10000\n",
    "df3 = df3.drop(index=range(window_size))\n",
    "df3.to_csv(working_directory + \"output\\\\{}_sum_plus_nn_features_for_{}.csv\".format(coin, target_var_3), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the final outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "working_directory = 'D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'  ## 서로 다른 환경에서는 이곳을 수정해야 함.\n",
    "# working_directory = 'C:\\\\Users\\\\user\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'\n",
    "\n",
    "os.chdir(working_directory)\n",
    "gc.collect()\n",
    "\n",
    "# combined_result_df = pd.read_csv(\"./DB/professor_BTC_sum_both_10m.csv\")\n",
    "coin = 'BTC'\n",
    "target_var = 'dv5_realized_volatility_mean0'\n",
    "target_var_3 = target_var[:3]\n",
    "df3 = pd.read_csv(\"./output/{}_sum_plus_nn_features_for_{}.csv\".format(coin, target_var_3))\n",
    "\n",
    "df3.head(3)\n",
    "print(\"# of rows:\", df3.shape[0])\n",
    "print(\"# of columns:\", df3.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
